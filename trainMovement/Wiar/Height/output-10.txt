['train-height-10.py', '0']
155 1
2_155_65_10_csi_a10_14.dat
155 3
155 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
155 9
155 10
155 11
155 12
155 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
155 17
2_155_65_10_csi_a10_13.dat
155 19
155 20
2_155_65_10_csi_a10_3.dat
155 22
155 23
155 24
2_155_65_10_csi_a10_4.dat
155 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
155 29
155 30
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
180 131
180 132
180 133
180 134
180 135
180 136
180 137
180 138
180 139
180 140
180 141
180 142
180 143
180 144
180 145
180 146
180 147
180 148
180 149
180 150
180 151
180 152
180 153
180 154
180 155
180 156
1_180_85_10_csi_a10_28.dat
180 158
180 159
180 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
180 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
180 178
180 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
180 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
180 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
173 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 170
 170 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.5046 - mae: 0.6080 - mse: 0.5046
64/87 [=====================>........] - ETA: 0s - loss: 0.4460 - mae: 0.5844 - mse: 0.4460
87/87 [==============================] - 1s 12ms/step - loss: 0.3887 - mae: 0.5430 - mse: 0.3887 - val_loss: 0.1587 - val_mae: 0.3500 - val_mse: 0.1587
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2039 - mae: 0.4042 - mse: 0.2039
64/87 [=====================>........] - ETA: 0s - loss: 0.1773 - mae: 0.3560 - mse: 0.1773
87/87 [==============================] - 1s 8ms/step - loss: 0.1832 - mae: 0.3528 - mse: 0.1832 - val_loss: 0.0339 - val_mae: 0.1640 - val_mse: 0.0339
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2228 - mae: 0.3331 - mse: 0.2228
64/87 [=====================>........] - ETA: 0s - loss: 0.2113 - mae: 0.3352 - mse: 0.2113
87/87 [==============================] - 1s 7ms/step - loss: 0.2032 - mae: 0.3349 - mse: 0.2032 - val_loss: 0.0652 - val_mae: 0.2329 - val_mse: 0.0652
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1441 - mae: 0.3021 - mse: 0.1441
64/87 [=====================>........] - ETA: 0s - loss: 0.1285 - mae: 0.2874 - mse: 0.1285
87/87 [==============================] - 1s 6ms/step - loss: 0.1401 - mae: 0.3095 - mse: 0.1401 - val_loss: 0.1532 - val_mae: 0.3285 - val_mse: 0.1532
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1584 - mae: 0.3308 - mse: 0.1584
64/87 [=====================>........] - ETA: 0s - loss: 0.1482 - mae: 0.3218 - mse: 0.1482
87/87 [==============================] - 1s 7ms/step - loss: 0.1428 - mae: 0.3187 - mse: 0.1428 - val_loss: 0.1877 - val_mae: 0.3750 - val_mse: 0.1877
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1100 - mae: 0.2779 - mse: 0.1100
64/87 [=====================>........] - ETA: 0s - loss: 0.1447 - mae: 0.3288 - mse: 0.1447
87/87 [==============================] - 1s 6ms/step - loss: 0.1429 - mae: 0.3237 - mse: 0.1429 - val_loss: 0.1473 - val_mae: 0.3169 - val_mse: 0.1473
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1244 - mae: 0.2853 - mse: 0.1244
64/87 [=====================>........] - ETA: 0s - loss: 0.1127 - mae: 0.2812 - mse: 0.1127
87/87 [==============================] - 1s 6ms/step - loss: 0.1236 - mae: 0.2948 - mse: 0.1236 - val_loss: 0.0793 - val_mae: 0.1999 - val_mse: 0.0793
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1250 - mae: 0.2943 - mse: 0.1250
64/87 [=====================>........] - ETA: 0s - loss: 0.1525 - mae: 0.3260 - mse: 0.1525
87/87 [==============================] - 1s 6ms/step - loss: 0.1405 - mae: 0.3105 - mse: 0.1405 - val_loss: 0.0622 - val_mae: 0.1618 - val_mse: 0.0622
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1149 - mae: 0.2775 - mse: 0.1149
64/87 [=====================>........] - ETA: 0s - loss: 0.1263 - mae: 0.2827 - mse: 0.1263
87/87 [==============================] - 1s 7ms/step - loss: 0.1203 - mae: 0.2823 - mse: 0.1203 - val_loss: 0.0866 - val_mae: 0.2075 - val_mse: 0.0866
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1213 - mae: 0.2987 - mse: 0.1213
64/87 [=====================>........] - ETA: 0s - loss: 0.1163 - mae: 0.2880 - mse: 0.1163
87/87 [==============================] - 1s 7ms/step - loss: 0.1103 - mae: 0.2760 - mse: 0.1103 - val_loss: 0.1173 - val_mae: 0.2755 - val_mse: 0.1173
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1065 - mae: 0.2566 - mse: 0.1065
64/87 [=====================>........] - ETA: 0s - loss: 0.0953 - mae: 0.2501 - mse: 0.0953
87/87 [==============================] - 1s 7ms/step - loss: 0.1125 - mae: 0.2740 - mse: 0.1125 - val_loss: 0.1332 - val_mae: 0.3052 - val_mse: 0.1332
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1121 - mae: 0.2644 - mse: 0.1121
64/87 [=====================>........] - ETA: 0s - loss: 0.1106 - mae: 0.2582 - mse: 0.1106
87/87 [==============================] - 1s 6ms/step - loss: 0.1226 - mae: 0.2782 - mse: 0.1226 - val_loss: 0.1064 - val_mae: 0.2621 - val_mse: 0.1064
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1198 - mae: 0.2737 - mse: 0.1198
64/87 [=====================>........] - ETA: 0s - loss: 0.1279 - mae: 0.2895 - mse: 0.1279
87/87 [==============================] - 1s 6ms/step - loss: 0.1244 - mae: 0.2873 - mse: 0.1244 - val_loss: 0.0762 - val_mae: 0.1985 - val_mse: 0.0762
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1030 - mae: 0.2687 - mse: 0.1030
64/87 [=====================>........] - ETA: 0s - loss: 0.1139 - mae: 0.2699 - mse: 0.1139
87/87 [==============================] - 1s 8ms/step - loss: 0.1147 - mae: 0.2691 - mse: 0.1147 - val_loss: 0.0752 - val_mae: 0.1957 - val_mse: 0.0752
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1051 - mae: 0.2732 - mse: 0.1051
64/87 [=====================>........] - ETA: 0s - loss: 0.1234 - mae: 0.2916 - mse: 0.1234
87/87 [==============================] - 1s 6ms/step - loss: 0.1102 - mae: 0.2670 - mse: 0.1102 - val_loss: 0.1178 - val_mae: 0.2821 - val_mse: 0.1178
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1158 - mae: 0.2703 - mse: 0.1158
64/87 [=====================>........] - ETA: 0s - loss: 0.1005 - mae: 0.2477 - mse: 0.1005
87/87 [==============================] - 1s 7ms/step - loss: 0.0946 - mae: 0.2383 - mse: 0.0946 - val_loss: 0.1255 - val_mae: 0.2965 - val_mse: 0.1255
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0975 - mae: 0.2449 - mse: 0.0975
64/87 [=====================>........] - ETA: 0s - loss: 0.1092 - mae: 0.2644 - mse: 0.1092
87/87 [==============================] - 1s 7ms/step - loss: 0.1098 - mae: 0.2647 - mse: 0.1098 - val_loss: 0.0974 - val_mae: 0.2512 - val_mse: 0.0974
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1085 - mae: 0.2670 - mse: 0.1085
64/87 [=====================>........] - ETA: 0s - loss: 0.0922 - mae: 0.2327 - mse: 0.0922
87/87 [==============================] - 1s 7ms/step - loss: 0.0940 - mae: 0.2365 - mse: 0.0940 - val_loss: 0.0741 - val_mae: 0.1999 - val_mse: 0.0741
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1055 - mae: 0.2480 - mse: 0.1055
64/87 [=====================>........] - ETA: 0s - loss: 0.1064 - mae: 0.2519 - mse: 0.1064
87/87 [==============================] - 1s 9ms/step - loss: 0.0964 - mae: 0.2382 - mse: 0.0964 - val_loss: 0.0770 - val_mae: 0.2027 - val_mse: 0.0770
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1023 - mae: 0.2626 - mse: 0.1023
64/87 [=====================>........] - ETA: 0s - loss: 0.0918 - mae: 0.2365 - mse: 0.0918
87/87 [==============================] - 1s 7ms/step - loss: 0.0921 - mae: 0.2356 - mse: 0.0921 - val_loss: 0.0991 - val_mae: 0.2503 - val_mse: 0.0991
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0709 - mae: 0.2091 - mse: 0.0709
64/87 [=====================>........] - ETA: 0s - loss: 0.0902 - mae: 0.2350 - mse: 0.0902
87/87 [==============================] - 1s 7ms/step - loss: 0.0911 - mae: 0.2366 - mse: 0.0911 - val_loss: 0.0988 - val_mae: 0.2507 - val_mse: 0.0988
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0965 - mae: 0.2381 - mse: 0.0965
64/87 [=====================>........] - ETA: 0s - loss: 0.1090 - mae: 0.2658 - mse: 0.1090
87/87 [==============================] - 1s 8ms/step - loss: 0.0996 - mae: 0.2472 - mse: 0.0996 - val_loss: 0.0940 - val_mae: 0.2388 - val_mse: 0.0940
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0961 - mae: 0.2471 - mse: 0.0961
64/87 [=====================>........] - ETA: 0s - loss: 0.0923 - mae: 0.2338 - mse: 0.0923
87/87 [==============================] - 1s 7ms/step - loss: 0.0950 - mae: 0.2340 - mse: 0.0950 - val_loss: 0.1025 - val_mae: 0.2519 - val_mse: 0.1025
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1018 - mae: 0.2488 - mse: 0.1018
64/87 [=====================>........] - ETA: 0s - loss: 0.0988 - mae: 0.2500 - mse: 0.0988
87/87 [==============================] - 1s 7ms/step - loss: 0.0916 - mae: 0.2388 - mse: 0.0916 - val_loss: 0.0948 - val_mae: 0.2475 - val_mse: 0.0948
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0733 - mae: 0.2161 - mse: 0.0733
64/87 [=====================>........] - ETA: 0s - loss: 0.0834 - mae: 0.2294 - mse: 0.0834
87/87 [==============================] - 1s 7ms/step - loss: 0.0806 - mae: 0.2146 - mse: 0.0806 - val_loss: 0.0936 - val_mae: 0.2491 - val_mse: 0.0936
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0888 - mae: 0.2340 - mse: 0.0888
64/87 [=====================>........] - ETA: 0s - loss: 0.0950 - mae: 0.2423 - mse: 0.0950
87/87 [==============================] - 1s 6ms/step - loss: 0.0816 - mae: 0.2172 - mse: 0.0816 - val_loss: 0.0836 - val_mae: 0.2286 - val_mse: 0.0836
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0862 - mae: 0.2294 - mse: 0.0862
64/87 [=====================>........] - ETA: 0s - loss: 0.0977 - mae: 0.2449 - mse: 0.0977
87/87 [==============================] - 1s 6ms/step - loss: 0.0806 - mae: 0.2162 - mse: 0.0806 - val_loss: 0.0788 - val_mae: 0.2128 - val_mse: 0.0788
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1115 - mae: 0.2568 - mse: 0.1115
64/87 [=====================>........] - ETA: 0s - loss: 0.0963 - mae: 0.2306 - mse: 0.0963
87/87 [==============================] - 1s 7ms/step - loss: 0.0907 - mae: 0.2236 - mse: 0.0907 - val_loss: 0.1070 - val_mae: 0.2608 - val_mse: 0.1070
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0724 - mae: 0.1995 - mse: 0.0724
64/87 [=====================>........] - ETA: 0s - loss: 0.0853 - mae: 0.2216 - mse: 0.0853
87/87 [==============================] - 1s 6ms/step - loss: 0.0801 - mae: 0.2149 - mse: 0.0801 - val_loss: 0.0945 - val_mae: 0.2410 - val_mse: 0.0945
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1069 - mae: 0.2449 - mse: 0.1069
64/87 [=====================>........] - ETA: 0s - loss: 0.0772 - mae: 0.2018 - mse: 0.0772
87/87 [==============================] - 1s 6ms/step - loss: 0.0826 - mae: 0.2078 - mse: 0.0826 - val_loss: 0.0713 - val_mae: 0.1835 - val_mse: 0.0713
Saving trained model...
98
Testing...
heightdiff= [0. 0. 0. 0. 0. 0.]
average prediction= [4.8281255]
baseline= 5.98
eachuser= [0. 0. 0. 0. 0. 0.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- nan
['train-height-10.py', '0']
155 1
2_155_65_10_csi_a10_14.dat
155 3
155 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
155 9
155 10
155 11
155 12
155 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
155 17
2_155_65_10_csi_a10_13.dat
155 19
155 20
2_155_65_10_csi_a10_3.dat
155 22
155 23
155 24
2_155_65_10_csi_a10_4.dat
155 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
155 29
155 30
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
180 131
180 132
180 133
180 134
180 135
180 136
180 137
180 138
180 139
180 140
180 141
180 142
180 143
180 144
180 145
180 146
180 147
180 148
180 149
180 150
180 151
180 152
180 153
180 154
180 155
180 156
1_180_85_10_csi_a10_28.dat
180 158
180 159
180 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
180 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
180 178
180 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
180 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
180 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
173 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 170
 170 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.6041 - mae: 0.7003 - mse: 0.6041
64/87 [=====================>........] - ETA: 0s - loss: 0.5039 - mae: 0.6293 - mse: 0.5039
87/87 [==============================] - 1s 11ms/step - loss: 0.4544 - mae: 0.5937 - mse: 0.4544 - val_loss: 0.3036 - val_mae: 0.4912 - val_mse: 0.3036
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.3017 - mae: 0.4958 - mse: 0.3017
64/87 [=====================>........] - ETA: 0s - loss: 0.2335 - mae: 0.4203 - mse: 0.2335
87/87 [==============================] - 1s 8ms/step - loss: 0.2151 - mae: 0.3982 - mse: 0.2151 - val_loss: 0.1821 - val_mae: 0.3769 - val_mse: 0.1821
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1411 - mae: 0.3088 - mse: 0.1411
64/87 [=====================>........] - ETA: 0s - loss: 0.1601 - mae: 0.3356 - mse: 0.1601
87/87 [==============================] - 1s 7ms/step - loss: 0.1674 - mae: 0.3433 - mse: 0.1674 - val_loss: 0.1999 - val_mae: 0.3741 - val_mse: 0.1999
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2100 - mae: 0.3580 - mse: 0.2100
64/87 [=====================>........] - ETA: 0s - loss: 0.1726 - mae: 0.3289 - mse: 0.1726
87/87 [==============================] - 1s 6ms/step - loss: 0.1502 - mae: 0.2988 - mse: 0.1502 - val_loss: 0.1805 - val_mae: 0.3784 - val_mse: 0.1805
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1233 - mae: 0.2716 - mse: 0.1233
64/87 [=====================>........] - ETA: 0s - loss: 0.1244 - mae: 0.2874 - mse: 0.1244
87/87 [==============================] - 1s 8ms/step - loss: 0.1272 - mae: 0.2925 - mse: 0.1272 - val_loss: 0.1937 - val_mae: 0.4038 - val_mse: 0.1937
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1203 - mae: 0.2747 - mse: 0.1203
64/87 [=====================>........] - ETA: 0s - loss: 0.1301 - mae: 0.2947 - mse: 0.1301
87/87 [==============================] - 1s 7ms/step - loss: 0.1299 - mae: 0.2960 - mse: 0.1299 - val_loss: 0.1944 - val_mae: 0.4090 - val_mse: 0.1944
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1090 - mae: 0.2666 - mse: 0.1090
64/87 [=====================>........] - ETA: 0s - loss: 0.1227 - mae: 0.2866 - mse: 0.1227
87/87 [==============================] - 1s 7ms/step - loss: 0.1149 - mae: 0.2797 - mse: 0.1149 - val_loss: 0.1808 - val_mae: 0.3835 - val_mse: 0.1808
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1045 - mae: 0.2351 - mse: 0.1045
64/87 [=====================>........] - ETA: 0s - loss: 0.1123 - mae: 0.2591 - mse: 0.1123
87/87 [==============================] - 1s 6ms/step - loss: 0.1174 - mae: 0.2585 - mse: 0.1174 - val_loss: 0.1744 - val_mae: 0.3551 - val_mse: 0.1744
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1024 - mae: 0.2240 - mse: 0.1024
64/87 [=====================>........] - ETA: 0s - loss: 0.1155 - mae: 0.2342 - mse: 0.1155
87/87 [==============================] - 1s 6ms/step - loss: 0.1190 - mae: 0.2445 - mse: 0.1190 - val_loss: 0.1673 - val_mae: 0.3438 - val_mse: 0.1673
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0739 - mae: 0.1873 - mse: 0.0739
64/87 [=====================>........] - ETA: 0s - loss: 0.0752 - mae: 0.1929 - mse: 0.0752
87/87 [==============================] - 1s 6ms/step - loss: 0.1021 - mae: 0.2169 - mse: 0.1021 - val_loss: 0.1545 - val_mae: 0.3387 - val_mse: 0.1545
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1164 - mae: 0.2401 - mse: 0.1164
64/87 [=====================>........] - ETA: 0s - loss: 0.0979 - mae: 0.2285 - mse: 0.0979
87/87 [==============================] - 1s 6ms/step - loss: 0.1000 - mae: 0.2347 - mse: 0.1000 - val_loss: 0.1436 - val_mae: 0.3346 - val_mse: 0.1436
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1108 - mae: 0.2690 - mse: 0.1108
64/87 [=====================>........] - ETA: 0s - loss: 0.1005 - mae: 0.2535 - mse: 0.1005
87/87 [==============================] - 1s 6ms/step - loss: 0.1010 - mae: 0.2588 - mse: 0.1010 - val_loss: 0.1359 - val_mae: 0.3242 - val_mse: 0.1359
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1128 - mae: 0.2470 - mse: 0.1128
64/87 [=====================>........] - ETA: 0s - loss: 0.1006 - mae: 0.2256 - mse: 0.1006
87/87 [==============================] - 1s 6ms/step - loss: 0.1016 - mae: 0.2306 - mse: 0.1016 - val_loss: 0.1312 - val_mae: 0.3138 - val_mse: 0.1312
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0579 - mae: 0.1843 - mse: 0.0579
64/87 [=====================>........] - ETA: 0s - loss: 0.0689 - mae: 0.1915 - mse: 0.0689
87/87 [==============================] - 1s 7ms/step - loss: 0.0909 - mae: 0.2216 - mse: 0.0909 - val_loss: 0.1311 - val_mae: 0.3057 - val_mse: 0.1311
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0984 - mae: 0.2483 - mse: 0.0984
64/87 [=====================>........] - ETA: 0s - loss: 0.1058 - mae: 0.2375 - mse: 0.1058
87/87 [==============================] - 1s 6ms/step - loss: 0.1069 - mae: 0.2380 - mse: 0.1069 - val_loss: 0.1283 - val_mae: 0.3078 - val_mse: 0.1283
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1094 - mae: 0.2657 - mse: 0.1094
64/87 [=====================>........] - ETA: 0s - loss: 0.1043 - mae: 0.2522 - mse: 0.1043
87/87 [==============================] - 1s 7ms/step - loss: 0.0982 - mae: 0.2425 - mse: 0.0982 - val_loss: 0.1268 - val_mae: 0.3125 - val_mse: 0.1268
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0702 - mae: 0.2022 - mse: 0.0702
64/87 [=====================>........] - ETA: 0s - loss: 0.0895 - mae: 0.2232 - mse: 0.0895
87/87 [==============================] - 1s 7ms/step - loss: 0.0951 - mae: 0.2359 - mse: 0.0951 - val_loss: 0.1253 - val_mae: 0.3086 - val_mse: 0.1253
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1247 - mae: 0.2718 - mse: 0.1247
64/87 [=====================>........] - ETA: 0s - loss: 0.1140 - mae: 0.2548 - mse: 0.1140
87/87 [==============================] - 1s 7ms/step - loss: 0.1047 - mae: 0.2437 - mse: 0.1047 - val_loss: 0.1237 - val_mae: 0.3029 - val_mse: 0.1237
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1070 - mae: 0.2641 - mse: 0.1070
64/87 [=====================>........] - ETA: 0s - loss: 0.0967 - mae: 0.2446 - mse: 0.0967
87/87 [==============================] - 1s 7ms/step - loss: 0.1015 - mae: 0.2426 - mse: 0.1015 - val_loss: 0.1260 - val_mae: 0.2880 - val_mse: 0.1260
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0663 - mae: 0.1766 - mse: 0.0663
64/87 [=====================>........] - ETA: 0s - loss: 0.0670 - mae: 0.1770 - mse: 0.0670
87/87 [==============================] - 1s 7ms/step - loss: 0.0937 - mae: 0.2114 - mse: 0.0937 - val_loss: 0.1272 - val_mae: 0.2835 - val_mse: 0.1272
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0765 - mae: 0.1989 - mse: 0.0765
64/87 [=====================>........] - ETA: 0s - loss: 0.0963 - mae: 0.2132 - mse: 0.0963
87/87 [==============================] - 0s 6ms/step - loss: 0.0951 - mae: 0.2150 - mse: 0.0951 - val_loss: 0.1201 - val_mae: 0.2933 - val_mse: 0.1201
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0649 - mae: 0.1865 - mse: 0.0649
64/87 [=====================>........] - ETA: 0s - loss: 0.0847 - mae: 0.2154 - mse: 0.0847
87/87 [==============================] - 0s 5ms/step - loss: 0.0942 - mae: 0.2337 - mse: 0.0942 - val_loss: 0.1190 - val_mae: 0.3069 - val_mse: 0.1190
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0663 - mae: 0.1901 - mse: 0.0663
64/87 [=====================>........] - ETA: 0s - loss: 0.0955 - mae: 0.2335 - mse: 0.0955
87/87 [==============================] - 1s 6ms/step - loss: 0.0967 - mae: 0.2376 - mse: 0.0967 - val_loss: 0.1144 - val_mae: 0.2870 - val_mse: 0.1144
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0829 - mae: 0.2107 - mse: 0.0829
64/87 [=====================>........] - ETA: 0s - loss: 0.0960 - mae: 0.2192 - mse: 0.0960
87/87 [==============================] - 0s 5ms/step - loss: 0.0931 - mae: 0.2172 - mse: 0.0931 - val_loss: 0.1132 - val_mae: 0.2603 - val_mse: 0.1132
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0960 - mae: 0.2187 - mse: 0.0960
64/87 [=====================>........] - ETA: 0s - loss: 0.0918 - mae: 0.2203 - mse: 0.0918
87/87 [==============================] - 0s 5ms/step - loss: 0.0922 - mae: 0.2160 - mse: 0.0922 - val_loss: 0.1156 - val_mae: 0.2482 - val_mse: 0.1156
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0575 - mae: 0.1735 - mse: 0.0575
64/87 [=====================>........] - ETA: 0s - loss: 0.0842 - mae: 0.2092 - mse: 0.0842
87/87 [==============================] - 0s 5ms/step - loss: 0.0842 - mae: 0.2074 - mse: 0.0842 - val_loss: 0.1082 - val_mae: 0.2524 - val_mse: 0.1082
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0771 - mae: 0.1927 - mse: 0.0771
64/87 [=====================>........] - ETA: 0s - loss: 0.0809 - mae: 0.2069 - mse: 0.0809
87/87 [==============================] - 0s 5ms/step - loss: 0.0889 - mae: 0.2205 - mse: 0.0889 - val_loss: 0.1058 - val_mae: 0.2745 - val_mse: 0.1058
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0735 - mae: 0.1895 - mse: 0.0735
64/87 [=====================>........] - ETA: 0s - loss: 0.0996 - mae: 0.2343 - mse: 0.0996
87/87 [==============================] - 0s 5ms/step - loss: 0.0961 - mae: 0.2338 - mse: 0.0961 - val_loss: 0.1044 - val_mae: 0.2724 - val_mse: 0.1044
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0864 - mae: 0.1921 - mse: 0.0864
64/87 [=====================>........] - ETA: 0s - loss: 0.0971 - mae: 0.2203 - mse: 0.0971
87/87 [==============================] - 0s 5ms/step - loss: 0.0972 - mae: 0.2202 - mse: 0.0972 - val_loss: 0.1051 - val_mae: 0.2501 - val_mse: 0.1051
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0784 - mae: 0.1989 - mse: 0.0784
64/87 [=====================>........] - ETA: 0s - loss: 0.0902 - mae: 0.2116 - mse: 0.0902
87/87 [==============================] - 0s 5ms/step - loss: 0.0787 - mae: 0.1991 - mse: 0.0787 - val_loss: 0.1043 - val_mae: 0.2429 - val_mse: 0.1043
Saving trained model...
98
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         45.28118896]
average prediction= [4.8332734]
baseline= 6.22
eachuser= [0. 0. 0. 0. 0. 3.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 15.093729654947916
['train-height-10.py', '0']
155 1
2_155_65_10_csi_a10_14.dat
155 3
155 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
155 9
155 10
155 11
155 12
155 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
155 17
2_155_65_10_csi_a10_13.dat
155 19
155 20
2_155_65_10_csi_a10_3.dat
155 22
155 23
155 24
2_155_65_10_csi_a10_4.dat
155 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
155 29
155 30
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
180 131
180 132
180 133
180 134
180 135
180 136
180 137
180 138
180 139
180 140
180 141
180 142
180 143
180 144
180 145
180 146
180 147
180 148
180 149
180 150
180 151
180 152
180 153
180 154
180 155
180 156
1_180_85_10_csi_a10_28.dat
180 158
180 159
180 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
180 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
180 178
180 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
180 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
180 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
173 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 170
 170 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.5073 - mae: 0.6424 - mse: 0.5073
64/87 [=====================>........] - ETA: 0s - loss: 0.3961 - mae: 0.5305 - mse: 0.3961
87/87 [==============================] - 1s 9ms/step - loss: 0.3440 - mae: 0.4875 - mse: 0.3440 - val_loss: 0.1929 - val_mae: 0.3788 - val_mse: 0.1929
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1793 - mae: 0.3572 - mse: 0.1793
64/87 [=====================>........] - ETA: 0s - loss: 0.1856 - mae: 0.3733 - mse: 0.1856
87/87 [==============================] - 0s 5ms/step - loss: 0.1934 - mae: 0.3832 - mse: 0.1934 - val_loss: 0.1732 - val_mae: 0.3414 - val_mse: 0.1732
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1754 - mae: 0.3766 - mse: 0.1754
64/87 [=====================>........] - ETA: 0s - loss: 0.1698 - mae: 0.3614 - mse: 0.1698
87/87 [==============================] - 0s 5ms/step - loss: 0.1635 - mae: 0.3500 - mse: 0.1635 - val_loss: 0.1730 - val_mae: 0.3534 - val_mse: 0.1730
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1262 - mae: 0.2959 - mse: 0.1262
64/87 [=====================>........] - ETA: 0s - loss: 0.1266 - mae: 0.2926 - mse: 0.1266
87/87 [==============================] - 0s 5ms/step - loss: 0.1330 - mae: 0.2994 - mse: 0.1330 - val_loss: 0.1950 - val_mae: 0.3899 - val_mse: 0.1950
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1541 - mae: 0.3118 - mse: 0.1541
64/87 [=====================>........] - ETA: 0s - loss: 0.1322 - mae: 0.2829 - mse: 0.1322
87/87 [==============================] - 0s 5ms/step - loss: 0.1289 - mae: 0.2782 - mse: 0.1289 - val_loss: 0.1779 - val_mae: 0.3698 - val_mse: 0.1779
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1294 - mae: 0.2880 - mse: 0.1294
64/87 [=====================>........] - ETA: 0s - loss: 0.1284 - mae: 0.2815 - mse: 0.1284
87/87 [==============================] - 0s 5ms/step - loss: 0.1164 - mae: 0.2706 - mse: 0.1164 - val_loss: 0.1559 - val_mae: 0.3244 - val_mse: 0.1559
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1078 - mae: 0.2605 - mse: 0.1078
64/87 [=====================>........] - ETA: 0s - loss: 0.1014 - mae: 0.2609 - mse: 0.1014
87/87 [==============================] - 0s 5ms/step - loss: 0.1053 - mae: 0.2651 - mse: 0.1053 - val_loss: 0.1545 - val_mae: 0.3078 - val_mse: 0.1545
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1231 - mae: 0.2790 - mse: 0.1231
64/87 [=====================>........] - ETA: 0s - loss: 0.1111 - mae: 0.2430 - mse: 0.1111
87/87 [==============================] - 0s 5ms/step - loss: 0.1064 - mae: 0.2445 - mse: 0.1064 - val_loss: 0.1564 - val_mae: 0.3177 - val_mse: 0.1564
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0746 - mae: 0.1706 - mse: 0.0746
64/87 [=====================>........] - ETA: 0s - loss: 0.0784 - mae: 0.1900 - mse: 0.0784
87/87 [==============================] - 0s 5ms/step - loss: 0.0922 - mae: 0.2088 - mse: 0.0922 - val_loss: 0.1608 - val_mae: 0.3451 - val_mse: 0.1608
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0410 - mae: 0.1553 - mse: 0.0410
64/87 [=====================>........] - ETA: 0s - loss: 0.0974 - mae: 0.2276 - mse: 0.0974
87/87 [==============================] - 0s 5ms/step - loss: 0.1023 - mae: 0.2439 - mse: 0.1023 - val_loss: 0.1671 - val_mae: 0.3704 - val_mse: 0.1671
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1071 - mae: 0.2521 - mse: 0.1071
64/87 [=====================>........] - ETA: 0s - loss: 0.0991 - mae: 0.2355 - mse: 0.0991
87/87 [==============================] - 0s 5ms/step - loss: 0.1022 - mae: 0.2377 - mse: 0.1022 - val_loss: 0.1618 - val_mae: 0.3619 - val_mse: 0.1618
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0764 - mae: 0.2055 - mse: 0.0764
64/87 [=====================>........] - ETA: 0s - loss: 0.0940 - mae: 0.2232 - mse: 0.0940
87/87 [==============================] - 0s 5ms/step - loss: 0.0847 - mae: 0.2114 - mse: 0.0847 - val_loss: 0.1494 - val_mae: 0.3334 - val_mse: 0.1494
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0957 - mae: 0.2094 - mse: 0.0957
64/87 [=====================>........] - ETA: 0s - loss: 0.0888 - mae: 0.2069 - mse: 0.0888
87/87 [==============================] - 0s 5ms/step - loss: 0.0996 - mae: 0.2165 - mse: 0.0996 - val_loss: 0.1414 - val_mae: 0.3150 - val_mse: 0.1414
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1069 - mae: 0.2333 - mse: 0.1069
64/87 [=====================>........] - ETA: 0s - loss: 0.0904 - mae: 0.2121 - mse: 0.0904
87/87 [==============================] - 0s 4ms/step - loss: 0.1002 - mae: 0.2241 - mse: 0.1002 - val_loss: 0.1450 - val_mae: 0.3434 - val_mse: 0.1450
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1086 - mae: 0.2462 - mse: 0.1086
64/87 [=====================>........] - ETA: 0s - loss: 0.1011 - mae: 0.2338 - mse: 0.1011
87/87 [==============================] - 0s 5ms/step - loss: 0.0940 - mae: 0.2284 - mse: 0.0940 - val_loss: 0.1404 - val_mae: 0.3426 - val_mse: 0.1404
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0907 - mae: 0.2172 - mse: 0.0907
64/87 [=====================>........] - ETA: 0s - loss: 0.0819 - mae: 0.2114 - mse: 0.0819
87/87 [==============================] - 0s 5ms/step - loss: 0.0844 - mae: 0.2136 - mse: 0.0844 - val_loss: 0.1218 - val_mae: 0.2953 - val_mse: 0.1218
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1126 - mae: 0.2380 - mse: 0.1126
64/87 [=====================>........] - ETA: 0s - loss: 0.0876 - mae: 0.2078 - mse: 0.0876
87/87 [==============================] - 0s 5ms/step - loss: 0.0855 - mae: 0.2085 - mse: 0.0855 - val_loss: 0.1184 - val_mae: 0.2915 - val_mse: 0.1184
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1032 - mae: 0.2249 - mse: 0.1032
64/87 [=====================>........] - ETA: 0s - loss: 0.0808 - mae: 0.1999 - mse: 0.0808
87/87 [==============================] - 1s 8ms/step - loss: 0.0894 - mae: 0.2157 - mse: 0.0894 - val_loss: 0.1227 - val_mae: 0.3101 - val_mse: 0.1227
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1048 - mae: 0.2316 - mse: 0.1048
64/87 [=====================>........] - ETA: 0s - loss: 0.0978 - mae: 0.2140 - mse: 0.0978
87/87 [==============================] - 1s 8ms/step - loss: 0.0848 - mae: 0.1968 - mse: 0.0848 - val_loss: 0.1229 - val_mae: 0.3072 - val_mse: 0.1229
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0332 - mae: 0.1309 - mse: 0.0332
64/87 [=====================>........] - ETA: 0s - loss: 0.0807 - mae: 0.1872 - mse: 0.0807
87/87 [==============================] - 1s 8ms/step - loss: 0.0882 - mae: 0.2040 - mse: 0.0882 - val_loss: 0.1244 - val_mae: 0.3063 - val_mse: 0.1244
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0762 - mae: 0.1813 - mse: 0.0762
64/87 [=====================>........] - ETA: 0s - loss: 0.0937 - mae: 0.2076 - mse: 0.0937
87/87 [==============================] - 1s 8ms/step - loss: 0.0863 - mae: 0.2053 - mse: 0.0863 - val_loss: 0.1250 - val_mae: 0.3128 - val_mse: 0.1250
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1260 - mae: 0.2558 - mse: 0.1260
64/87 [=====================>........] - ETA: 0s - loss: 0.0985 - mae: 0.2219 - mse: 0.0985
87/87 [==============================] - 1s 7ms/step - loss: 0.0865 - mae: 0.1997 - mse: 0.0865 - val_loss: 0.1230 - val_mae: 0.3135 - val_mse: 0.1230
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0931 - mae: 0.1985 - mse: 0.0931
64/87 [=====================>........] - ETA: 0s - loss: 0.0879 - mae: 0.2148 - mse: 0.0879
87/87 [==============================] - 1s 7ms/step - loss: 0.0737 - mae: 0.1964 - mse: 0.0737 - val_loss: 0.1176 - val_mae: 0.3028 - val_mse: 0.1176
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0897 - mae: 0.2145 - mse: 0.0897
64/87 [=====================>........] - ETA: 0s - loss: 0.0867 - mae: 0.2148 - mse: 0.0867
87/87 [==============================] - 1s 7ms/step - loss: 0.0772 - mae: 0.1979 - mse: 0.0772 - val_loss: 0.1131 - val_mae: 0.2865 - val_mse: 0.1131
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0714 - mae: 0.1842 - mse: 0.0714
64/87 [=====================>........] - ETA: 0s - loss: 0.0791 - mae: 0.1945 - mse: 0.0791
87/87 [==============================] - 1s 8ms/step - loss: 0.0829 - mae: 0.1967 - mse: 0.0829 - val_loss: 0.1147 - val_mae: 0.2922 - val_mse: 0.1147
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0501 - mae: 0.1574 - mse: 0.0501
64/87 [=====================>........] - ETA: 0s - loss: 0.0642 - mae: 0.1697 - mse: 0.0642
87/87 [==============================] - 1s 6ms/step - loss: 0.0802 - mae: 0.1866 - mse: 0.0802 - val_loss: 0.1120 - val_mae: 0.2867 - val_mse: 0.1120
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0247 - mae: 0.1118 - mse: 0.0247
64/87 [=====================>........] - ETA: 0s - loss: 0.0760 - mae: 0.1945 - mse: 0.0760
87/87 [==============================] - 1s 6ms/step - loss: 0.0855 - mae: 0.2047 - mse: 0.0855 - val_loss: 0.1166 - val_mae: 0.3084 - val_mse: 0.1166
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0821 - mae: 0.1734 - mse: 0.0821
64/87 [=====================>........] - ETA: 0s - loss: 0.0885 - mae: 0.1968 - mse: 0.0885
87/87 [==============================] - 1s 7ms/step - loss: 0.0948 - mae: 0.2117 - mse: 0.0948 - val_loss: 0.1206 - val_mae: 0.3196 - val_mse: 0.1206
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0901 - mae: 0.2118 - mse: 0.0901
64/87 [=====================>........] - ETA: 0s - loss: 0.0750 - mae: 0.1906 - mse: 0.0750
87/87 [==============================] - 1s 6ms/step - loss: 0.0741 - mae: 0.1927 - mse: 0.0741 - val_loss: 0.1085 - val_mae: 0.2882 - val_mse: 0.1085
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0828 - mae: 0.2017 - mse: 0.0828
64/87 [=====================>........] - ETA: 0s - loss: 0.0920 - mae: 0.2158 - mse: 0.0920
87/87 [==============================] - 1s 7ms/step - loss: 0.0862 - mae: 0.2049 - mse: 0.0862 - val_loss: 0.1046 - val_mae: 0.2664 - val_mse: 0.1046
Saving trained model...
98
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         85.18917847]
average prediction= [7.1512175]
baseline= 8.3
eachuser= [0. 0. 0. 0. 0. 6.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 14.198196411132812
['train-height-10.py', '0']
155 1
2_155_65_10_csi_a10_14.dat
155 3
155 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
155 9
155 10
155 11
155 12
155 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
155 17
2_155_65_10_csi_a10_13.dat
155 19
155 20
2_155_65_10_csi_a10_3.dat
155 22
155 23
155 24
2_155_65_10_csi_a10_4.dat
155 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
155 29
155 30
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
180 131
180 132
180 133
180 134
180 135
180 136
180 137
180 138
180 139
180 140
180 141
180 142
180 143
180 144
180 145
180 146
180 147
180 148
180 149
180 150
180 151
180 152
180 153
180 154
180 155
180 156
1_180_85_10_csi_a10_28.dat
180 158
180 159
180 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
180 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
180 178
180 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
180 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
180 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
173 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 170
 170 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 1s - loss: 0.8421 - mae: 0.8406 - mse: 0.8421
64/87 [=====================>........] - ETA: 0s - loss: 0.6622 - mae: 0.7243 - mse: 0.6622
87/87 [==============================] - 1s 12ms/step - loss: 0.6100 - mae: 0.6932 - mse: 0.6100 - val_loss: 0.1877 - val_mae: 0.3815 - val_mse: 0.1877
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2257 - mae: 0.4178 - mse: 0.2257
64/87 [=====================>........] - ETA: 0s - loss: 0.1786 - mae: 0.3628 - mse: 0.1786
87/87 [==============================] - 1s 7ms/step - loss: 0.1666 - mae: 0.3515 - mse: 0.1666 - val_loss: 0.1404 - val_mae: 0.2701 - val_mse: 0.1404
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2171 - mae: 0.3836 - mse: 0.2171
64/87 [=====================>........] - ETA: 0s - loss: 0.2136 - mae: 0.3651 - mse: 0.2136
87/87 [==============================] - 1s 7ms/step - loss: 0.1938 - mae: 0.3431 - mse: 0.1938 - val_loss: 0.1160 - val_mae: 0.2660 - val_mse: 0.1160
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1001 - mae: 0.2646 - mse: 0.1001
64/87 [=====================>........] - ETA: 0s - loss: 0.1233 - mae: 0.2868 - mse: 0.1233
87/87 [==============================] - 1s 7ms/step - loss: 0.1304 - mae: 0.2939 - mse: 0.1304 - val_loss: 0.1143 - val_mae: 0.2962 - val_mse: 0.1143
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1542 - mae: 0.3246 - mse: 0.1542
64/87 [=====================>........] - ETA: 0s - loss: 0.1392 - mae: 0.3091 - mse: 0.1392
87/87 [==============================] - 1s 7ms/step - loss: 0.1339 - mae: 0.3048 - mse: 0.1339 - val_loss: 0.1381 - val_mae: 0.3273 - val_mse: 0.1381
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0961 - mae: 0.2440 - mse: 0.0961
64/87 [=====================>........] - ETA: 0s - loss: 0.1178 - mae: 0.2738 - mse: 0.1178
87/87 [==============================] - 1s 7ms/step - loss: 0.1331 - mae: 0.2957 - mse: 0.1331 - val_loss: 0.1359 - val_mae: 0.3272 - val_mse: 0.1359
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1597 - mae: 0.3438 - mse: 0.1597
64/87 [=====================>........] - ETA: 0s - loss: 0.1230 - mae: 0.2845 - mse: 0.1230
87/87 [==============================] - 1s 7ms/step - loss: 0.1223 - mae: 0.2791 - mse: 0.1223 - val_loss: 0.1113 - val_mae: 0.2813 - val_mse: 0.1113
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1182 - mae: 0.2737 - mse: 0.1182
64/87 [=====================>........] - ETA: 0s - loss: 0.1333 - mae: 0.2916 - mse: 0.1333
87/87 [==============================] - 1s 6ms/step - loss: 0.1218 - mae: 0.2739 - mse: 0.1218 - val_loss: 0.0962 - val_mae: 0.2200 - val_mse: 0.0962
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0868 - mae: 0.2402 - mse: 0.0868
64/87 [=====================>........] - ETA: 0s - loss: 0.1181 - mae: 0.2665 - mse: 0.1181
87/87 [==============================] - 1s 7ms/step - loss: 0.1232 - mae: 0.2738 - mse: 0.1232 - val_loss: 0.0986 - val_mae: 0.2037 - val_mse: 0.0986
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0882 - mae: 0.2296 - mse: 0.0882
64/87 [=====================>........] - ETA: 0s - loss: 0.1090 - mae: 0.2601 - mse: 0.1090
87/87 [==============================] - 1s 8ms/step - loss: 0.1149 - mae: 0.2644 - mse: 0.1149 - val_loss: 0.1000 - val_mae: 0.2251 - val_mse: 0.1000
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0859 - mae: 0.2337 - mse: 0.0859
64/87 [=====================>........] - ETA: 0s - loss: 0.0895 - mae: 0.2328 - mse: 0.0895
87/87 [==============================] - 1s 6ms/step - loss: 0.0949 - mae: 0.2351 - mse: 0.0949 - val_loss: 0.1059 - val_mae: 0.2696 - val_mse: 0.1059
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0858 - mae: 0.2355 - mse: 0.0858
64/87 [=====================>........] - ETA: 0s - loss: 0.1076 - mae: 0.2552 - mse: 0.1076
87/87 [==============================] - 1s 6ms/step - loss: 0.1006 - mae: 0.2474 - mse: 0.1006 - val_loss: 0.1114 - val_mae: 0.2891 - val_mse: 0.1114
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1215 - mae: 0.2538 - mse: 0.1215
64/87 [=====================>........] - ETA: 0s - loss: 0.1006 - mae: 0.2317 - mse: 0.1006
87/87 [==============================] - 1s 6ms/step - loss: 0.1057 - mae: 0.2442 - mse: 0.1057 - val_loss: 0.1122 - val_mae: 0.2917 - val_mse: 0.1122
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1083 - mae: 0.2676 - mse: 0.1083
64/87 [=====================>........] - ETA: 0s - loss: 0.1101 - mae: 0.2630 - mse: 0.1101
87/87 [==============================] - 1s 7ms/step - loss: 0.1144 - mae: 0.2610 - mse: 0.1144 - val_loss: 0.1080 - val_mae: 0.2706 - val_mse: 0.1080
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0935 - mae: 0.2304 - mse: 0.0935
64/87 [=====================>........] - ETA: 0s - loss: 0.0965 - mae: 0.2183 - mse: 0.0965
87/87 [==============================] - 1s 6ms/step - loss: 0.0956 - mae: 0.2231 - mse: 0.0956 - val_loss: 0.1064 - val_mae: 0.2571 - val_mse: 0.1064
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0956 - mae: 0.2290 - mse: 0.0956
64/87 [=====================>........] - ETA: 0s - loss: 0.0877 - mae: 0.2170 - mse: 0.0877
87/87 [==============================] - 1s 6ms/step - loss: 0.0988 - mae: 0.2343 - mse: 0.0988 - val_loss: 0.1060 - val_mae: 0.2539 - val_mse: 0.1060
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1102 - mae: 0.2460 - mse: 0.1102
64/87 [=====================>........] - ETA: 0s - loss: 0.1087 - mae: 0.2546 - mse: 0.1087
87/87 [==============================] - 1s 6ms/step - loss: 0.0978 - mae: 0.2393 - mse: 0.0978 - val_loss: 0.1054 - val_mae: 0.2488 - val_mse: 0.1054
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1141 - mae: 0.2537 - mse: 0.1141
64/87 [=====================>........] - ETA: 0s - loss: 0.0907 - mae: 0.2135 - mse: 0.0907
87/87 [==============================] - 1s 6ms/step - loss: 0.0934 - mae: 0.2168 - mse: 0.0934 - val_loss: 0.1059 - val_mae: 0.2541 - val_mse: 0.1059
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0883 - mae: 0.2208 - mse: 0.0883
64/87 [=====================>........] - ETA: 0s - loss: 0.0923 - mae: 0.2214 - mse: 0.0923
87/87 [==============================] - 1s 6ms/step - loss: 0.0975 - mae: 0.2308 - mse: 0.0975 - val_loss: 0.1070 - val_mae: 0.2591 - val_mse: 0.1070
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0494 - mae: 0.1618 - mse: 0.0494
64/87 [=====================>........] - ETA: 0s - loss: 0.0826 - mae: 0.2060 - mse: 0.0826
87/87 [==============================] - 1s 7ms/step - loss: 0.0959 - mae: 0.2265 - mse: 0.0959 - val_loss: 0.1091 - val_mae: 0.2670 - val_mse: 0.1091
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0890 - mae: 0.2325 - mse: 0.0890
64/87 [=====================>........] - ETA: 0s - loss: 0.0897 - mae: 0.2190 - mse: 0.0897
87/87 [==============================] - 1s 8ms/step - loss: 0.0957 - mae: 0.2238 - mse: 0.0957 - val_loss: 0.1125 - val_mae: 0.2821 - val_mse: 0.1125
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0781 - mae: 0.2183 - mse: 0.0781
64/87 [=====================>........] - ETA: 0s - loss: 0.0840 - mae: 0.2135 - mse: 0.0840
87/87 [==============================] - 1s 8ms/step - loss: 0.0904 - mae: 0.2228 - mse: 0.0904 - val_loss: 0.1130 - val_mae: 0.2833 - val_mse: 0.1130
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0801 - mae: 0.2038 - mse: 0.0801
64/87 [=====================>........] - ETA: 0s - loss: 0.0874 - mae: 0.2213 - mse: 0.0874
87/87 [==============================] - 1s 8ms/step - loss: 0.0845 - mae: 0.2141 - mse: 0.0845 - val_loss: 0.1113 - val_mae: 0.2759 - val_mse: 0.1113
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0600 - mae: 0.1751 - mse: 0.0600
64/87 [=====================>........] - ETA: 0s - loss: 0.0830 - mae: 0.2146 - mse: 0.0830
87/87 [==============================] - 1s 8ms/step - loss: 0.0866 - mae: 0.2170 - mse: 0.0866 - val_loss: 0.1110 - val_mae: 0.2688 - val_mse: 0.1110
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0955 - mae: 0.2292 - mse: 0.0955
64/87 [=====================>........] - ETA: 0s - loss: 0.0850 - mae: 0.2108 - mse: 0.0850
87/87 [==============================] - 1s 8ms/step - loss: 0.0798 - mae: 0.2020 - mse: 0.0798 - val_loss: 0.1117 - val_mae: 0.2722 - val_mse: 0.1117
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0625 - mae: 0.1645 - mse: 0.0625
64/87 [=====================>........] - ETA: 0s - loss: 0.0724 - mae: 0.1897 - mse: 0.0724
87/87 [==============================] - 1s 8ms/step - loss: 0.0840 - mae: 0.2046 - mse: 0.0840 - val_loss: 0.1114 - val_mae: 0.2714 - val_mse: 0.1114
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1002 - mae: 0.2256 - mse: 0.1002
64/87 [=====================>........] - ETA: 0s - loss: 0.0901 - mae: 0.2152 - mse: 0.0901
87/87 [==============================] - 1s 7ms/step - loss: 0.0921 - mae: 0.2244 - mse: 0.0921 - val_loss: 0.1148 - val_mae: 0.2899 - val_mse: 0.1148
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0805 - mae: 0.2129 - mse: 0.0805
64/87 [=====================>........] - ETA: 0s - loss: 0.0888 - mae: 0.2259 - mse: 0.0888
87/87 [==============================] - 1s 6ms/step - loss: 0.0858 - mae: 0.2216 - mse: 0.0858 - val_loss: 0.1121 - val_mae: 0.2782 - val_mse: 0.1121
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0526 - mae: 0.1624 - mse: 0.0526
64/87 [=====================>........] - ETA: 0s - loss: 0.0828 - mae: 0.2075 - mse: 0.0828
87/87 [==============================] - 1s 7ms/step - loss: 0.0865 - mae: 0.2125 - mse: 0.0865 - val_loss: 0.1115 - val_mae: 0.2683 - val_mse: 0.1115
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0905 - mae: 0.2174 - mse: 0.0905
64/87 [=====================>........] - ETA: 0s - loss: 0.0960 - mae: 0.2235 - mse: 0.0960
87/87 [==============================] - 1s 7ms/step - loss: 0.0873 - mae: 0.2130 - mse: 0.0873 - val_loss: 0.1120 - val_mae: 0.2685 - val_mse: 0.1120
Saving trained model...
98
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         50.36181641]
average prediction= [5.380973]
baseline= 7.22
eachuser= [0. 0. 0. 0. 0. 4.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 12.5904541015625
['train-height-10.py', '0']
155 1
2_155_65_10_csi_a10_14.dat
155 3
155 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
155 9
155 10
155 11
155 12
155 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
155 17
2_155_65_10_csi_a10_13.dat
155 19
155 20
2_155_65_10_csi_a10_3.dat
155 22
155 23
155 24
2_155_65_10_csi_a10_4.dat
155 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
155 29
155 30
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
180 131
180 132
180 133
180 134
180 135
180 136
180 137
180 138
180 139
180 140
180 141
180 142
180 143
180 144
180 145
180 146
180 147
180 148
180 149
180 150
180 151
180 152
180 153
180 154
180 155
180 156
1_180_85_10_csi_a10_28.dat
180 158
180 159
180 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
180 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
180 178
180 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
180 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
180 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
173 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 170
 170 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.6260 - mae: 0.7055 - mse: 0.6260
64/87 [=====================>........] - ETA: 0s - loss: 0.4211 - mae: 0.5378 - mse: 0.4211
87/87 [==============================] - 1s 12ms/step - loss: 0.3578 - mae: 0.4938 - mse: 0.3578 - val_loss: 0.1019 - val_mae: 0.2685 - val_mse: 0.1019
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1864 - mae: 0.3834 - mse: 0.1864
64/87 [=====================>........] - ETA: 0s - loss: 0.2155 - mae: 0.3821 - mse: 0.2155
87/87 [==============================] - 1s 6ms/step - loss: 0.2131 - mae: 0.3769 - mse: 0.2131 - val_loss: 0.1006 - val_mae: 0.2640 - val_mse: 0.1006
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1790 - mae: 0.3520 - mse: 0.1790
64/87 [=====================>........] - ETA: 0s - loss: 0.1663 - mae: 0.3448 - mse: 0.1663
87/87 [==============================] - 1s 7ms/step - loss: 0.1586 - mae: 0.3366 - mse: 0.1586 - val_loss: 0.1219 - val_mae: 0.2761 - val_mse: 0.1219
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1821 - mae: 0.3767 - mse: 0.1821
64/87 [=====================>........] - ETA: 0s - loss: 0.1839 - mae: 0.3721 - mse: 0.1839
87/87 [==============================] - 1s 8ms/step - loss: 0.1674 - mae: 0.3496 - mse: 0.1674 - val_loss: 0.1501 - val_mae: 0.3300 - val_mse: 0.1501
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1028 - mae: 0.2628 - mse: 0.1028
64/87 [=====================>........] - ETA: 0s - loss: 0.1267 - mae: 0.2922 - mse: 0.1267
87/87 [==============================] - 1s 8ms/step - loss: 0.1345 - mae: 0.3051 - mse: 0.1345 - val_loss: 0.1291 - val_mae: 0.2942 - val_mse: 0.1291
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1233 - mae: 0.2720 - mse: 0.1233
64/87 [=====================>........] - ETA: 0s - loss: 0.1258 - mae: 0.2859 - mse: 0.1258
87/87 [==============================] - 1s 7ms/step - loss: 0.1336 - mae: 0.3016 - mse: 0.1336 - val_loss: 0.0938 - val_mae: 0.2332 - val_mse: 0.0938
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1564 - mae: 0.3371 - mse: 0.1564
64/87 [=====================>........] - ETA: 0s - loss: 0.1258 - mae: 0.2831 - mse: 0.1258
87/87 [==============================] - 1s 7ms/step - loss: 0.1194 - mae: 0.2714 - mse: 0.1194 - val_loss: 0.0825 - val_mae: 0.2099 - val_mse: 0.0825
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1180 - mae: 0.2816 - mse: 0.1180
64/87 [=====================>........] - ETA: 0s - loss: 0.1337 - mae: 0.2964 - mse: 0.1337
87/87 [==============================] - 1s 7ms/step - loss: 0.1298 - mae: 0.2906 - mse: 0.1298 - val_loss: 0.0827 - val_mae: 0.2177 - val_mse: 0.0827
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1218 - mae: 0.2687 - mse: 0.1218
64/87 [=====================>........] - ETA: 0s - loss: 0.1315 - mae: 0.2924 - mse: 0.1315
87/87 [==============================] - 1s 8ms/step - loss: 0.1248 - mae: 0.2832 - mse: 0.1248 - val_loss: 0.1079 - val_mae: 0.2587 - val_mse: 0.1079
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1444 - mae: 0.3275 - mse: 0.1444
64/87 [=====================>........] - ETA: 0s - loss: 0.1188 - mae: 0.2893 - mse: 0.1188
87/87 [==============================] - 1s 7ms/step - loss: 0.1141 - mae: 0.2794 - mse: 0.1141 - val_loss: 0.1190 - val_mae: 0.2848 - val_mse: 0.1190
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1171 - mae: 0.2675 - mse: 0.1171
64/87 [=====================>........] - ETA: 0s - loss: 0.1162 - mae: 0.2709 - mse: 0.1162
87/87 [==============================] - 1s 7ms/step - loss: 0.1177 - mae: 0.2741 - mse: 0.1177 - val_loss: 0.0983 - val_mae: 0.2467 - val_mse: 0.0983
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1196 - mae: 0.2825 - mse: 0.1196
64/87 [=====================>........] - ETA: 0s - loss: 0.1173 - mae: 0.2709 - mse: 0.1173
87/87 [==============================] - 1s 6ms/step - loss: 0.1119 - mae: 0.2602 - mse: 0.1119 - val_loss: 0.0864 - val_mae: 0.2323 - val_mse: 0.0864
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0965 - mae: 0.2409 - mse: 0.0965
64/87 [=====================>........] - ETA: 0s - loss: 0.0938 - mae: 0.2302 - mse: 0.0938
87/87 [==============================] - 1s 7ms/step - loss: 0.1016 - mae: 0.2417 - mse: 0.1016 - val_loss: 0.0813 - val_mae: 0.2251 - val_mse: 0.0813
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1010 - mae: 0.2361 - mse: 0.1010
64/87 [=====================>........] - ETA: 0s - loss: 0.1126 - mae: 0.2575 - mse: 0.1126
87/87 [==============================] - 1s 6ms/step - loss: 0.1237 - mae: 0.2720 - mse: 0.1237 - val_loss: 0.0859 - val_mae: 0.2322 - val_mse: 0.0859
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1234 - mae: 0.2780 - mse: 0.1234
64/87 [=====================>........] - ETA: 0s - loss: 0.1051 - mae: 0.2506 - mse: 0.1051
87/87 [==============================] - 0s 6ms/step - loss: 0.1033 - mae: 0.2446 - mse: 0.1033 - val_loss: 0.0958 - val_mae: 0.2470 - val_mse: 0.0958
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0839 - mae: 0.2190 - mse: 0.0839
64/87 [=====================>........] - ETA: 0s - loss: 0.0870 - mae: 0.2313 - mse: 0.0870
87/87 [==============================] - 0s 5ms/step - loss: 0.1004 - mae: 0.2504 - mse: 0.1004 - val_loss: 0.0917 - val_mae: 0.2410 - val_mse: 0.0917
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0951 - mae: 0.2333 - mse: 0.0951
64/87 [=====================>........] - ETA: 0s - loss: 0.1163 - mae: 0.2582 - mse: 0.1163
87/87 [==============================] - 0s 5ms/step - loss: 0.1091 - mae: 0.2529 - mse: 0.1091 - val_loss: 0.0897 - val_mae: 0.2374 - val_mse: 0.0897
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1287 - mae: 0.2747 - mse: 0.1287
64/87 [=====================>........] - ETA: 0s - loss: 0.1070 - mae: 0.2549 - mse: 0.1070
87/87 [==============================] - 0s 5ms/step - loss: 0.1035 - mae: 0.2505 - mse: 0.1035 - val_loss: 0.0855 - val_mae: 0.2297 - val_mse: 0.0855
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1096 - mae: 0.2553 - mse: 0.1096
64/87 [=====================>........] - ETA: 0s - loss: 0.0937 - mae: 0.2367 - mse: 0.0937
87/87 [==============================] - 0s 5ms/step - loss: 0.1056 - mae: 0.2518 - mse: 0.1056 - val_loss: 0.0930 - val_mae: 0.2461 - val_mse: 0.0930
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1052 - mae: 0.2727 - mse: 0.1052
64/87 [=====================>........] - ETA: 0s - loss: 0.1105 - mae: 0.2678 - mse: 0.1105
87/87 [==============================] - 0s 5ms/step - loss: 0.1056 - mae: 0.2561 - mse: 0.1056 - val_loss: 0.1018 - val_mae: 0.2671 - val_mse: 0.1018
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1092 - mae: 0.2599 - mse: 0.1092
64/87 [=====================>........] - ETA: 0s - loss: 0.1007 - mae: 0.2439 - mse: 0.1007
87/87 [==============================] - 0s 5ms/step - loss: 0.0969 - mae: 0.2405 - mse: 0.0969 - val_loss: 0.0988 - val_mae: 0.2616 - val_mse: 0.0988
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1474 - mae: 0.3168 - mse: 0.1474
64/87 [=====================>........] - ETA: 0s - loss: 0.1219 - mae: 0.2798 - mse: 0.1219
87/87 [==============================] - 0s 5ms/step - loss: 0.1094 - mae: 0.2583 - mse: 0.1094 - val_loss: 0.0992 - val_mae: 0.2606 - val_mse: 0.0992
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1189 - mae: 0.2729 - mse: 0.1189
64/87 [=====================>........] - ETA: 0s - loss: 0.0945 - mae: 0.2310 - mse: 0.0945
87/87 [==============================] - 0s 5ms/step - loss: 0.1137 - mae: 0.2535 - mse: 0.1137 - val_loss: 0.0885 - val_mae: 0.2359 - val_mse: 0.0885
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1261 - mae: 0.2835 - mse: 0.1261
64/87 [=====================>........] - ETA: 0s - loss: 0.1020 - mae: 0.2389 - mse: 0.1020
87/87 [==============================] - 0s 5ms/step - loss: 0.1051 - mae: 0.2507 - mse: 0.1051 - val_loss: 0.0988 - val_mae: 0.2582 - val_mse: 0.0988
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0971 - mae: 0.2349 - mse: 0.0971
64/87 [=====================>........] - ETA: 0s - loss: 0.0970 - mae: 0.2350 - mse: 0.0970
87/87 [==============================] - 0s 5ms/step - loss: 0.1004 - mae: 0.2395 - mse: 0.1004 - val_loss: 0.0917 - val_mae: 0.2452 - val_mse: 0.0917
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1046 - mae: 0.2519 - mse: 0.1046
64/87 [=====================>........] - ETA: 0s - loss: 0.1039 - mae: 0.2493 - mse: 0.1039
87/87 [==============================] - 0s 5ms/step - loss: 0.0986 - mae: 0.2456 - mse: 0.0986 - val_loss: 0.0914 - val_mae: 0.2471 - val_mse: 0.0914
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1089 - mae: 0.2614 - mse: 0.1089
64/87 [=====================>........] - ETA: 0s - loss: 0.1003 - mae: 0.2386 - mse: 0.1003
87/87 [==============================] - 0s 5ms/step - loss: 0.0975 - mae: 0.2293 - mse: 0.0975 - val_loss: 0.0755 - val_mae: 0.2145 - val_mse: 0.0755
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1104 - mae: 0.2602 - mse: 0.1104
64/87 [=====================>........] - ETA: 0s - loss: 0.1055 - mae: 0.2426 - mse: 0.1055
87/87 [==============================] - 0s 5ms/step - loss: 0.1060 - mae: 0.2483 - mse: 0.1060 - val_loss: 0.0765 - val_mae: 0.2228 - val_mse: 0.0765
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0933 - mae: 0.2321 - mse: 0.0933
64/87 [=====================>........] - ETA: 0s - loss: 0.0890 - mae: 0.2243 - mse: 0.0890
87/87 [==============================] - 0s 5ms/step - loss: 0.1006 - mae: 0.2397 - mse: 0.1006 - val_loss: 0.0925 - val_mae: 0.2627 - val_mse: 0.0925
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1064 - mae: 0.2482 - mse: 0.1064
64/87 [=====================>........] - ETA: 0s - loss: 0.0869 - mae: 0.2184 - mse: 0.0869
87/87 [==============================] - 0s 5ms/step - loss: 0.0927 - mae: 0.2322 - mse: 0.0927 - val_loss: 0.1019 - val_mae: 0.2822 - val_mse: 0.1019
Saving trained model...
98
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         23.68429565]
average prediction= [6.138468]
baseline= 6.22
eachuser= [0. 0. 0. 0. 0. 2.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 11.842147827148438
['train-height-10.py', '0']
155 1
2_155_65_10_csi_a10_14.dat
155 3
155 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
155 9
155 10
155 11
155 12
155 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
155 17
2_155_65_10_csi_a10_13.dat
155 19
155 20
2_155_65_10_csi_a10_3.dat
155 22
155 23
155 24
2_155_65_10_csi_a10_4.dat
155 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
155 29
155 30
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
180 131
180 132
180 133
180 134
180 135
180 136
180 137
180 138
180 139
180 140
180 141
180 142
180 143
180 144
180 145
180 146
180 147
180 148
180 149
180 150
180 151
180 152
180 153
180 154
180 155
180 156
1_180_85_10_csi_a10_28.dat
180 158
180 159
180 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
180 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
180 178
180 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
180 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
180 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
173 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 170
 170 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.5974 - mae: 0.7107 - mse: 0.5974
64/87 [=====================>........] - ETA: 0s - loss: 0.4915 - mae: 0.6341 - mse: 0.4915
87/87 [==============================] - 1s 10ms/step - loss: 0.4445 - mae: 0.6005 - mse: 0.4445 - val_loss: 0.2364 - val_mae: 0.4477 - val_mse: 0.2364
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2581 - mae: 0.4560 - mse: 0.2581
64/87 [=====================>........] - ETA: 0s - loss: 0.1965 - mae: 0.3762 - mse: 0.1965
87/87 [==============================] - 1s 6ms/step - loss: 0.1699 - mae: 0.3456 - mse: 0.1699 - val_loss: 0.0933 - val_mae: 0.2235 - val_mse: 0.0933
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1501 - mae: 0.2809 - mse: 0.1501
64/87 [=====================>........] - ETA: 0s - loss: 0.1625 - mae: 0.3107 - mse: 0.1625
87/87 [==============================] - 0s 5ms/step - loss: 0.1999 - mae: 0.3335 - mse: 0.1999 - val_loss: 0.0922 - val_mae: 0.2054 - val_mse: 0.0922
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0959 - mae: 0.2368 - mse: 0.0959
64/87 [=====================>........] - ETA: 0s - loss: 0.1435 - mae: 0.2764 - mse: 0.1435
87/87 [==============================] - 0s 5ms/step - loss: 0.1354 - mae: 0.2708 - mse: 0.1354 - val_loss: 0.0973 - val_mae: 0.2517 - val_mse: 0.0973
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0723 - mae: 0.2314 - mse: 0.0723
64/87 [=====================>........] - ETA: 0s - loss: 0.0996 - mae: 0.2638 - mse: 0.0996
87/87 [==============================] - 0s 5ms/step - loss: 0.1079 - mae: 0.2730 - mse: 0.1079 - val_loss: 0.1280 - val_mae: 0.3124 - val_mse: 0.1280
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0937 - mae: 0.2407 - mse: 0.0937
64/87 [=====================>........] - ETA: 0s - loss: 0.1152 - mae: 0.2728 - mse: 0.1152
87/87 [==============================] - 0s 5ms/step - loss: 0.1233 - mae: 0.2839 - mse: 0.1233 - val_loss: 0.1305 - val_mae: 0.3173 - val_mse: 0.1305
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0967 - mae: 0.2529 - mse: 0.0967
64/87 [=====================>........] - ETA: 0s - loss: 0.1145 - mae: 0.2798 - mse: 0.1145
87/87 [==============================] - 0s 5ms/step - loss: 0.1078 - mae: 0.2722 - mse: 0.1078 - val_loss: 0.1064 - val_mae: 0.2690 - val_mse: 0.1064
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0756 - mae: 0.2176 - mse: 0.0756
64/87 [=====================>........] - ETA: 0s - loss: 0.1103 - mae: 0.2656 - mse: 0.1103
87/87 [==============================] - 0s 5ms/step - loss: 0.1013 - mae: 0.2526 - mse: 0.1013 - val_loss: 0.0819 - val_mae: 0.2055 - val_mse: 0.0819
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0852 - mae: 0.2040 - mse: 0.0852
64/87 [=====================>........] - ETA: 0s - loss: 0.1167 - mae: 0.2514 - mse: 0.1167
87/87 [==============================] - 0s 5ms/step - loss: 0.1001 - mae: 0.2388 - mse: 0.1001 - val_loss: 0.0775 - val_mae: 0.1863 - val_mse: 0.0775
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1236 - mae: 0.2803 - mse: 0.1236
64/87 [=====================>........] - ETA: 0s - loss: 0.1040 - mae: 0.2521 - mse: 0.1040
87/87 [==============================] - 0s 5ms/step - loss: 0.1044 - mae: 0.2478 - mse: 0.1044 - val_loss: 0.0775 - val_mae: 0.1931 - val_mse: 0.0775
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1172 - mae: 0.2400 - mse: 0.1172
64/87 [=====================>........] - ETA: 0s - loss: 0.1026 - mae: 0.2232 - mse: 0.1026
87/87 [==============================] - 0s 5ms/step - loss: 0.1015 - mae: 0.2281 - mse: 0.1015 - val_loss: 0.0871 - val_mae: 0.2242 - val_mse: 0.0871
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1018 - mae: 0.2477 - mse: 0.1018
64/87 [=====================>........] - ETA: 0s - loss: 0.0883 - mae: 0.2293 - mse: 0.0883
87/87 [==============================] - 0s 5ms/step - loss: 0.0939 - mae: 0.2366 - mse: 0.0939 - val_loss: 0.0948 - val_mae: 0.2467 - val_mse: 0.0948
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1222 - mae: 0.2788 - mse: 0.1222
64/87 [=====================>........] - ETA: 0s - loss: 0.0952 - mae: 0.2338 - mse: 0.0952
87/87 [==============================] - 0s 5ms/step - loss: 0.1063 - mae: 0.2491 - mse: 0.1063 - val_loss: 0.0932 - val_mae: 0.2432 - val_mse: 0.0932
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0693 - mae: 0.1979 - mse: 0.0693
64/87 [=====================>........] - ETA: 0s - loss: 0.0868 - mae: 0.2232 - mse: 0.0868
87/87 [==============================] - 1s 6ms/step - loss: 0.0880 - mae: 0.2265 - mse: 0.0880 - val_loss: 0.0860 - val_mae: 0.2264 - val_mse: 0.0860
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0468 - mae: 0.1632 - mse: 0.0468
64/87 [=====================>........] - ETA: 0s - loss: 0.0708 - mae: 0.1966 - mse: 0.0708
87/87 [==============================] - 1s 7ms/step - loss: 0.0890 - mae: 0.2134 - mse: 0.0890 - val_loss: 0.0787 - val_mae: 0.2065 - val_mse: 0.0787
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0675 - mae: 0.1798 - mse: 0.0675
64/87 [=====================>........] - ETA: 0s - loss: 0.0885 - mae: 0.2122 - mse: 0.0885
87/87 [==============================] - 1s 7ms/step - loss: 0.0849 - mae: 0.2098 - mse: 0.0849 - val_loss: 0.0788 - val_mae: 0.2052 - val_mse: 0.0788
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0679 - mae: 0.1965 - mse: 0.0679
64/87 [=====================>........] - ETA: 0s - loss: 0.0950 - mae: 0.2347 - mse: 0.0950
87/87 [==============================] - 1s 7ms/step - loss: 0.0934 - mae: 0.2313 - mse: 0.0934 - val_loss: 0.0781 - val_mae: 0.2039 - val_mse: 0.0781
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0945 - mae: 0.2251 - mse: 0.0945
64/87 [=====================>........] - ETA: 0s - loss: 0.0803 - mae: 0.2086 - mse: 0.0803
87/87 [==============================] - 1s 7ms/step - loss: 0.0837 - mae: 0.2091 - mse: 0.0837 - val_loss: 0.0821 - val_mae: 0.2159 - val_mse: 0.0821
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1221 - mae: 0.2769 - mse: 0.1221
64/87 [=====================>........] - ETA: 0s - loss: 0.0835 - mae: 0.2174 - mse: 0.0835
87/87 [==============================] - 1s 7ms/step - loss: 0.0923 - mae: 0.2254 - mse: 0.0923 - val_loss: 0.0861 - val_mae: 0.2270 - val_mse: 0.0861
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0918 - mae: 0.2309 - mse: 0.0918
64/87 [=====================>........] - ETA: 0s - loss: 0.0850 - mae: 0.2194 - mse: 0.0850
87/87 [==============================] - 1s 7ms/step - loss: 0.0899 - mae: 0.2293 - mse: 0.0899 - val_loss: 0.0886 - val_mae: 0.2331 - val_mse: 0.0886
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1096 - mae: 0.2554 - mse: 0.1096
64/87 [=====================>........] - ETA: 0s - loss: 0.1073 - mae: 0.2439 - mse: 0.1073
87/87 [==============================] - 1s 7ms/step - loss: 0.0990 - mae: 0.2355 - mse: 0.0990 - val_loss: 0.0835 - val_mae: 0.2182 - val_mse: 0.0835
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0906 - mae: 0.2297 - mse: 0.0906
64/87 [=====================>........] - ETA: 0s - loss: 0.0672 - mae: 0.1959 - mse: 0.0672
87/87 [==============================] - 1s 7ms/step - loss: 0.0849 - mae: 0.2157 - mse: 0.0849 - val_loss: 0.0753 - val_mae: 0.1904 - val_mse: 0.0753
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1041 - mae: 0.2102 - mse: 0.1041
64/87 [=====================>........] - ETA: 0s - loss: 0.0926 - mae: 0.2090 - mse: 0.0926
87/87 [==============================] - 1s 7ms/step - loss: 0.0886 - mae: 0.2051 - mse: 0.0886 - val_loss: 0.0812 - val_mae: 0.2050 - val_mse: 0.0812
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1007 - mae: 0.2122 - mse: 0.1007
64/87 [=====================>........] - ETA: 0s - loss: 0.0942 - mae: 0.2156 - mse: 0.0942
87/87 [==============================] - 1s 7ms/step - loss: 0.0880 - mae: 0.2108 - mse: 0.0880 - val_loss: 0.0866 - val_mae: 0.2172 - val_mse: 0.0866
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0825 - mae: 0.2228 - mse: 0.0825
64/87 [=====================>........] - ETA: 0s - loss: 0.0631 - mae: 0.1844 - mse: 0.0631
87/87 [==============================] - 1s 7ms/step - loss: 0.0777 - mae: 0.2051 - mse: 0.0777 - val_loss: 0.0791 - val_mae: 0.1963 - val_mse: 0.0791
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0534 - mae: 0.1623 - mse: 0.0534
64/87 [=====================>........] - ETA: 0s - loss: 0.0727 - mae: 0.1947 - mse: 0.0727
87/87 [==============================] - 1s 7ms/step - loss: 0.0744 - mae: 0.1881 - mse: 0.0744 - val_loss: 0.0823 - val_mae: 0.2132 - val_mse: 0.0823
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0827 - mae: 0.1904 - mse: 0.0827
64/87 [=====================>........] - ETA: 0s - loss: 0.0843 - mae: 0.2102 - mse: 0.0843
87/87 [==============================] - 1s 7ms/step - loss: 0.0811 - mae: 0.2033 - mse: 0.0811 - val_loss: 0.0905 - val_mae: 0.2379 - val_mse: 0.0905
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0768 - mae: 0.1917 - mse: 0.0768
64/87 [=====================>........] - ETA: 0s - loss: 0.0658 - mae: 0.1778 - mse: 0.0658
87/87 [==============================] - 1s 7ms/step - loss: 0.0802 - mae: 0.1999 - mse: 0.0802 - val_loss: 0.0914 - val_mae: 0.2453 - val_mse: 0.0914
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1117 - mae: 0.2583 - mse: 0.1117
64/87 [=====================>........] - ETA: 0s - loss: 0.0780 - mae: 0.2080 - mse: 0.0780
87/87 [==============================] - 1s 7ms/step - loss: 0.0728 - mae: 0.1970 - mse: 0.0728 - val_loss: 0.0886 - val_mae: 0.2380 - val_mse: 0.0886
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0584 - mae: 0.1674 - mse: 0.0584
64/87 [=====================>........] - ETA: 0s - loss: 0.0674 - mae: 0.1794 - mse: 0.0674
87/87 [==============================] - 1s 7ms/step - loss: 0.0764 - mae: 0.1916 - mse: 0.0764 - val_loss: 0.0755 - val_mae: 0.1961 - val_mse: 0.0755
Saving trained model...
98
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         81.30386353]
average prediction= [6.071446]
baseline= 8.42
eachuser= [0. 0. 0. 0. 0. 6.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 13.550643920898438
['train-height-10.py', '0']
155 1
2_155_65_10_csi_a10_14.dat
155 3
155 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
155 9
155 10
155 11
155 12
155 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
155 17
2_155_65_10_csi_a10_13.dat
155 19
155 20
2_155_65_10_csi_a10_3.dat
155 22
155 23
155 24
2_155_65_10_csi_a10_4.dat
155 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
155 29
155 30
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
180 131
180 132
180 133
180 134
180 135
180 136
180 137
180 138
180 139
180 140
180 141
180 142
180 143
180 144
180 145
180 146
180 147
180 148
180 149
180 150
180 151
180 152
180 153
180 154
180 155
180 156
1_180_85_10_csi_a10_28.dat
180 158
180 159
180 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
180 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
180 178
180 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
180 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
180 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
173 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 170
 170 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.6185 - mae: 0.7195 - mse: 0.6185
64/87 [=====================>........] - ETA: 0s - loss: 0.4880 - mae: 0.6170 - mse: 0.4880
87/87 [==============================] - 1s 10ms/step - loss: 0.4783 - mae: 0.6182 - mse: 0.4783 - val_loss: 0.2029 - val_mae: 0.3963 - val_mse: 0.2029
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1710 - mae: 0.3316 - mse: 0.1710
64/87 [=====================>........] - ETA: 0s - loss: 0.1626 - mae: 0.3311 - mse: 0.1626
87/87 [==============================] - 1s 6ms/step - loss: 0.1530 - mae: 0.3252 - mse: 0.1530 - val_loss: 0.1740 - val_mae: 0.3536 - val_mse: 0.1740
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2536 - mae: 0.4353 - mse: 0.2536
64/87 [=====================>........] - ETA: 0s - loss: 0.2331 - mae: 0.4211 - mse: 0.2331
87/87 [==============================] - 1s 6ms/step - loss: 0.2025 - mae: 0.3920 - mse: 0.2025 - val_loss: 0.1629 - val_mae: 0.3327 - val_mse: 0.1629
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1557 - mae: 0.3234 - mse: 0.1557
64/87 [=====================>........] - ETA: 0s - loss: 0.1342 - mae: 0.3068 - mse: 0.1342
87/87 [==============================] - 0s 5ms/step - loss: 0.1287 - mae: 0.2986 - mse: 0.1287 - val_loss: 0.1561 - val_mae: 0.3473 - val_mse: 0.1561
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0705 - mae: 0.2068 - mse: 0.0705
64/87 [=====================>........] - ETA: 0s - loss: 0.1112 - mae: 0.2614 - mse: 0.1112
87/87 [==============================] - 0s 4ms/step - loss: 0.1091 - mae: 0.2625 - mse: 0.1091 - val_loss: 0.1703 - val_mae: 0.3761 - val_mse: 0.1703
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1089 - mae: 0.2784 - mse: 0.1089
64/87 [=====================>........] - ETA: 0s - loss: 0.1133 - mae: 0.2856 - mse: 0.1133
87/87 [==============================] - 0s 5ms/step - loss: 0.1153 - mae: 0.2812 - mse: 0.1153 - val_loss: 0.1784 - val_mae: 0.3815 - val_mse: 0.1784
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0955 - mae: 0.2405 - mse: 0.0955
64/87 [=====================>........] - ETA: 0s - loss: 0.1284 - mae: 0.2744 - mse: 0.1284
87/87 [==============================] - 0s 5ms/step - loss: 0.1146 - mae: 0.2607 - mse: 0.1146 - val_loss: 0.1784 - val_mae: 0.3723 - val_mse: 0.1784
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1060 - mae: 0.2536 - mse: 0.1060
64/87 [=====================>........] - ETA: 0s - loss: 0.0907 - mae: 0.2315 - mse: 0.0907
87/87 [==============================] - 0s 5ms/step - loss: 0.1035 - mae: 0.2401 - mse: 0.1035 - val_loss: 0.1766 - val_mae: 0.3509 - val_mse: 0.1766
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0937 - mae: 0.2339 - mse: 0.0937
64/87 [=====================>........] - ETA: 0s - loss: 0.1004 - mae: 0.2175 - mse: 0.1004
87/87 [==============================] - 0s 5ms/step - loss: 0.0943 - mae: 0.2089 - mse: 0.0943 - val_loss: 0.1618 - val_mae: 0.3347 - val_mse: 0.1618
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1346 - mae: 0.2673 - mse: 0.1346
64/87 [=====================>........] - ETA: 0s - loss: 0.0873 - mae: 0.2111 - mse: 0.0873
87/87 [==============================] - 0s 5ms/step - loss: 0.0829 - mae: 0.2096 - mse: 0.0829 - val_loss: 0.1438 - val_mae: 0.3185 - val_mse: 0.1438
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0722 - mae: 0.1870 - mse: 0.0722
64/87 [=====================>........] - ETA: 0s - loss: 0.0932 - mae: 0.2143 - mse: 0.0932
87/87 [==============================] - 0s 5ms/step - loss: 0.0831 - mae: 0.2038 - mse: 0.0831 - val_loss: 0.1339 - val_mae: 0.3101 - val_mse: 0.1339
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1318 - mae: 0.2756 - mse: 0.1318
64/87 [=====================>........] - ETA: 0s - loss: 0.0819 - mae: 0.2025 - mse: 0.0819
87/87 [==============================] - 0s 5ms/step - loss: 0.0903 - mae: 0.2194 - mse: 0.0903 - val_loss: 0.1278 - val_mae: 0.3027 - val_mse: 0.1278
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0998 - mae: 0.2382 - mse: 0.0998
64/87 [=====================>........] - ETA: 0s - loss: 0.0812 - mae: 0.2099 - mse: 0.0812
87/87 [==============================] - 0s 5ms/step - loss: 0.0896 - mae: 0.2201 - mse: 0.0896 - val_loss: 0.1241 - val_mae: 0.2964 - val_mse: 0.1241
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0649 - mae: 0.1837 - mse: 0.0649
64/87 [=====================>........] - ETA: 0s - loss: 0.0763 - mae: 0.2050 - mse: 0.0763
87/87 [==============================] - 0s 5ms/step - loss: 0.0964 - mae: 0.2234 - mse: 0.0964 - val_loss: 0.1235 - val_mae: 0.2968 - val_mse: 0.1235
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0714 - mae: 0.2127 - mse: 0.0714
64/87 [=====================>........] - ETA: 0s - loss: 0.0734 - mae: 0.2013 - mse: 0.0734
87/87 [==============================] - 0s 5ms/step - loss: 0.0791 - mae: 0.2107 - mse: 0.0791 - val_loss: 0.1287 - val_mae: 0.3174 - val_mse: 0.1287
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0855 - mae: 0.2229 - mse: 0.0855
64/87 [=====================>........] - ETA: 0s - loss: 0.0860 - mae: 0.2165 - mse: 0.0860
87/87 [==============================] - 0s 5ms/step - loss: 0.0813 - mae: 0.2102 - mse: 0.0813 - val_loss: 0.1282 - val_mae: 0.3159 - val_mse: 0.1282
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1103 - mae: 0.2438 - mse: 0.1103
64/87 [=====================>........] - ETA: 0s - loss: 0.0799 - mae: 0.2031 - mse: 0.0799
87/87 [==============================] - 0s 5ms/step - loss: 0.0843 - mae: 0.2098 - mse: 0.0843 - val_loss: 0.1274 - val_mae: 0.3037 - val_mse: 0.1274
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1344 - mae: 0.2594 - mse: 0.1344
64/87 [=====================>........] - ETA: 0s - loss: 0.1181 - mae: 0.2391 - mse: 0.1181
87/87 [==============================] - 0s 5ms/step - loss: 0.1016 - mae: 0.2252 - mse: 0.1016 - val_loss: 0.1279 - val_mae: 0.3122 - val_mse: 0.1279
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0984 - mae: 0.2195 - mse: 0.0984
64/87 [=====================>........] - ETA: 0s - loss: 0.0938 - mae: 0.2167 - mse: 0.0938
87/87 [==============================] - 0s 5ms/step - loss: 0.0901 - mae: 0.2116 - mse: 0.0901 - val_loss: 0.1266 - val_mae: 0.3062 - val_mse: 0.1266
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0938 - mae: 0.2279 - mse: 0.0938
64/87 [=====================>........] - ETA: 0s - loss: 0.0746 - mae: 0.1967 - mse: 0.0746
87/87 [==============================] - 0s 5ms/step - loss: 0.0752 - mae: 0.1992 - mse: 0.0752 - val_loss: 0.1264 - val_mae: 0.3045 - val_mse: 0.1264
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0720 - mae: 0.1887 - mse: 0.0720
64/87 [=====================>........] - ETA: 0s - loss: 0.0813 - mae: 0.2055 - mse: 0.0813
87/87 [==============================] - 0s 5ms/step - loss: 0.0793 - mae: 0.1985 - mse: 0.0793 - val_loss: 0.1277 - val_mae: 0.3075 - val_mse: 0.1277
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0846 - mae: 0.1890 - mse: 0.0846
64/87 [=====================>........] - ETA: 0s - loss: 0.0993 - mae: 0.2267 - mse: 0.0993
87/87 [==============================] - 0s 5ms/step - loss: 0.0834 - mae: 0.2030 - mse: 0.0834 - val_loss: 0.1298 - val_mae: 0.3150 - val_mse: 0.1298
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0608 - mae: 0.1766 - mse: 0.0608
64/87 [=====================>........] - ETA: 0s - loss: 0.0719 - mae: 0.1971 - mse: 0.0719
87/87 [==============================] - 0s 4ms/step - loss: 0.0693 - mae: 0.1875 - mse: 0.0693 - val_loss: 0.1322 - val_mae: 0.3168 - val_mse: 0.1322
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0991 - mae: 0.2209 - mse: 0.0991
64/87 [=====================>........] - ETA: 0s - loss: 0.0847 - mae: 0.2077 - mse: 0.0847
87/87 [==============================] - 0s 5ms/step - loss: 0.0807 - mae: 0.2017 - mse: 0.0807 - val_loss: 0.1340 - val_mae: 0.3205 - val_mse: 0.1340
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0819 - mae: 0.1946 - mse: 0.0819
64/87 [=====================>........] - ETA: 0s - loss: 0.0886 - mae: 0.2089 - mse: 0.0886
87/87 [==============================] - 0s 5ms/step - loss: 0.0857 - mae: 0.2042 - mse: 0.0857 - val_loss: 0.1352 - val_mae: 0.3204 - val_mse: 0.1352
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0491 - mae: 0.1500 - mse: 0.0491
64/87 [=====================>........] - ETA: 0s - loss: 0.0585 - mae: 0.1632 - mse: 0.0585
87/87 [==============================] - 0s 5ms/step - loss: 0.0691 - mae: 0.1752 - mse: 0.0691 - val_loss: 0.1347 - val_mae: 0.3176 - val_mse: 0.1347
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0640 - mae: 0.1738 - mse: 0.0640
64/87 [=====================>........] - ETA: 0s - loss: 0.0750 - mae: 0.1753 - mse: 0.0750
87/87 [==============================] - 0s 5ms/step - loss: 0.0737 - mae: 0.1807 - mse: 0.0737 - val_loss: 0.1329 - val_mae: 0.3218 - val_mse: 0.1329
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0702 - mae: 0.1812 - mse: 0.0702
64/87 [=====================>........] - ETA: 0s - loss: 0.0823 - mae: 0.1923 - mse: 0.0823
87/87 [==============================] - 1s 6ms/step - loss: 0.0702 - mae: 0.1833 - mse: 0.0702 - val_loss: 0.1301 - val_mae: 0.3191 - val_mse: 0.1301
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0547 - mae: 0.1536 - mse: 0.0547
64/87 [=====================>........] - ETA: 0s - loss: 0.0907 - mae: 0.1990 - mse: 0.0907
87/87 [==============================] - 0s 5ms/step - loss: 0.0793 - mae: 0.1863 - mse: 0.0793 - val_loss: 0.1250 - val_mae: 0.3026 - val_mse: 0.1250
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0629 - mae: 0.1766 - mse: 0.0629
64/87 [=====================>........] - ETA: 0s - loss: 0.0883 - mae: 0.2039 - mse: 0.0883
87/87 [==============================] - 0s 4ms/step - loss: 0.0705 - mae: 0.1787 - mse: 0.0705 - val_loss: 0.1231 - val_mae: 0.3041 - val_mse: 0.1231
Saving trained model...
98
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         66.31988525]
average prediction= [6.926019]
baseline= 7.9
eachuser= [0. 0. 0. 0. 0. 5.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 13.26397705078125
['train-height-10.py', '0']
155 1
2_155_65_10_csi_a10_14.dat
155 3
155 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
155 9
155 10
155 11
155 12
155 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
155 17
2_155_65_10_csi_a10_13.dat
155 19
155 20
2_155_65_10_csi_a10_3.dat
155 22
155 23
155 24
2_155_65_10_csi_a10_4.dat
155 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
155 29
155 30
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
180 131
180 132
180 133
180 134
180 135
180 136
180 137
180 138
180 139
180 140
180 141
180 142
180 143
180 144
180 145
180 146
180 147
180 148
180 149
180 150
180 151
180 152
180 153
180 154
180 155
180 156
1_180_85_10_csi_a10_28.dat
180 158
180 159
180 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
180 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
180 178
180 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
180 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
180 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
173 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 170
 170 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.4614 - mae: 0.6198 - mse: 0.4614
64/87 [=====================>........] - ETA: 0s - loss: 0.4037 - mae: 0.5711 - mse: 0.4037
87/87 [==============================] - 1s 9ms/step - loss: 0.3446 - mae: 0.5205 - mse: 0.3446 - val_loss: 0.1814 - val_mae: 0.4021 - val_mse: 0.1814
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1533 - mae: 0.3320 - mse: 0.1533
64/87 [=====================>........] - ETA: 0s - loss: 0.1230 - mae: 0.2892 - mse: 0.1230
87/87 [==============================] - 1s 8ms/step - loss: 0.1467 - mae: 0.3002 - mse: 0.1467 - val_loss: 0.2557 - val_mae: 0.3851 - val_mse: 0.2557
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2125 - mae: 0.3584 - mse: 0.2125
64/87 [=====================>........] - ETA: 0s - loss: 0.1763 - mae: 0.3279 - mse: 0.1763
87/87 [==============================] - 1s 6ms/step - loss: 0.1572 - mae: 0.3127 - mse: 0.1572 - val_loss: 0.1741 - val_mae: 0.3835 - val_mse: 0.1741
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0915 - mae: 0.2372 - mse: 0.0915
64/87 [=====================>........] - ETA: 0s - loss: 0.1132 - mae: 0.2675 - mse: 0.1132
87/87 [==============================] - 1s 6ms/step - loss: 0.1196 - mae: 0.2777 - mse: 0.1196 - val_loss: 0.1795 - val_mae: 0.3974 - val_mse: 0.1795
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1040 - mae: 0.2649 - mse: 0.1040
64/87 [=====================>........] - ETA: 0s - loss: 0.1272 - mae: 0.2947 - mse: 0.1272
87/87 [==============================] - 0s 6ms/step - loss: 0.1324 - mae: 0.3014 - mse: 0.1324 - val_loss: 0.1821 - val_mae: 0.3986 - val_mse: 0.1821
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1536 - mae: 0.3359 - mse: 0.1536
64/87 [=====================>........] - ETA: 0s - loss: 0.1564 - mae: 0.3384 - mse: 0.1564
87/87 [==============================] - 1s 6ms/step - loss: 0.1311 - mae: 0.3002 - mse: 0.1311 - val_loss: 0.1697 - val_mae: 0.3834 - val_mse: 0.1697
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1655 - mae: 0.3547 - mse: 0.1655
64/87 [=====================>........] - ETA: 0s - loss: 0.1244 - mae: 0.2873 - mse: 0.1244
87/87 [==============================] - 1s 6ms/step - loss: 0.1032 - mae: 0.2548 - mse: 0.1032 - val_loss: 0.1691 - val_mae: 0.3759 - val_mse: 0.1691
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0824 - mae: 0.2248 - mse: 0.0824
64/87 [=====================>........] - ETA: 0s - loss: 0.0951 - mae: 0.2366 - mse: 0.0951
87/87 [==============================] - 0s 6ms/step - loss: 0.1143 - mae: 0.2533 - mse: 0.1143 - val_loss: 0.1769 - val_mae: 0.3756 - val_mse: 0.1769
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1006 - mae: 0.2503 - mse: 0.1006
64/87 [=====================>........] - ETA: 0s - loss: 0.1246 - mae: 0.2783 - mse: 0.1246
87/87 [==============================] - 0s 5ms/step - loss: 0.1156 - mae: 0.2616 - mse: 0.1156 - val_loss: 0.1694 - val_mae: 0.3768 - val_mse: 0.1694
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1144 - mae: 0.2704 - mse: 0.1144
64/87 [=====================>........] - ETA: 0s - loss: 0.0974 - mae: 0.2452 - mse: 0.0974
87/87 [==============================] - 0s 5ms/step - loss: 0.0951 - mae: 0.2413 - mse: 0.0951 - val_loss: 0.1693 - val_mae: 0.3848 - val_mse: 0.1693
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0878 - mae: 0.2432 - mse: 0.0878
64/87 [=====================>........] - ETA: 0s - loss: 0.1178 - mae: 0.2763 - mse: 0.1178
87/87 [==============================] - 0s 5ms/step - loss: 0.1027 - mae: 0.2539 - mse: 0.1027 - val_loss: 0.1701 - val_mae: 0.3869 - val_mse: 0.1701
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1194 - mae: 0.2856 - mse: 0.1194
64/87 [=====================>........] - ETA: 0s - loss: 0.1202 - mae: 0.2820 - mse: 0.1202
87/87 [==============================] - 0s 5ms/step - loss: 0.1084 - mae: 0.2678 - mse: 0.1084 - val_loss: 0.1684 - val_mae: 0.3809 - val_mse: 0.1684
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0509 - mae: 0.1507 - mse: 0.0509
64/87 [=====================>........] - ETA: 0s - loss: 0.0816 - mae: 0.2098 - mse: 0.0816
87/87 [==============================] - 0s 5ms/step - loss: 0.0902 - mae: 0.2240 - mse: 0.0902 - val_loss: 0.1719 - val_mae: 0.3732 - val_mse: 0.1719
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0572 - mae: 0.1727 - mse: 0.0572
64/87 [=====================>........] - ETA: 0s - loss: 0.0889 - mae: 0.2131 - mse: 0.0889
87/87 [==============================] - 0s 5ms/step - loss: 0.0997 - mae: 0.2239 - mse: 0.0997 - val_loss: 0.1735 - val_mae: 0.3709 - val_mse: 0.1735
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0992 - mae: 0.2534 - mse: 0.0992
64/87 [=====================>........] - ETA: 0s - loss: 0.0827 - mae: 0.2187 - mse: 0.0827
87/87 [==============================] - 0s 5ms/step - loss: 0.0917 - mae: 0.2267 - mse: 0.0917 - val_loss: 0.1680 - val_mae: 0.3759 - val_mse: 0.1680
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0661 - mae: 0.1921 - mse: 0.0661
64/87 [=====================>........] - ETA: 0s - loss: 0.0764 - mae: 0.2065 - mse: 0.0764
87/87 [==============================] - 0s 5ms/step - loss: 0.0819 - mae: 0.2178 - mse: 0.0819 - val_loss: 0.1674 - val_mae: 0.3841 - val_mse: 0.1674
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0862 - mae: 0.2188 - mse: 0.0862
64/87 [=====================>........] - ETA: 0s - loss: 0.1023 - mae: 0.2436 - mse: 0.1023
87/87 [==============================] - 0s 5ms/step - loss: 0.0968 - mae: 0.2378 - mse: 0.0968 - val_loss: 0.1652 - val_mae: 0.3807 - val_mse: 0.1652
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0988 - mae: 0.2456 - mse: 0.0988
64/87 [=====================>........] - ETA: 0s - loss: 0.1021 - mae: 0.2444 - mse: 0.1021
87/87 [==============================] - 0s 5ms/step - loss: 0.0915 - mae: 0.2286 - mse: 0.0915 - val_loss: 0.1641 - val_mae: 0.3695 - val_mse: 0.1641
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0672 - mae: 0.1781 - mse: 0.0672
64/87 [=====================>........] - ETA: 0s - loss: 0.1080 - mae: 0.2345 - mse: 0.1080
87/87 [==============================] - 0s 5ms/step - loss: 0.0920 - mae: 0.2166 - mse: 0.0920 - val_loss: 0.1659 - val_mae: 0.3610 - val_mse: 0.1659
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0670 - mae: 0.1885 - mse: 0.0670
64/87 [=====================>........] - ETA: 0s - loss: 0.0765 - mae: 0.1987 - mse: 0.0765
87/87 [==============================] - 0s 5ms/step - loss: 0.0887 - mae: 0.2186 - mse: 0.0887 - val_loss: 0.1652 - val_mae: 0.3609 - val_mse: 0.1652
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1191 - mae: 0.2655 - mse: 0.1191
64/87 [=====================>........] - ETA: 0s - loss: 0.0902 - mae: 0.2171 - mse: 0.0902
87/87 [==============================] - 0s 5ms/step - loss: 0.0969 - mae: 0.2236 - mse: 0.0969 - val_loss: 0.1599 - val_mae: 0.3664 - val_mse: 0.1599
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0766 - mae: 0.1812 - mse: 0.0766
64/87 [=====================>........] - ETA: 0s - loss: 0.0847 - mae: 0.2106 - mse: 0.0847
87/87 [==============================] - 0s 5ms/step - loss: 0.0869 - mae: 0.2196 - mse: 0.0869 - val_loss: 0.1576 - val_mae: 0.3712 - val_mse: 0.1576
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0773 - mae: 0.2265 - mse: 0.0773
64/87 [=====================>........] - ETA: 0s - loss: 0.0838 - mae: 0.2220 - mse: 0.0838
87/87 [==============================] - 0s 5ms/step - loss: 0.0842 - mae: 0.2198 - mse: 0.0842 - val_loss: 0.1548 - val_mae: 0.3643 - val_mse: 0.1548
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1253 - mae: 0.2711 - mse: 0.1253
64/87 [=====================>........] - ETA: 0s - loss: 0.1023 - mae: 0.2388 - mse: 0.1023
87/87 [==============================] - 0s 5ms/step - loss: 0.0959 - mae: 0.2231 - mse: 0.0959 - val_loss: 0.1544 - val_mae: 0.3538 - val_mse: 0.1544
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0722 - mae: 0.1910 - mse: 0.0722
64/87 [=====================>........] - ETA: 0s - loss: 0.0651 - mae: 0.1837 - mse: 0.0651
87/87 [==============================] - 0s 4ms/step - loss: 0.0807 - mae: 0.2059 - mse: 0.0807 - val_loss: 0.1538 - val_mae: 0.3501 - val_mse: 0.1538
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1008 - mae: 0.2170 - mse: 0.1008
64/87 [=====================>........] - ETA: 0s - loss: 0.0960 - mae: 0.2226 - mse: 0.0960
87/87 [==============================] - 0s 5ms/step - loss: 0.0962 - mae: 0.2280 - mse: 0.0962 - val_loss: 0.1486 - val_mae: 0.3582 - val_mse: 0.1486
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0925 - mae: 0.2281 - mse: 0.0925
64/87 [=====================>........] - ETA: 0s - loss: 0.0936 - mae: 0.2334 - mse: 0.0936
87/87 [==============================] - 0s 5ms/step - loss: 0.0894 - mae: 0.2277 - mse: 0.0894 - val_loss: 0.1478 - val_mae: 0.3591 - val_mse: 0.1478
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0891 - mae: 0.2274 - mse: 0.0891
64/87 [=====================>........] - ETA: 0s - loss: 0.0904 - mae: 0.2290 - mse: 0.0904
87/87 [==============================] - 0s 5ms/step - loss: 0.0780 - mae: 0.2063 - mse: 0.0780 - val_loss: 0.1473 - val_mae: 0.3514 - val_mse: 0.1473
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0964 - mae: 0.2245 - mse: 0.0964
64/87 [=====================>........] - ETA: 0s - loss: 0.0818 - mae: 0.2032 - mse: 0.0818
87/87 [==============================] - 0s 5ms/step - loss: 0.0807 - mae: 0.2010 - mse: 0.0807 - val_loss: 0.1481 - val_mae: 0.3483 - val_mse: 0.1481
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0892 - mae: 0.2265 - mse: 0.0892
64/87 [=====================>........] - ETA: 0s - loss: 0.0751 - mae: 0.1981 - mse: 0.0751
87/87 [==============================] - 0s 5ms/step - loss: 0.0787 - mae: 0.2006 - mse: 0.0787 - val_loss: 0.1493 - val_mae: 0.3531 - val_mse: 0.1493
Saving trained model...
98
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         11.32124329]
average prediction= [5.603243]
baseline= 6.98
eachuser= [0. 0. 0. 0. 0. 1.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 11.321243286132812
['train-height-10.py', '0']
155 1
2_155_65_10_csi_a10_14.dat
155 3
155 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
155 9
155 10
155 11
155 12
155 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
155 17
2_155_65_10_csi_a10_13.dat
155 19
155 20
2_155_65_10_csi_a10_3.dat
155 22
155 23
155 24
2_155_65_10_csi_a10_4.dat
155 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
155 29
155 30
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
180 131
180 132
180 133
180 134
180 135
180 136
180 137
180 138
180 139
180 140
180 141
180 142
180 143
180 144
180 145
180 146
180 147
180 148
180 149
180 150
180 151
180 152
180 153
180 154
180 155
180 156
1_180_85_10_csi_a10_28.dat
180 158
180 159
180 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
180 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
180 178
180 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
180 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
180 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
173 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 170
 170 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.4643 - mae: 0.5709 - mse: 0.4643
64/87 [=====================>........] - ETA: 0s - loss: 0.4459 - mae: 0.5853 - mse: 0.4459
87/87 [==============================] - 1s 11ms/step - loss: 0.3784 - mae: 0.5353 - mse: 0.3784 - val_loss: 0.1603 - val_mae: 0.3317 - val_mse: 0.1603
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1212 - mae: 0.2934 - mse: 0.1212
64/87 [=====================>........] - ETA: 0s - loss: 0.1345 - mae: 0.3000 - mse: 0.1345
87/87 [==============================] - 0s 5ms/step - loss: 0.1795 - mae: 0.3294 - mse: 0.1795 - val_loss: 0.0931 - val_mae: 0.2608 - val_mse: 0.0931
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2266 - mae: 0.3600 - mse: 0.2266
64/87 [=====================>........] - ETA: 0s - loss: 0.1571 - mae: 0.2875 - mse: 0.1571
87/87 [==============================] - 1s 6ms/step - loss: 0.1557 - mae: 0.2879 - mse: 0.1557 - val_loss: 0.1044 - val_mae: 0.2705 - val_mse: 0.1044
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1463 - mae: 0.3030 - mse: 0.1463
64/87 [=====================>........] - ETA: 0s - loss: 0.1311 - mae: 0.2972 - mse: 0.1311
87/87 [==============================] - 1s 7ms/step - loss: 0.1223 - mae: 0.2877 - mse: 0.1223 - val_loss: 0.1535 - val_mae: 0.3132 - val_mse: 0.1535
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0985 - mae: 0.2699 - mse: 0.0985
64/87 [=====================>........] - ETA: 0s - loss: 0.1093 - mae: 0.2761 - mse: 0.1093
87/87 [==============================] - 0s 6ms/step - loss: 0.1171 - mae: 0.2871 - mse: 0.1171 - val_loss: 0.1719 - val_mae: 0.3429 - val_mse: 0.1719
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1242 - mae: 0.2852 - mse: 0.1242
64/87 [=====================>........] - ETA: 0s - loss: 0.1168 - mae: 0.2768 - mse: 0.1168
87/87 [==============================] - 0s 5ms/step - loss: 0.1154 - mae: 0.2796 - mse: 0.1154 - val_loss: 0.1496 - val_mae: 0.3115 - val_mse: 0.1496
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0986 - mae: 0.2372 - mse: 0.0986
64/87 [=====================>........] - ETA: 0s - loss: 0.1081 - mae: 0.2646 - mse: 0.1081
87/87 [==============================] - 0s 5ms/step - loss: 0.1145 - mae: 0.2673 - mse: 0.1145 - val_loss: 0.1268 - val_mae: 0.2848 - val_mse: 0.1268
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0977 - mae: 0.2418 - mse: 0.0977
64/87 [=====================>........] - ETA: 0s - loss: 0.1072 - mae: 0.2518 - mse: 0.1072
87/87 [==============================] - 0s 5ms/step - loss: 0.1236 - mae: 0.2772 - mse: 0.1236 - val_loss: 0.1186 - val_mae: 0.2778 - val_mse: 0.1186
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0910 - mae: 0.2340 - mse: 0.0910
64/87 [=====================>........] - ETA: 0s - loss: 0.1117 - mae: 0.2730 - mse: 0.1117
87/87 [==============================] - 0s 6ms/step - loss: 0.1055 - mae: 0.2584 - mse: 0.1055 - val_loss: 0.1230 - val_mae: 0.2880 - val_mse: 0.1230
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1143 - mae: 0.2678 - mse: 0.1143
64/87 [=====================>........] - ETA: 0s - loss: 0.1025 - mae: 0.2413 - mse: 0.1025
87/87 [==============================] - 0s 5ms/step - loss: 0.1121 - mae: 0.2583 - mse: 0.1121 - val_loss: 0.1342 - val_mae: 0.3036 - val_mse: 0.1342
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1463 - mae: 0.2993 - mse: 0.1463
64/87 [=====================>........] - ETA: 0s - loss: 0.1033 - mae: 0.2468 - mse: 0.1033
87/87 [==============================] - 0s 5ms/step - loss: 0.1103 - mae: 0.2584 - mse: 0.1103 - val_loss: 0.1387 - val_mae: 0.3101 - val_mse: 0.1387
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0853 - mae: 0.2268 - mse: 0.0853
64/87 [=====================>........] - ETA: 0s - loss: 0.0964 - mae: 0.2326 - mse: 0.0964
87/87 [==============================] - 0s 5ms/step - loss: 0.1017 - mae: 0.2456 - mse: 0.1017 - val_loss: 0.1360 - val_mae: 0.3078 - val_mse: 0.1360
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0857 - mae: 0.2244 - mse: 0.0857
64/87 [=====================>........] - ETA: 0s - loss: 0.0982 - mae: 0.2433 - mse: 0.0982
87/87 [==============================] - 0s 5ms/step - loss: 0.1053 - mae: 0.2510 - mse: 0.1053 - val_loss: 0.1391 - val_mae: 0.3121 - val_mse: 0.1391
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0884 - mae: 0.2337 - mse: 0.0884
64/87 [=====================>........] - ETA: 0s - loss: 0.0921 - mae: 0.2337 - mse: 0.0921
87/87 [==============================] - 0s 5ms/step - loss: 0.0980 - mae: 0.2439 - mse: 0.0980 - val_loss: 0.1445 - val_mae: 0.3181 - val_mse: 0.1445
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1131 - mae: 0.2545 - mse: 0.1131
64/87 [=====================>........] - ETA: 0s - loss: 0.1043 - mae: 0.2411 - mse: 0.1043
87/87 [==============================] - 0s 6ms/step - loss: 0.1041 - mae: 0.2446 - mse: 0.1041 - val_loss: 0.1350 - val_mae: 0.3082 - val_mse: 0.1350
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1297 - mae: 0.2853 - mse: 0.1297
64/87 [=====================>........] - ETA: 0s - loss: 0.1258 - mae: 0.2804 - mse: 0.1258
87/87 [==============================] - 0s 5ms/step - loss: 0.1127 - mae: 0.2669 - mse: 0.1127 - val_loss: 0.1460 - val_mae: 0.3183 - val_mse: 0.1460
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1133 - mae: 0.2725 - mse: 0.1133
64/87 [=====================>........] - ETA: 0s - loss: 0.0874 - mae: 0.2317 - mse: 0.0874
87/87 [==============================] - 0s 5ms/step - loss: 0.0870 - mae: 0.2277 - mse: 0.0870 - val_loss: 0.1456 - val_mae: 0.3168 - val_mse: 0.1456
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0836 - mae: 0.2092 - mse: 0.0836
64/87 [=====================>........] - ETA: 0s - loss: 0.0957 - mae: 0.2296 - mse: 0.0957
87/87 [==============================] - 0s 5ms/step - loss: 0.0987 - mae: 0.2408 - mse: 0.0987 - val_loss: 0.1330 - val_mae: 0.3033 - val_mse: 0.1330
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1118 - mae: 0.2560 - mse: 0.1118
64/87 [=====================>........] - ETA: 0s - loss: 0.0950 - mae: 0.2365 - mse: 0.0950
87/87 [==============================] - 0s 4ms/step - loss: 0.0927 - mae: 0.2363 - mse: 0.0927 - val_loss: 0.1240 - val_mae: 0.2922 - val_mse: 0.1240
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0885 - mae: 0.2139 - mse: 0.0885
64/87 [=====================>........] - ETA: 0s - loss: 0.0781 - mae: 0.1961 - mse: 0.0781
87/87 [==============================] - 0s 5ms/step - loss: 0.0960 - mae: 0.2187 - mse: 0.0960 - val_loss: 0.1168 - val_mae: 0.2817 - val_mse: 0.1168
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0822 - mae: 0.2108 - mse: 0.0822
64/87 [=====================>........] - ETA: 0s - loss: 0.0805 - mae: 0.2095 - mse: 0.0805
87/87 [==============================] - 0s 5ms/step - loss: 0.0802 - mae: 0.2090 - mse: 0.0802 - val_loss: 0.1288 - val_mae: 0.2932 - val_mse: 0.1288
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0752 - mae: 0.1961 - mse: 0.0752
64/87 [=====================>........] - ETA: 0s - loss: 0.0867 - mae: 0.2162 - mse: 0.0867
87/87 [==============================] - 0s 5ms/step - loss: 0.0911 - mae: 0.2294 - mse: 0.0911 - val_loss: 0.1425 - val_mae: 0.3072 - val_mse: 0.1425
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1141 - mae: 0.2577 - mse: 0.1141
64/87 [=====================>........] - ETA: 0s - loss: 0.0827 - mae: 0.2076 - mse: 0.0827
87/87 [==============================] - 0s 5ms/step - loss: 0.0895 - mae: 0.2207 - mse: 0.0895 - val_loss: 0.1399 - val_mae: 0.3037 - val_mse: 0.1399
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0951 - mae: 0.2370 - mse: 0.0951
64/87 [=====================>........] - ETA: 0s - loss: 0.0751 - mae: 0.2108 - mse: 0.0751
87/87 [==============================] - 0s 5ms/step - loss: 0.0850 - mae: 0.2274 - mse: 0.0850 - val_loss: 0.1157 - val_mae: 0.2739 - val_mse: 0.1157
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0858 - mae: 0.2217 - mse: 0.0858
64/87 [=====================>........] - ETA: 0s - loss: 0.0773 - mae: 0.2111 - mse: 0.0773
87/87 [==============================] - 0s 5ms/step - loss: 0.0805 - mae: 0.2179 - mse: 0.0805 - val_loss: 0.1142 - val_mae: 0.2717 - val_mse: 0.1142
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0850 - mae: 0.2131 - mse: 0.0850
64/87 [=====================>........] - ETA: 0s - loss: 0.1058 - mae: 0.2436 - mse: 0.1058
87/87 [==============================] - 0s 5ms/step - loss: 0.0936 - mae: 0.2271 - mse: 0.0936 - val_loss: 0.1310 - val_mae: 0.2949 - val_mse: 0.1310
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0829 - mae: 0.2173 - mse: 0.0829
64/87 [=====================>........] - ETA: 0s - loss: 0.0878 - mae: 0.2242 - mse: 0.0878
87/87 [==============================] - 0s 5ms/step - loss: 0.0920 - mae: 0.2310 - mse: 0.0920 - val_loss: 0.1288 - val_mae: 0.2913 - val_mse: 0.1288
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0757 - mae: 0.1924 - mse: 0.0757
64/87 [=====================>........] - ETA: 0s - loss: 0.0824 - mae: 0.2129 - mse: 0.0824
87/87 [==============================] - 0s 5ms/step - loss: 0.0843 - mae: 0.2124 - mse: 0.0843 - val_loss: 0.1181 - val_mae: 0.2755 - val_mse: 0.1181
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0653 - mae: 0.1800 - mse: 0.0653
64/87 [=====================>........] - ETA: 0s - loss: 0.0671 - mae: 0.1887 - mse: 0.0671
87/87 [==============================] - 0s 5ms/step - loss: 0.0794 - mae: 0.2070 - mse: 0.0794 - val_loss: 0.1248 - val_mae: 0.2873 - val_mse: 0.1248
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0589 - mae: 0.1741 - mse: 0.0589
64/87 [=====================>........] - ETA: 0s - loss: 0.0773 - mae: 0.1991 - mse: 0.0773
87/87 [==============================] - 0s 5ms/step - loss: 0.0865 - mae: 0.2145 - mse: 0.0865 - val_loss: 0.1284 - val_mae: 0.2945 - val_mse: 0.1284
Saving trained model...
98
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         31.60887146]
average prediction= [6.868028]
baseline= 7.46
eachuser= [0. 0. 0. 0. 0. 3.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 10.536290486653646
['train-height-10.py', '0']
155 1
2_155_65_10_csi_a10_14.dat
155 3
155 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
155 9
155 10
155 11
155 12
155 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
155 17
2_155_65_10_csi_a10_13.dat
155 19
155 20
2_155_65_10_csi_a10_3.dat
155 22
155 23
155 24
2_155_65_10_csi_a10_4.dat
155 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
155 29
155 30
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
180 131
180 132
180 133
180 134
180 135
180 136
180 137
180 138
180 139
180 140
180 141
180 142
180 143
180 144
180 145
180 146
180 147
180 148
180 149
180 150
180 151
180 152
180 153
180 154
180 155
180 156
1_180_85_10_csi_a10_28.dat
180 158
180 159
180 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
180 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
180 178
180 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
180 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
180 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
173 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 170
 170 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.5996 - mae: 0.6910 - mse: 0.5996
64/87 [=====================>........] - ETA: 0s - loss: 0.5002 - mae: 0.6297 - mse: 0.5002
87/87 [==============================] - 1s 10ms/step - loss: 0.4593 - mae: 0.5965 - mse: 0.4593 - val_loss: 0.3122 - val_mae: 0.5235 - val_mse: 0.3122
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2191 - mae: 0.4138 - mse: 0.2191
64/87 [=====================>........] - ETA: 0s - loss: 0.1979 - mae: 0.3833 - mse: 0.1979
87/87 [==============================] - 1s 7ms/step - loss: 0.1733 - mae: 0.3575 - mse: 0.1733 - val_loss: 0.1022 - val_mae: 0.2708 - val_mse: 0.1022
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1203 - mae: 0.2719 - mse: 0.1203
64/87 [=====================>........] - ETA: 0s - loss: 0.1880 - mae: 0.3284 - mse: 0.1880
87/87 [==============================] - 1s 6ms/step - loss: 0.1820 - mae: 0.3201 - mse: 0.1820 - val_loss: 0.0979 - val_mae: 0.2380 - val_mse: 0.0979
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1607 - mae: 0.3318 - mse: 0.1607
64/87 [=====================>........] - ETA: 0s - loss: 0.1707 - mae: 0.3162 - mse: 0.1707
87/87 [==============================] - 0s 5ms/step - loss: 0.1608 - mae: 0.3094 - mse: 0.1608 - val_loss: 0.1260 - val_mae: 0.3143 - val_mse: 0.1260
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1023 - mae: 0.2662 - mse: 0.1023
64/87 [=====================>........] - ETA: 0s - loss: 0.1131 - mae: 0.2862 - mse: 0.1131
87/87 [==============================] - 0s 6ms/step - loss: 0.1120 - mae: 0.2809 - mse: 0.1120 - val_loss: 0.1688 - val_mae: 0.3704 - val_mse: 0.1688
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1218 - mae: 0.2934 - mse: 0.1218
64/87 [=====================>........] - ETA: 0s - loss: 0.1351 - mae: 0.3126 - mse: 0.1351
87/87 [==============================] - 0s 5ms/step - loss: 0.1267 - mae: 0.2955 - mse: 0.1267 - val_loss: 0.1688 - val_mae: 0.3704 - val_mse: 0.1688
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1339 - mae: 0.2944 - mse: 0.1339
64/87 [=====================>........] - ETA: 0s - loss: 0.1494 - mae: 0.3223 - mse: 0.1494
87/87 [==============================] - 0s 5ms/step - loss: 0.1355 - mae: 0.3011 - mse: 0.1355 - val_loss: 0.1343 - val_mae: 0.3206 - val_mse: 0.1343
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1434 - mae: 0.3024 - mse: 0.1434
64/87 [=====================>........] - ETA: 0s - loss: 0.1101 - mae: 0.2583 - mse: 0.1101
87/87 [==============================] - 0s 5ms/step - loss: 0.1119 - mae: 0.2651 - mse: 0.1119 - val_loss: 0.1016 - val_mae: 0.2661 - val_mse: 0.1016
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1175 - mae: 0.2653 - mse: 0.1175
64/87 [=====================>........] - ETA: 0s - loss: 0.1010 - mae: 0.2514 - mse: 0.1010
87/87 [==============================] - 0s 5ms/step - loss: 0.1010 - mae: 0.2499 - mse: 0.1010 - val_loss: 0.0904 - val_mae: 0.2331 - val_mse: 0.0904
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1090 - mae: 0.2818 - mse: 0.1090
64/87 [=====================>........] - ETA: 0s - loss: 0.1138 - mae: 0.2756 - mse: 0.1138
87/87 [==============================] - 0s 5ms/step - loss: 0.1241 - mae: 0.2692 - mse: 0.1241 - val_loss: 0.0915 - val_mae: 0.2388 - val_mse: 0.0915
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1476 - mae: 0.3103 - mse: 0.1476
64/87 [=====================>........] - ETA: 0s - loss: 0.1178 - mae: 0.2675 - mse: 0.1178
87/87 [==============================] - 0s 5ms/step - loss: 0.1132 - mae: 0.2656 - mse: 0.1132 - val_loss: 0.1111 - val_mae: 0.2875 - val_mse: 0.1111
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1433 - mae: 0.3167 - mse: 0.1433
64/87 [=====================>........] - ETA: 0s - loss: 0.1121 - mae: 0.2618 - mse: 0.1121
87/87 [==============================] - 0s 5ms/step - loss: 0.1140 - mae: 0.2677 - mse: 0.1140 - val_loss: 0.1164 - val_mae: 0.2973 - val_mse: 0.1164
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1166 - mae: 0.2991 - mse: 0.1166
64/87 [=====================>........] - ETA: 0s - loss: 0.1007 - mae: 0.2497 - mse: 0.1007
87/87 [==============================] - 0s 5ms/step - loss: 0.0949 - mae: 0.2382 - mse: 0.0949 - val_loss: 0.1036 - val_mae: 0.2771 - val_mse: 0.1036
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0784 - mae: 0.2080 - mse: 0.0784
64/87 [=====================>........] - ETA: 0s - loss: 0.1312 - mae: 0.2753 - mse: 0.1312
87/87 [==============================] - 0s 5ms/step - loss: 0.1066 - mae: 0.2349 - mse: 0.1066 - val_loss: 0.0928 - val_mae: 0.2541 - val_mse: 0.0928
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1188 - mae: 0.2658 - mse: 0.1188
64/87 [=====================>........] - ETA: 0s - loss: 0.1142 - mae: 0.2541 - mse: 0.1142
87/87 [==============================] - 0s 4ms/step - loss: 0.0953 - mae: 0.2270 - mse: 0.0953 - val_loss: 0.0942 - val_mae: 0.2565 - val_mse: 0.0942
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0855 - mae: 0.2134 - mse: 0.0855
64/87 [=====================>........] - ETA: 0s - loss: 0.1139 - mae: 0.2514 - mse: 0.1139
87/87 [==============================] - 0s 5ms/step - loss: 0.1045 - mae: 0.2393 - mse: 0.1045 - val_loss: 0.0967 - val_mae: 0.2607 - val_mse: 0.0967
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1080 - mae: 0.2480 - mse: 0.1080
64/87 [=====================>........] - ETA: 0s - loss: 0.1054 - mae: 0.2374 - mse: 0.1054
87/87 [==============================] - 0s 5ms/step - loss: 0.0949 - mae: 0.2238 - mse: 0.0949 - val_loss: 0.0991 - val_mae: 0.2631 - val_mse: 0.0991
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1045 - mae: 0.2523 - mse: 0.1045
64/87 [=====================>........] - ETA: 0s - loss: 0.0884 - mae: 0.2254 - mse: 0.0884
87/87 [==============================] - 0s 5ms/step - loss: 0.0919 - mae: 0.2274 - mse: 0.0919 - val_loss: 0.0930 - val_mae: 0.2503 - val_mse: 0.0930
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0989 - mae: 0.2532 - mse: 0.0989
64/87 [=====================>........] - ETA: 0s - loss: 0.0779 - mae: 0.2084 - mse: 0.0779
87/87 [==============================] - 0s 5ms/step - loss: 0.0810 - mae: 0.2054 - mse: 0.0810 - val_loss: 0.0850 - val_mae: 0.2394 - val_mse: 0.0850
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0381 - mae: 0.1477 - mse: 0.0381
64/87 [=====================>........] - ETA: 0s - loss: 0.0857 - mae: 0.1970 - mse: 0.0857
87/87 [==============================] - 0s 5ms/step - loss: 0.1010 - mae: 0.2159 - mse: 0.1010 - val_loss: 0.0889 - val_mae: 0.2472 - val_mse: 0.0889
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0684 - mae: 0.1663 - mse: 0.0684
64/87 [=====================>........] - ETA: 0s - loss: 0.0697 - mae: 0.1807 - mse: 0.0697
87/87 [==============================] - 0s 5ms/step - loss: 0.0898 - mae: 0.2190 - mse: 0.0898 - val_loss: 0.1047 - val_mae: 0.2660 - val_mse: 0.1047
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1146 - mae: 0.2831 - mse: 0.1146
64/87 [=====================>........] - ETA: 0s - loss: 0.0974 - mae: 0.2436 - mse: 0.0974
87/87 [==============================] - 0s 5ms/step - loss: 0.0971 - mae: 0.2374 - mse: 0.0971 - val_loss: 0.1033 - val_mae: 0.2656 - val_mse: 0.1033
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1029 - mae: 0.2444 - mse: 0.1029
64/87 [=====================>........] - ETA: 0s - loss: 0.1085 - mae: 0.2523 - mse: 0.1085
87/87 [==============================] - 0s 4ms/step - loss: 0.0941 - mae: 0.2323 - mse: 0.0941 - val_loss: 0.0969 - val_mae: 0.2587 - val_mse: 0.0969
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1333 - mae: 0.2738 - mse: 0.1333
64/87 [=====================>........] - ETA: 0s - loss: 0.1000 - mae: 0.2244 - mse: 0.1000
87/87 [==============================] - 0s 5ms/step - loss: 0.0870 - mae: 0.2093 - mse: 0.0870 - val_loss: 0.0889 - val_mae: 0.2476 - val_mse: 0.0889
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0671 - mae: 0.1800 - mse: 0.0671
64/87 [=====================>........] - ETA: 0s - loss: 0.1292 - mae: 0.2555 - mse: 0.1292
87/87 [==============================] - 0s 5ms/step - loss: 0.1121 - mae: 0.2367 - mse: 0.1121 - val_loss: 0.0920 - val_mae: 0.2510 - val_mse: 0.0920
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1195 - mae: 0.2598 - mse: 0.1195
64/87 [=====================>........] - ETA: 0s - loss: 0.0907 - mae: 0.2210 - mse: 0.0907
87/87 [==============================] - 0s 5ms/step - loss: 0.0938 - mae: 0.2208 - mse: 0.0938 - val_loss: 0.0967 - val_mae: 0.2538 - val_mse: 0.0967
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0792 - mae: 0.2094 - mse: 0.0792
64/87 [=====================>........] - ETA: 0s - loss: 0.1046 - mae: 0.2324 - mse: 0.1046
87/87 [==============================] - 0s 5ms/step - loss: 0.0964 - mae: 0.2238 - mse: 0.0964 - val_loss: 0.0997 - val_mae: 0.2533 - val_mse: 0.0997
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0941 - mae: 0.2426 - mse: 0.0941
64/87 [=====================>........] - ETA: 0s - loss: 0.0874 - mae: 0.2292 - mse: 0.0874
87/87 [==============================] - 0s 5ms/step - loss: 0.0931 - mae: 0.2306 - mse: 0.0931 - val_loss: 0.0906 - val_mae: 0.2404 - val_mse: 0.0906
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0942 - mae: 0.2345 - mse: 0.0942
64/87 [=====================>........] - ETA: 0s - loss: 0.0906 - mae: 0.2258 - mse: 0.0906
87/87 [==============================] - 0s 5ms/step - loss: 0.0846 - mae: 0.2159 - mse: 0.0846 - val_loss: 0.0879 - val_mae: 0.2350 - val_mse: 0.0879
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0954 - mae: 0.2334 - mse: 0.0954
64/87 [=====================>........] - ETA: 0s - loss: 0.0872 - mae: 0.2167 - mse: 0.0872
87/87 [==============================] - 1s 6ms/step - loss: 0.0888 - mae: 0.2179 - mse: 0.0888 - val_loss: 0.0860 - val_mae: 0.2297 - val_mse: 0.0860
Saving trained model...
98
Testing...
heightdiff= [ 0.         0.         0.         0.         0.        66.8392334]
average prediction= [4.3786016]
baseline= 7.14
eachuser= [0. 0. 0. 0. 0. 5.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 13.3678466796875
