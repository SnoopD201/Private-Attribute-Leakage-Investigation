['train-height-16.py', '0']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
155 5
155 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
155 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
155 16
155 17
2_155_65_16_csi_a16_21.dat
155 19
155 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
155 25
2_155_65_16_csi_a16_10.dat
155 27
155 28
2_155_65_16_csi_a16_19.dat
155 30
170 31
2_170_60_16_csi_a16_3.dat
170 33
170 34
170 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
170 38
170 39
170 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_16_csi_a16_19.dat
165 90
2_165_50_16_csi_a16_3.dat
165 92
165 93
2_165_50_16_csi_a16_27.dat
165 95
165 96
165 97
165 98
165 99
165 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
175 105
1_175_70_16_csi_a16_23.dat
175 107
175 108
175 109
1_175_70_16_csi_a16_14.dat
175 111
175 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
175 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
175 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
175 123
175 124
1_175_70_16_csi_a16_5.dat
175 126
1_175_70_16_csi_a16_18.dat
175 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
180 132
1_180_85_16_csi_a16_7.dat
180 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
180 139
180 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
180 148
1_180_85_16_csi_a16_17.dat
180 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
180 155
180 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
180 161
180 162
180 163
1_180_75_16_csi_a16_18.dat
180 165
180 166
1_180_75_16_csi_a16_16.dat
180 168
1_180_75_16_csi_a16_9.dat
180 170
180 171
1_180_75_16_csi_a16_17.dat
180 173
180 174
1_180_75_16_csi_a16_13.dat
180 176
1_180_75_16_csi_a16_10.dat
180 178
180 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
180 183
1_180_75_16_csi_a16_5.dat
180 185
180 186
180 187
180 188
180 189
180 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
173 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
173 197
173 198
173 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
173 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
173 205
173 206
173 207
173 208
173 209
173 210
1_173_85_16_csi_a16_29.dat
173 212
1_173_85_16_csi_a16_17.dat
173 214
173 215
1_173_85_16_csi_a16_22.dat
173 217
1_173_85_16_csi_a16_26.dat
173 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173 173 173 173 173
 173 173 173 173 173 173 173 173 173 173 173]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.5539 - mae: 0.6803 - mse: 0.5539
64/72 [=========================>....] - ETA: 0s - loss: 0.4188 - mae: 0.5776 - mse: 0.4188
72/72 [==============================] - 1s 13ms/step - loss: 0.3933 - mae: 0.5581 - mse: 0.3933 - val_loss: 0.1543 - val_mae: 0.3354 - val_mse: 0.1543
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1486 - mae: 0.3230 - mse: 0.1486
64/72 [=========================>....] - ETA: 0s - loss: 0.1143 - mae: 0.2724 - mse: 0.1143
72/72 [==============================] - 1s 8ms/step - loss: 0.1512 - mae: 0.3074 - mse: 0.1512 - val_loss: 0.1941 - val_mae: 0.3277 - val_mse: 0.1941
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1758 - mae: 0.3045 - mse: 0.1758
64/72 [=========================>....] - ETA: 0s - loss: 0.1519 - mae: 0.2927 - mse: 0.1519
72/72 [==============================] - 1s 8ms/step - loss: 0.1504 - mae: 0.2907 - mse: 0.1504 - val_loss: 0.1413 - val_mae: 0.3165 - val_mse: 0.1413
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.0917 - mae: 0.2504 - mse: 0.0917
64/72 [=========================>....] - ETA: 0s - loss: 0.1029 - mae: 0.2668 - mse: 0.1029
72/72 [==============================] - 1s 8ms/step - loss: 0.1042 - mae: 0.2696 - mse: 0.1042 - val_loss: 0.1712 - val_mae: 0.3795 - val_mse: 0.1712
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.1523 - mae: 0.3292 - mse: 0.1523
64/72 [=========================>....] - ETA: 0s - loss: 0.1444 - mae: 0.3121 - mse: 0.1444
72/72 [==============================] - 1s 9ms/step - loss: 0.1447 - mae: 0.3184 - mse: 0.1447 - val_loss: 0.1586 - val_mae: 0.3612 - val_mse: 0.1586
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.1181 - mae: 0.2962 - mse: 0.1181
64/72 [=========================>....] - ETA: 0s - loss: 0.1097 - mae: 0.2789 - mse: 0.1097
72/72 [==============================] - 1s 8ms/step - loss: 0.1130 - mae: 0.2795 - mse: 0.1130 - val_loss: 0.1266 - val_mae: 0.3034 - val_mse: 0.1266
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0985 - mae: 0.2438 - mse: 0.0985
64/72 [=========================>....] - ETA: 0s - loss: 0.0955 - mae: 0.2393 - mse: 0.0955
72/72 [==============================] - 1s 9ms/step - loss: 0.0874 - mae: 0.2244 - mse: 0.0874 - val_loss: 0.1119 - val_mae: 0.2801 - val_mse: 0.1119
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0721 - mae: 0.2109 - mse: 0.0721
64/72 [=========================>....] - ETA: 0s - loss: 0.0828 - mae: 0.2077 - mse: 0.0828
72/72 [==============================] - 1s 9ms/step - loss: 0.0886 - mae: 0.2101 - mse: 0.0886 - val_loss: 0.1062 - val_mae: 0.2707 - val_mse: 0.1062
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.1000 - mae: 0.2371 - mse: 0.1000
64/72 [=========================>....] - ETA: 0s - loss: 0.0923 - mae: 0.2292 - mse: 0.0923
72/72 [==============================] - 1s 9ms/step - loss: 0.0888 - mae: 0.2269 - mse: 0.0888 - val_loss: 0.1126 - val_mae: 0.2878 - val_mse: 0.1126
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0681 - mae: 0.2083 - mse: 0.0681
64/72 [=========================>....] - ETA: 0s - loss: 0.0748 - mae: 0.2122 - mse: 0.0748
72/72 [==============================] - 1s 8ms/step - loss: 0.0776 - mae: 0.2158 - mse: 0.0776 - val_loss: 0.1244 - val_mae: 0.3175 - val_mse: 0.1244
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0889 - mae: 0.2418 - mse: 0.0889
64/72 [=========================>....] - ETA: 0s - loss: 0.0913 - mae: 0.2393 - mse: 0.0913
72/72 [==============================] - 1s 8ms/step - loss: 0.0907 - mae: 0.2400 - mse: 0.0907 - val_loss: 0.1145 - val_mae: 0.3007 - val_mse: 0.1145
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0456 - mae: 0.1547 - mse: 0.0456
64/72 [=========================>....] - ETA: 0s - loss: 0.0689 - mae: 0.1963 - mse: 0.0689
72/72 [==============================] - 1s 8ms/step - loss: 0.0699 - mae: 0.1981 - mse: 0.0699 - val_loss: 0.0963 - val_mae: 0.2587 - val_mse: 0.0963
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0749 - mae: 0.1957 - mse: 0.0749
64/72 [=========================>....] - ETA: 0s - loss: 0.0685 - mae: 0.1855 - mse: 0.0685
72/72 [==============================] - 1s 8ms/step - loss: 0.0627 - mae: 0.1752 - mse: 0.0627 - val_loss: 0.0852 - val_mae: 0.2361 - val_mse: 0.0852
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0605 - mae: 0.1785 - mse: 0.0605
64/72 [=========================>....] - ETA: 0s - loss: 0.0804 - mae: 0.2044 - mse: 0.0804
72/72 [==============================] - 1s 8ms/step - loss: 0.0745 - mae: 0.1920 - mse: 0.0745 - val_loss: 0.0865 - val_mae: 0.2472 - val_mse: 0.0865
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0711 - mae: 0.1934 - mse: 0.0711
64/72 [=========================>....] - ETA: 0s - loss: 0.0651 - mae: 0.1873 - mse: 0.0651
72/72 [==============================] - 1s 8ms/step - loss: 0.0613 - mae: 0.1814 - mse: 0.0613 - val_loss: 0.1031 - val_mae: 0.2847 - val_mse: 0.1031
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0936 - mae: 0.2288 - mse: 0.0936
64/72 [=========================>....] - ETA: 0s - loss: 0.0709 - mae: 0.1972 - mse: 0.0709
72/72 [==============================] - 1s 8ms/step - loss: 0.0683 - mae: 0.1900 - mse: 0.0683 - val_loss: 0.0868 - val_mae: 0.2569 - val_mse: 0.0868
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0455 - mae: 0.1573 - mse: 0.0455
64/72 [=========================>....] - ETA: 0s - loss: 0.0660 - mae: 0.1933 - mse: 0.0660
72/72 [==============================] - 1s 7ms/step - loss: 0.0611 - mae: 0.1848 - mse: 0.0611 - val_loss: 0.0752 - val_mae: 0.2330 - val_mse: 0.0752
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0548 - mae: 0.1787 - mse: 0.0548
64/72 [=========================>....] - ETA: 0s - loss: 0.0584 - mae: 0.1746 - mse: 0.0584
72/72 [==============================] - 1s 8ms/step - loss: 0.0590 - mae: 0.1744 - mse: 0.0590 - val_loss: 0.0742 - val_mae: 0.2339 - val_mse: 0.0742
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0421 - mae: 0.1653 - mse: 0.0421
64/72 [=========================>....] - ETA: 0s - loss: 0.0517 - mae: 0.1726 - mse: 0.0517
72/72 [==============================] - 1s 8ms/step - loss: 0.0570 - mae: 0.1825 - mse: 0.0570 - val_loss: 0.0767 - val_mae: 0.2415 - val_mse: 0.0767
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0463 - mae: 0.1736 - mse: 0.0463
64/72 [=========================>....] - ETA: 0s - loss: 0.0466 - mae: 0.1668 - mse: 0.0466
72/72 [==============================] - 1s 7ms/step - loss: 0.0446 - mae: 0.1601 - mse: 0.0446 - val_loss: 0.0601 - val_mae: 0.1986 - val_mse: 0.0601
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0367 - mae: 0.1457 - mse: 0.0367
64/72 [=========================>....] - ETA: 0s - loss: 0.0537 - mae: 0.1635 - mse: 0.0537
72/72 [==============================] - 1s 7ms/step - loss: 0.0507 - mae: 0.1572 - mse: 0.0507 - val_loss: 0.0678 - val_mae: 0.2238 - val_mse: 0.0678
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0472 - mae: 0.1703 - mse: 0.0472
64/72 [=========================>....] - ETA: 0s - loss: 0.0529 - mae: 0.1748 - mse: 0.0529
72/72 [==============================] - 0s 6ms/step - loss: 0.0535 - mae: 0.1781 - mse: 0.0535 - val_loss: 0.0774 - val_mae: 0.2413 - val_mse: 0.0774
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0586 - mae: 0.1850 - mse: 0.0586
64/72 [=========================>....] - ETA: 0s - loss: 0.0477 - mae: 0.1647 - mse: 0.0477
72/72 [==============================] - 0s 6ms/step - loss: 0.0431 - mae: 0.1537 - mse: 0.0431 - val_loss: 0.0533 - val_mae: 0.1779 - val_mse: 0.0533
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0564 - mae: 0.1677 - mse: 0.0564
64/72 [=========================>....] - ETA: 0s - loss: 0.0536 - mae: 0.1674 - mse: 0.0536
72/72 [==============================] - 0s 6ms/step - loss: 0.0563 - mae: 0.1725 - mse: 0.0563 - val_loss: 0.0517 - val_mae: 0.1733 - val_mse: 0.0517
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0705 - mae: 0.1986 - mse: 0.0705
64/72 [=========================>....] - ETA: 0s - loss: 0.0545 - mae: 0.1763 - mse: 0.0545
72/72 [==============================] - 0s 6ms/step - loss: 0.0510 - mae: 0.1660 - mse: 0.0510 - val_loss: 0.0793 - val_mae: 0.2352 - val_mse: 0.0793
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0819 - mae: 0.2234 - mse: 0.0819
64/72 [=========================>....] - ETA: 0s - loss: 0.0579 - mae: 0.1801 - mse: 0.0579
72/72 [==============================] - 0s 6ms/step - loss: 0.0553 - mae: 0.1791 - mse: 0.0553 - val_loss: 0.0698 - val_mae: 0.2146 - val_mse: 0.0698
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0438 - mae: 0.1613 - mse: 0.0438
64/72 [=========================>....] - ETA: 0s - loss: 0.0476 - mae: 0.1651 - mse: 0.0476
72/72 [==============================] - 0s 5ms/step - loss: 0.0464 - mae: 0.1618 - mse: 0.0464 - val_loss: 0.0456 - val_mae: 0.1496 - val_mse: 0.0456
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0524 - mae: 0.1749 - mse: 0.0524
64/72 [=========================>....] - ETA: 0s - loss: 0.0423 - mae: 0.1535 - mse: 0.0423
72/72 [==============================] - 0s 6ms/step - loss: 0.0410 - mae: 0.1486 - mse: 0.0410 - val_loss: 0.0491 - val_mae: 0.1629 - val_mse: 0.0491
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0520 - mae: 0.1621 - mse: 0.0520
64/72 [=========================>....] - ETA: 0s - loss: 0.0462 - mae: 0.1596 - mse: 0.0462
72/72 [==============================] - 0s 6ms/step - loss: 0.0470 - mae: 0.1638 - mse: 0.0470 - val_loss: 0.0807 - val_mae: 0.2353 - val_mse: 0.0807
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0457 - mae: 0.1727 - mse: 0.0457
64/72 [=========================>....] - ETA: 0s - loss: 0.0457 - mae: 0.1668 - mse: 0.0457
72/72 [==============================] - 0s 5ms/step - loss: 0.0443 - mae: 0.1657 - mse: 0.0443 - val_loss: 0.0801 - val_mae: 0.2354 - val_mse: 0.0801
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         0.         0.         4.34655762]
average prediction= [4.2971497]
baseline= 5.4523809523809526
eachuser= [0. 0. 0. 0. 0. 1.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 4.3465576171875
['train-height-16.py', '0']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
155 5
155 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
155 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
155 16
155 17
2_155_65_16_csi_a16_21.dat
155 19
155 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
155 25
2_155_65_16_csi_a16_10.dat
155 27
155 28
2_155_65_16_csi_a16_19.dat
155 30
170 31
2_170_60_16_csi_a16_3.dat
170 33
170 34
170 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
170 38
170 39
170 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_16_csi_a16_19.dat
165 90
2_165_50_16_csi_a16_3.dat
165 92
165 93
2_165_50_16_csi_a16_27.dat
165 95
165 96
165 97
165 98
165 99
165 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
175 105
1_175_70_16_csi_a16_23.dat
175 107
175 108
175 109
1_175_70_16_csi_a16_14.dat
175 111
175 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
175 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
175 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
175 123
175 124
1_175_70_16_csi_a16_5.dat
175 126
1_175_70_16_csi_a16_18.dat
175 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
180 132
1_180_85_16_csi_a16_7.dat
180 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
180 139
180 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
180 148
1_180_85_16_csi_a16_17.dat
180 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
180 155
180 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
180 161
180 162
180 163
1_180_75_16_csi_a16_18.dat
180 165
180 166
1_180_75_16_csi_a16_16.dat
180 168
1_180_75_16_csi_a16_9.dat
180 170
180 171
1_180_75_16_csi_a16_17.dat
180 173
180 174
1_180_75_16_csi_a16_13.dat
180 176
1_180_75_16_csi_a16_10.dat
180 178
180 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
180 183
1_180_75_16_csi_a16_5.dat
180 185
180 186
180 187
180 188
180 189
180 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
173 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
173 197
173 198
173 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
173 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
173 205
173 206
173 207
173 208
173 209
173 210
1_173_85_16_csi_a16_29.dat
173 212
1_173_85_16_csi_a16_17.dat
173 214
173 215
1_173_85_16_csi_a16_22.dat
173 217
1_173_85_16_csi_a16_26.dat
173 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173 173 173 173 173
 173 173 173 173 173 173 173 173 173 173 173]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.4798 - mae: 0.6074 - mse: 0.4798
64/72 [=========================>....] - ETA: 0s - loss: 0.4499 - mae: 0.6021 - mse: 0.4499
72/72 [==============================] - 1s 15ms/step - loss: 0.4288 - mae: 0.5866 - mse: 0.4288 - val_loss: 0.1017 - val_mae: 0.2378 - val_mse: 0.1017
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1385 - mae: 0.3066 - mse: 0.1385
64/72 [=========================>....] - ETA: 0s - loss: 0.1137 - mae: 0.2736 - mse: 0.1137
72/72 [==============================] - 1s 8ms/step - loss: 0.1192 - mae: 0.2803 - mse: 0.1192 - val_loss: 0.1452 - val_mae: 0.3168 - val_mse: 0.1452
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1995 - mae: 0.3335 - mse: 0.1995
64/72 [=========================>....] - ETA: 0s - loss: 0.1696 - mae: 0.3063 - mse: 0.1696
72/72 [==============================] - 1s 8ms/step - loss: 0.1776 - mae: 0.3163 - mse: 0.1776 - val_loss: 0.0970 - val_mae: 0.2306 - val_mse: 0.0970
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.1840 - mae: 0.3084 - mse: 0.1840
64/72 [=========================>....] - ETA: 0s - loss: 0.1436 - mae: 0.2976 - mse: 0.1436
72/72 [==============================] - 1s 9ms/step - loss: 0.1434 - mae: 0.2965 - mse: 0.1434 - val_loss: 0.0794 - val_mae: 0.1889 - val_mse: 0.0794
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.1086 - mae: 0.2602 - mse: 0.1086
64/72 [=========================>....] - ETA: 0s - loss: 0.1240 - mae: 0.2823 - mse: 0.1240
72/72 [==============================] - 1s 8ms/step - loss: 0.1234 - mae: 0.2811 - mse: 0.1234 - val_loss: 0.0939 - val_mae: 0.2545 - val_mse: 0.0939
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.1484 - mae: 0.3414 - mse: 0.1484
64/72 [=========================>....] - ETA: 0s - loss: 0.1297 - mae: 0.3012 - mse: 0.1297
72/72 [==============================] - 1s 8ms/step - loss: 0.1241 - mae: 0.2947 - mse: 0.1241 - val_loss: 0.0914 - val_mae: 0.2550 - val_mse: 0.0914
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.1072 - mae: 0.2748 - mse: 0.1072
64/72 [=========================>....] - ETA: 0s - loss: 0.1270 - mae: 0.2908 - mse: 0.1270
72/72 [==============================] - 1s 9ms/step - loss: 0.1176 - mae: 0.2770 - mse: 0.1176 - val_loss: 0.0776 - val_mae: 0.2109 - val_mse: 0.0776
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0803 - mae: 0.2120 - mse: 0.0803
64/72 [=========================>....] - ETA: 0s - loss: 0.0918 - mae: 0.2169 - mse: 0.0918
72/72 [==============================] - 1s 8ms/step - loss: 0.0875 - mae: 0.2161 - mse: 0.0875 - val_loss: 0.0746 - val_mae: 0.1821 - val_mse: 0.0746
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0969 - mae: 0.2221 - mse: 0.0969
64/72 [=========================>....] - ETA: 0s - loss: 0.0936 - mae: 0.2166 - mse: 0.0936
72/72 [==============================] - 1s 7ms/step - loss: 0.0898 - mae: 0.2159 - mse: 0.0898 - val_loss: 0.0785 - val_mae: 0.1756 - val_mse: 0.0785
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0798 - mae: 0.1961 - mse: 0.0798
64/72 [=========================>....] - ETA: 0s - loss: 0.0887 - mae: 0.2153 - mse: 0.0887
72/72 [==============================] - 0s 6ms/step - loss: 0.1071 - mae: 0.2308 - mse: 0.1071 - val_loss: 0.0737 - val_mae: 0.1903 - val_mse: 0.0737
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.1011 - mae: 0.2188 - mse: 0.1011
64/72 [=========================>....] - ETA: 0s - loss: 0.0836 - mae: 0.2047 - mse: 0.0836
72/72 [==============================] - 0s 5ms/step - loss: 0.0863 - mae: 0.2080 - mse: 0.0863 - val_loss: 0.0778 - val_mae: 0.2262 - val_mse: 0.0778
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0911 - mae: 0.2420 - mse: 0.0911
64/72 [=========================>....] - ETA: 0s - loss: 0.0960 - mae: 0.2462 - mse: 0.0960
72/72 [==============================] - 0s 6ms/step - loss: 0.0875 - mae: 0.2326 - mse: 0.0875 - val_loss: 0.0816 - val_mae: 0.2440 - val_mse: 0.0816
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.1028 - mae: 0.2480 - mse: 0.1028
64/72 [=========================>....] - ETA: 0s - loss: 0.1015 - mae: 0.2585 - mse: 0.1015
72/72 [==============================] - 0s 6ms/step - loss: 0.1019 - mae: 0.2574 - mse: 0.1019 - val_loss: 0.0758 - val_mae: 0.2292 - val_mse: 0.0758
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0861 - mae: 0.2240 - mse: 0.0861
64/72 [=========================>....] - ETA: 0s - loss: 0.0825 - mae: 0.2279 - mse: 0.0825
72/72 [==============================] - 0s 6ms/step - loss: 0.0792 - mae: 0.2253 - mse: 0.0792 - val_loss: 0.0676 - val_mae: 0.2036 - val_mse: 0.0676
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.1058 - mae: 0.2469 - mse: 0.1058
64/72 [=========================>....] - ETA: 0s - loss: 0.0974 - mae: 0.2303 - mse: 0.0974
72/72 [==============================] - 0s 5ms/step - loss: 0.0874 - mae: 0.2131 - mse: 0.0874 - val_loss: 0.0649 - val_mae: 0.1765 - val_mse: 0.0649
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0804 - mae: 0.1774 - mse: 0.0804
64/72 [=========================>....] - ETA: 0s - loss: 0.0684 - mae: 0.1772 - mse: 0.0684
72/72 [==============================] - 0s 6ms/step - loss: 0.0747 - mae: 0.1820 - mse: 0.0747 - val_loss: 0.0621 - val_mae: 0.1791 - val_mse: 0.0621
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0413 - mae: 0.1572 - mse: 0.0413
64/72 [=========================>....] - ETA: 0s - loss: 0.0726 - mae: 0.1862 - mse: 0.0726
72/72 [==============================] - 0s 6ms/step - loss: 0.0734 - mae: 0.1850 - mse: 0.0734 - val_loss: 0.0653 - val_mae: 0.2089 - val_mse: 0.0653
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0553 - mae: 0.1925 - mse: 0.0553
64/72 [=========================>....] - ETA: 0s - loss: 0.0751 - mae: 0.2094 - mse: 0.0751
72/72 [==============================] - 0s 6ms/step - loss: 0.0720 - mae: 0.2046 - mse: 0.0720 - val_loss: 0.0737 - val_mae: 0.2411 - val_mse: 0.0737
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0716 - mae: 0.2223 - mse: 0.0716
64/72 [=========================>....] - ETA: 0s - loss: 0.0673 - mae: 0.2134 - mse: 0.0673
72/72 [==============================] - 0s 5ms/step - loss: 0.0754 - mae: 0.2227 - mse: 0.0754 - val_loss: 0.0659 - val_mae: 0.2227 - val_mse: 0.0659
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0620 - mae: 0.1976 - mse: 0.0620
64/72 [=========================>....] - ETA: 0s - loss: 0.0803 - mae: 0.2177 - mse: 0.0803
72/72 [==============================] - 0s 5ms/step - loss: 0.0784 - mae: 0.2102 - mse: 0.0784 - val_loss: 0.0559 - val_mae: 0.1867 - val_mse: 0.0559
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0421 - mae: 0.1320 - mse: 0.0421
64/72 [=========================>....] - ETA: 0s - loss: 0.0509 - mae: 0.1537 - mse: 0.0509
72/72 [==============================] - 0s 5ms/step - loss: 0.0552 - mae: 0.1618 - mse: 0.0552 - val_loss: 0.0525 - val_mae: 0.1759 - val_mse: 0.0525
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0799 - mae: 0.2038 - mse: 0.0799
64/72 [=========================>....] - ETA: 0s - loss: 0.0765 - mae: 0.1901 - mse: 0.0765
72/72 [==============================] - 0s 5ms/step - loss: 0.0719 - mae: 0.1877 - mse: 0.0719 - val_loss: 0.0511 - val_mae: 0.1708 - val_mse: 0.0511
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0727 - mae: 0.1718 - mse: 0.0727
64/72 [=========================>....] - ETA: 0s - loss: 0.0595 - mae: 0.1589 - mse: 0.0595
72/72 [==============================] - 0s 5ms/step - loss: 0.0627 - mae: 0.1670 - mse: 0.0627 - val_loss: 0.0505 - val_mae: 0.1753 - val_mse: 0.0505
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0612 - mae: 0.1619 - mse: 0.0612
64/72 [=========================>....] - ETA: 0s - loss: 0.0604 - mae: 0.1689 - mse: 0.0604
72/72 [==============================] - 0s 6ms/step - loss: 0.0584 - mae: 0.1714 - mse: 0.0584 - val_loss: 0.0501 - val_mae: 0.1858 - val_mse: 0.0501
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0559 - mae: 0.1711 - mse: 0.0559
64/72 [=========================>....] - ETA: 0s - loss: 0.0626 - mae: 0.1843 - mse: 0.0626
72/72 [==============================] - 0s 5ms/step - loss: 0.0580 - mae: 0.1756 - mse: 0.0580 - val_loss: 0.0449 - val_mae: 0.1657 - val_mse: 0.0449
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0521 - mae: 0.1741 - mse: 0.0521
64/72 [=========================>....] - ETA: 0s - loss: 0.0561 - mae: 0.1712 - mse: 0.0561
72/72 [==============================] - 0s 5ms/step - loss: 0.0520 - mae: 0.1657 - mse: 0.0520 - val_loss: 0.0417 - val_mae: 0.1570 - val_mse: 0.0417
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0401 - mae: 0.1321 - mse: 0.0401
64/72 [=========================>....] - ETA: 0s - loss: 0.0580 - mae: 0.1510 - mse: 0.0580
72/72 [==============================] - 0s 6ms/step - loss: 0.0574 - mae: 0.1502 - mse: 0.0574 - val_loss: 0.0409 - val_mae: 0.1683 - val_mse: 0.0409
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0369 - mae: 0.1408 - mse: 0.0369
64/72 [=========================>....] - ETA: 0s - loss: 0.0508 - mae: 0.1634 - mse: 0.0508
72/72 [==============================] - 0s 6ms/step - loss: 0.0524 - mae: 0.1619 - mse: 0.0524 - val_loss: 0.0397 - val_mae: 0.1681 - val_mse: 0.0397
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0434 - mae: 0.1549 - mse: 0.0434
64/72 [=========================>....] - ETA: 0s - loss: 0.0419 - mae: 0.1456 - mse: 0.0419
72/72 [==============================] - 0s 5ms/step - loss: 0.0438 - mae: 0.1496 - mse: 0.0438 - val_loss: 0.0406 - val_mae: 0.1815 - val_mse: 0.0406
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0441 - mae: 0.1517 - mse: 0.0441
64/72 [=========================>....] - ETA: 0s - loss: 0.0491 - mae: 0.1598 - mse: 0.0491
72/72 [==============================] - 0s 5ms/step - loss: 0.0468 - mae: 0.1563 - mse: 0.0468 - val_loss: 0.0382 - val_mae: 0.1752 - val_mse: 0.0382
Saving trained model...
119
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         28.49687195]
average prediction= [4.80809]
baseline= 7.5476190476190474
eachuser= [0. 0. 0. 0. 0. 3.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 9.498957316080729
['train-height-16.py', '0']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
155 5
155 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
155 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
155 16
155 17
2_155_65_16_csi_a16_21.dat
155 19
155 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
155 25
2_155_65_16_csi_a16_10.dat
155 27
155 28
2_155_65_16_csi_a16_19.dat
155 30
170 31
2_170_60_16_csi_a16_3.dat
170 33
170 34
170 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
170 38
170 39
170 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_16_csi_a16_19.dat
165 90
2_165_50_16_csi_a16_3.dat
165 92
165 93
2_165_50_16_csi_a16_27.dat
165 95
165 96
165 97
165 98
165 99
165 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
175 105
1_175_70_16_csi_a16_23.dat
175 107
175 108
175 109
1_175_70_16_csi_a16_14.dat
175 111
175 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
175 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
175 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
175 123
175 124
1_175_70_16_csi_a16_5.dat
175 126
1_175_70_16_csi_a16_18.dat
175 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
180 132
1_180_85_16_csi_a16_7.dat
180 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
180 139
180 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
180 148
1_180_85_16_csi_a16_17.dat
180 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
180 155
180 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
180 161
180 162
180 163
1_180_75_16_csi_a16_18.dat
180 165
180 166
1_180_75_16_csi_a16_16.dat
180 168
1_180_75_16_csi_a16_9.dat
180 170
180 171
1_180_75_16_csi_a16_17.dat
180 173
180 174
1_180_75_16_csi_a16_13.dat
180 176
1_180_75_16_csi_a16_10.dat
180 178
180 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
180 183
1_180_75_16_csi_a16_5.dat
180 185
180 186
180 187
180 188
180 189
180 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
173 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
173 197
173 198
173 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
173 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
173 205
173 206
173 207
173 208
173 209
173 210
1_173_85_16_csi_a16_29.dat
173 212
1_173_85_16_csi_a16_17.dat
173 214
173 215
1_173_85_16_csi_a16_22.dat
173 217
1_173_85_16_csi_a16_26.dat
173 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173 173 173 173 173
 173 173 173 173 173 173 173 173 173 173 173]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.4305 - mae: 0.5854 - mse: 0.4305
64/72 [=========================>....] - ETA: 0s - loss: 0.4226 - mae: 0.5913 - mse: 0.4226
72/72 [==============================] - 1s 16ms/step - loss: 0.4127 - mae: 0.5858 - mse: 0.4127 - val_loss: 0.1605 - val_mae: 0.3391 - val_mse: 0.1605
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1785 - mae: 0.3632 - mse: 0.1785
64/72 [=========================>....] - ETA: 0s - loss: 0.1544 - mae: 0.3427 - mse: 0.1544
72/72 [==============================] - 1s 11ms/step - loss: 0.1572 - mae: 0.3440 - mse: 0.1572 - val_loss: 0.2307 - val_mae: 0.4098 - val_mse: 0.2307
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1732 - mae: 0.3395 - mse: 0.1732
64/72 [=========================>....] - ETA: 0s - loss: 0.1682 - mae: 0.3285 - mse: 0.1682
72/72 [==============================] - 1s 10ms/step - loss: 0.1636 - mae: 0.3239 - mse: 0.1636 - val_loss: 0.2062 - val_mae: 0.3683 - val_mse: 0.2062
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.1629 - mae: 0.2762 - mse: 0.1629
64/72 [=========================>....] - ETA: 0s - loss: 0.1411 - mae: 0.2798 - mse: 0.1411
72/72 [==============================] - 1s 10ms/step - loss: 0.1324 - mae: 0.2733 - mse: 0.1324 - val_loss: 0.1547 - val_mae: 0.2950 - val_mse: 0.1547
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.1325 - mae: 0.2674 - mse: 0.1325
64/72 [=========================>....] - ETA: 0s - loss: 0.1054 - mae: 0.2469 - mse: 0.1054
72/72 [==============================] - 1s 10ms/step - loss: 0.1007 - mae: 0.2410 - mse: 0.1007 - val_loss: 0.1450 - val_mae: 0.2930 - val_mse: 0.1450
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0929 - mae: 0.2229 - mse: 0.0929
64/72 [=========================>....] - ETA: 0s - loss: 0.1031 - mae: 0.2431 - mse: 0.1031
72/72 [==============================] - 1s 11ms/step - loss: 0.0953 - mae: 0.2335 - mse: 0.0953 - val_loss: 0.1487 - val_mae: 0.2964 - val_mse: 0.1487
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0692 - mae: 0.1906 - mse: 0.0692
64/72 [=========================>....] - ETA: 0s - loss: 0.0797 - mae: 0.2011 - mse: 0.0797
72/72 [==============================] - 1s 11ms/step - loss: 0.0715 - mae: 0.1859 - mse: 0.0715 - val_loss: 0.1542 - val_mae: 0.2948 - val_mse: 0.1542
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0551 - mae: 0.1635 - mse: 0.0551
64/72 [=========================>....] - ETA: 0s - loss: 0.0985 - mae: 0.2122 - mse: 0.0985
72/72 [==============================] - 1s 11ms/step - loss: 0.0980 - mae: 0.2044 - mse: 0.0980 - val_loss: 0.1510 - val_mae: 0.2910 - val_mse: 0.1510
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0960 - mae: 0.2055 - mse: 0.0960
64/72 [=========================>....] - ETA: 0s - loss: 0.0896 - mae: 0.2071 - mse: 0.0896
72/72 [==============================] - 1s 10ms/step - loss: 0.0974 - mae: 0.2178 - mse: 0.0974 - val_loss: 0.1326 - val_mae: 0.2862 - val_mse: 0.1326
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.1036 - mae: 0.2301 - mse: 0.1036
64/72 [=========================>....] - ETA: 0s - loss: 0.0832 - mae: 0.2126 - mse: 0.0832
72/72 [==============================] - 1s 10ms/step - loss: 0.0856 - mae: 0.2207 - mse: 0.0856 - val_loss: 0.1226 - val_mae: 0.2816 - val_mse: 0.1226
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0686 - mae: 0.2000 - mse: 0.0686
64/72 [=========================>....] - ETA: 0s - loss: 0.0808 - mae: 0.2157 - mse: 0.0808
72/72 [==============================] - 1s 10ms/step - loss: 0.0899 - mae: 0.2267 - mse: 0.0899 - val_loss: 0.1178 - val_mae: 0.2668 - val_mse: 0.1178
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0664 - mae: 0.1980 - mse: 0.0664
64/72 [=========================>....] - ETA: 0s - loss: 0.0911 - mae: 0.2195 - mse: 0.0911
72/72 [==============================] - 1s 10ms/step - loss: 0.0888 - mae: 0.2182 - mse: 0.0888 - val_loss: 0.1153 - val_mae: 0.2536 - val_mse: 0.1153
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.1111 - mae: 0.2507 - mse: 0.1111
64/72 [=========================>....] - ETA: 0s - loss: 0.0970 - mae: 0.2255 - mse: 0.0970
72/72 [==============================] - 1s 10ms/step - loss: 0.0910 - mae: 0.2187 - mse: 0.0910 - val_loss: 0.1112 - val_mae: 0.2472 - val_mse: 0.1112
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0445 - mae: 0.1595 - mse: 0.0445
64/72 [=========================>....] - ETA: 0s - loss: 0.0677 - mae: 0.1916 - mse: 0.0677
72/72 [==============================] - 1s 10ms/step - loss: 0.0623 - mae: 0.1841 - mse: 0.0623 - val_loss: 0.1075 - val_mae: 0.2442 - val_mse: 0.1075
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0773 - mae: 0.1919 - mse: 0.0773
64/72 [=========================>....] - ETA: 0s - loss: 0.0694 - mae: 0.1817 - mse: 0.0694
72/72 [==============================] - 1s 10ms/step - loss: 0.0642 - mae: 0.1761 - mse: 0.0642 - val_loss: 0.1046 - val_mae: 0.2403 - val_mse: 0.1046
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0683 - mae: 0.1889 - mse: 0.0683
64/72 [=========================>....] - ETA: 0s - loss: 0.0733 - mae: 0.1955 - mse: 0.0733
72/72 [==============================] - 1s 10ms/step - loss: 0.0754 - mae: 0.1936 - mse: 0.0754 - val_loss: 0.1017 - val_mae: 0.2364 - val_mse: 0.1017
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0871 - mae: 0.2090 - mse: 0.0871
64/72 [=========================>....] - ETA: 0s - loss: 0.0672 - mae: 0.1819 - mse: 0.0672
72/72 [==============================] - 1s 10ms/step - loss: 0.0617 - mae: 0.1724 - mse: 0.0617 - val_loss: 0.0942 - val_mae: 0.2374 - val_mse: 0.0942
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0387 - mae: 0.1338 - mse: 0.0387
64/72 [=========================>....] - ETA: 0s - loss: 0.0650 - mae: 0.1720 - mse: 0.0650
72/72 [==============================] - 1s 10ms/step - loss: 0.0593 - mae: 0.1642 - mse: 0.0593 - val_loss: 0.0900 - val_mae: 0.2316 - val_mse: 0.0900
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0560 - mae: 0.1701 - mse: 0.0560
64/72 [=========================>....] - ETA: 0s - loss: 0.0501 - mae: 0.1574 - mse: 0.0501
72/72 [==============================] - 1s 10ms/step - loss: 0.0572 - mae: 0.1669 - mse: 0.0572 - val_loss: 0.0858 - val_mae: 0.2239 - val_mse: 0.0858
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0779 - mae: 0.2098 - mse: 0.0779
64/72 [=========================>....] - ETA: 0s - loss: 0.0582 - mae: 0.1811 - mse: 0.0582
72/72 [==============================] - 1s 10ms/step - loss: 0.0629 - mae: 0.1884 - mse: 0.0629 - val_loss: 0.0798 - val_mae: 0.2172 - val_mse: 0.0798
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0421 - mae: 0.1301 - mse: 0.0421
64/72 [=========================>....] - ETA: 0s - loss: 0.0531 - mae: 0.1562 - mse: 0.0531
72/72 [==============================] - 1s 10ms/step - loss: 0.0571 - mae: 0.1656 - mse: 0.0571 - val_loss: 0.0728 - val_mae: 0.2121 - val_mse: 0.0728
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0491 - mae: 0.1627 - mse: 0.0491
64/72 [=========================>....] - ETA: 0s - loss: 0.0374 - mae: 0.1414 - mse: 0.0374
72/72 [==============================] - 1s 10ms/step - loss: 0.0490 - mae: 0.1600 - mse: 0.0490 - val_loss: 0.0653 - val_mae: 0.1974 - val_mse: 0.0653
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0618 - mae: 0.1876 - mse: 0.0618
64/72 [=========================>....] - ETA: 0s - loss: 0.0529 - mae: 0.1712 - mse: 0.0529
72/72 [==============================] - 1s 10ms/step - loss: 0.0526 - mae: 0.1717 - mse: 0.0526 - val_loss: 0.0574 - val_mae: 0.1847 - val_mse: 0.0574
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0507 - mae: 0.1588 - mse: 0.0507
64/72 [=========================>....] - ETA: 0s - loss: 0.0453 - mae: 0.1493 - mse: 0.0453
72/72 [==============================] - 1s 9ms/step - loss: 0.0426 - mae: 0.1448 - mse: 0.0426 - val_loss: 0.0505 - val_mae: 0.1714 - val_mse: 0.0505
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0315 - mae: 0.1418 - mse: 0.0315
64/72 [=========================>....] - ETA: 0s - loss: 0.0376 - mae: 0.1499 - mse: 0.0376
72/72 [==============================] - 1s 8ms/step - loss: 0.0437 - mae: 0.1608 - mse: 0.0437 - val_loss: 0.0460 - val_mae: 0.1601 - val_mse: 0.0460
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0396 - mae: 0.1495 - mse: 0.0396
64/72 [=========================>....] - ETA: 0s - loss: 0.0476 - mae: 0.1514 - mse: 0.0476
72/72 [==============================] - 1s 7ms/step - loss: 0.0438 - mae: 0.1435 - mse: 0.0438 - val_loss: 0.0429 - val_mae: 0.1661 - val_mse: 0.0429
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0371 - mae: 0.1435 - mse: 0.0371
64/72 [=========================>....] - ETA: 0s - loss: 0.0436 - mae: 0.1429 - mse: 0.0436
72/72 [==============================] - 1s 8ms/step - loss: 0.0420 - mae: 0.1409 - mse: 0.0420 - val_loss: 0.0428 - val_mae: 0.1795 - val_mse: 0.0428
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0388 - mae: 0.1503 - mse: 0.0388
64/72 [=========================>....] - ETA: 0s - loss: 0.0454 - mae: 0.1533 - mse: 0.0454
72/72 [==============================] - 1s 8ms/step - loss: 0.0419 - mae: 0.1469 - mse: 0.0419 - val_loss: 0.0459 - val_mae: 0.1934 - val_mse: 0.0459
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0594 - mae: 0.1917 - mse: 0.0594
64/72 [=========================>....] - ETA: 0s - loss: 0.0454 - mae: 0.1671 - mse: 0.0454
72/72 [==============================] - 1s 8ms/step - loss: 0.0469 - mae: 0.1660 - mse: 0.0469 - val_loss: 0.0422 - val_mae: 0.1732 - val_mse: 0.0422
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0365 - mae: 0.1424 - mse: 0.0365
64/72 [=========================>....] - ETA: 0s - loss: 0.0341 - mae: 0.1380 - mse: 0.0341
72/72 [==============================] - 1s 8ms/step - loss: 0.0307 - mae: 0.1272 - mse: 0.0307 - val_loss: 0.0379 - val_mae: 0.1423 - val_mse: 0.0379
Saving trained model...
119
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         23.79432678]
average prediction= [3.228557]
baseline= 7.4523809523809526
eachuser= [0. 0. 0. 0. 0. 3.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 7.9314422607421875
['train-height-16.py', '0']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
155 5
155 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
155 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
155 16
155 17
2_155_65_16_csi_a16_21.dat
155 19
155 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
155 25
2_155_65_16_csi_a16_10.dat
155 27
155 28
2_155_65_16_csi_a16_19.dat
155 30
170 31
2_170_60_16_csi_a16_3.dat
170 33
170 34
170 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
170 38
170 39
170 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_16_csi_a16_19.dat
165 90
2_165_50_16_csi_a16_3.dat
165 92
165 93
2_165_50_16_csi_a16_27.dat
165 95
165 96
165 97
165 98
165 99
165 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
175 105
1_175_70_16_csi_a16_23.dat
175 107
175 108
175 109
1_175_70_16_csi_a16_14.dat
175 111
175 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
175 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
175 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
175 123
175 124
1_175_70_16_csi_a16_5.dat
175 126
1_175_70_16_csi_a16_18.dat
175 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
180 132
1_180_85_16_csi_a16_7.dat
180 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
180 139
180 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
180 148
1_180_85_16_csi_a16_17.dat
180 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
180 155
180 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
180 161
180 162
180 163
1_180_75_16_csi_a16_18.dat
180 165
180 166
1_180_75_16_csi_a16_16.dat
180 168
1_180_75_16_csi_a16_9.dat
180 170
180 171
1_180_75_16_csi_a16_17.dat
180 173
180 174
1_180_75_16_csi_a16_13.dat
180 176
1_180_75_16_csi_a16_10.dat
180 178
180 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
180 183
1_180_75_16_csi_a16_5.dat
180 185
180 186
180 187
180 188
180 189
180 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
173 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
173 197
173 198
173 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
173 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
173 205
173 206
173 207
173 208
173 209
173 210
1_173_85_16_csi_a16_29.dat
173 212
1_173_85_16_csi_a16_17.dat
173 214
173 215
1_173_85_16_csi_a16_22.dat
173 217
1_173_85_16_csi_a16_26.dat
173 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173 173 173 173 173
 173 173 173 173 173 173 173 173 173 173 173]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.6317 - mae: 0.7027 - mse: 0.6317
64/72 [=========================>....] - ETA: 0s - loss: 0.5664 - mae: 0.6826 - mse: 0.5664
72/72 [==============================] - 1s 14ms/step - loss: 0.5434 - mae: 0.6663 - mse: 0.5434 - val_loss: 0.2148 - val_mae: 0.4449 - val_mse: 0.2148
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1787 - mae: 0.3673 - mse: 0.1787
64/72 [=========================>....] - ETA: 0s - loss: 0.1735 - mae: 0.3551 - mse: 0.1735
72/72 [==============================] - 1s 7ms/step - loss: 0.1580 - mae: 0.3343 - mse: 0.1580 - val_loss: 0.0276 - val_mae: 0.1420 - val_mse: 0.0276
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.2305 - mae: 0.3946 - mse: 0.2305
64/72 [=========================>....] - ETA: 0s - loss: 0.1836 - mae: 0.3406 - mse: 0.1836
72/72 [==============================] - 1s 8ms/step - loss: 0.1946 - mae: 0.3462 - mse: 0.1946 - val_loss: 0.0263 - val_mae: 0.1407 - val_mse: 0.0263
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.1080 - mae: 0.2816 - mse: 0.1080
64/72 [=========================>....] - ETA: 0s - loss: 0.1350 - mae: 0.2908 - mse: 0.1350
72/72 [==============================] - 1s 8ms/step - loss: 0.1378 - mae: 0.2908 - mse: 0.1378 - val_loss: 0.0682 - val_mae: 0.2194 - val_mse: 0.0682
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.1684 - mae: 0.3327 - mse: 0.1684
64/72 [=========================>....] - ETA: 0s - loss: 0.1206 - mae: 0.2735 - mse: 0.1206
72/72 [==============================] - 1s 8ms/step - loss: 0.1177 - mae: 0.2723 - mse: 0.1177 - val_loss: 0.1269 - val_mae: 0.3279 - val_mse: 0.1269
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0782 - mae: 0.2206 - mse: 0.0782
64/72 [=========================>....] - ETA: 0s - loss: 0.0874 - mae: 0.2398 - mse: 0.0874
72/72 [==============================] - 1s 8ms/step - loss: 0.0923 - mae: 0.2473 - mse: 0.0923 - val_loss: 0.1301 - val_mae: 0.3333 - val_mse: 0.1301
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.1214 - mae: 0.2944 - mse: 0.1214
64/72 [=========================>....] - ETA: 0s - loss: 0.1140 - mae: 0.2854 - mse: 0.1140
72/72 [==============================] - 1s 8ms/step - loss: 0.1068 - mae: 0.2732 - mse: 0.1068 - val_loss: 0.1004 - val_mae: 0.2849 - val_mse: 0.1004
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0753 - mae: 0.2251 - mse: 0.0753
64/72 [=========================>....] - ETA: 0s - loss: 0.0996 - mae: 0.2631 - mse: 0.0996
72/72 [==============================] - 1s 8ms/step - loss: 0.0923 - mae: 0.2494 - mse: 0.0923 - val_loss: 0.0623 - val_mae: 0.2107 - val_mse: 0.0623
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.1173 - mae: 0.2597 - mse: 0.1173
64/72 [=========================>....] - ETA: 0s - loss: 0.0994 - mae: 0.2389 - mse: 0.0994
72/72 [==============================] - 1s 8ms/step - loss: 0.0927 - mae: 0.2311 - mse: 0.0927 - val_loss: 0.0374 - val_mae: 0.1752 - val_mse: 0.0374
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.1082 - mae: 0.2314 - mse: 0.1082
64/72 [=========================>....] - ETA: 0s - loss: 0.0868 - mae: 0.2125 - mse: 0.0868
72/72 [==============================] - 1s 8ms/step - loss: 0.0801 - mae: 0.2036 - mse: 0.0801 - val_loss: 0.0276 - val_mae: 0.1517 - val_mse: 0.0276
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0586 - mae: 0.1769 - mse: 0.0586
64/72 [=========================>....] - ETA: 0s - loss: 0.0838 - mae: 0.2009 - mse: 0.0838
72/72 [==============================] - 1s 9ms/step - loss: 0.1073 - mae: 0.2270 - mse: 0.1073 - val_loss: 0.0326 - val_mae: 0.1630 - val_mse: 0.0326
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.1061 - mae: 0.2308 - mse: 0.1061
64/72 [=========================>....] - ETA: 0s - loss: 0.1004 - mae: 0.2267 - mse: 0.1004
72/72 [==============================] - 1s 10ms/step - loss: 0.0924 - mae: 0.2162 - mse: 0.0924 - val_loss: 0.0590 - val_mae: 0.2029 - val_mse: 0.0590
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0908 - mae: 0.2090 - mse: 0.0908
64/72 [=========================>....] - ETA: 0s - loss: 0.0779 - mae: 0.2017 - mse: 0.0779
72/72 [==============================] - 1s 10ms/step - loss: 0.0791 - mae: 0.2077 - mse: 0.0791 - val_loss: 0.0807 - val_mae: 0.2534 - val_mse: 0.0807
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0725 - mae: 0.2013 - mse: 0.0725
64/72 [=========================>....] - ETA: 0s - loss: 0.0829 - mae: 0.2120 - mse: 0.0829
72/72 [==============================] - 1s 8ms/step - loss: 0.0865 - mae: 0.2208 - mse: 0.0865 - val_loss: 0.0767 - val_mae: 0.2469 - val_mse: 0.0767
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0804 - mae: 0.2249 - mse: 0.0804
64/72 [=========================>....] - ETA: 0s - loss: 0.0858 - mae: 0.2269 - mse: 0.0858
72/72 [==============================] - 1s 8ms/step - loss: 0.0787 - mae: 0.2162 - mse: 0.0787 - val_loss: 0.0517 - val_mae: 0.1911 - val_mse: 0.0517
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0853 - mae: 0.1951 - mse: 0.0853
64/72 [=========================>....] - ETA: 0s - loss: 0.0929 - mae: 0.2160 - mse: 0.0929
72/72 [==============================] - 1s 8ms/step - loss: 0.0940 - mae: 0.2176 - mse: 0.0940 - val_loss: 0.0397 - val_mae: 0.1617 - val_mse: 0.0397
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0835 - mae: 0.1929 - mse: 0.0835
64/72 [=========================>....] - ETA: 0s - loss: 0.0821 - mae: 0.1927 - mse: 0.0821
72/72 [==============================] - 1s 7ms/step - loss: 0.0805 - mae: 0.1946 - mse: 0.0805 - val_loss: 0.0488 - val_mae: 0.1869 - val_mse: 0.0488
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0689 - mae: 0.1725 - mse: 0.0689
64/72 [=========================>....] - ETA: 0s - loss: 0.0788 - mae: 0.1887 - mse: 0.0788
72/72 [==============================] - 1s 8ms/step - loss: 0.0738 - mae: 0.1854 - mse: 0.0738 - val_loss: 0.0588 - val_mae: 0.2134 - val_mse: 0.0588
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0877 - mae: 0.2350 - mse: 0.0877
64/72 [=========================>....] - ETA: 0s - loss: 0.0778 - mae: 0.2143 - mse: 0.0778
72/72 [==============================] - 1s 8ms/step - loss: 0.0725 - mae: 0.2071 - mse: 0.0725 - val_loss: 0.0517 - val_mae: 0.1960 - val_mse: 0.0517
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0589 - mae: 0.1694 - mse: 0.0589
64/72 [=========================>....] - ETA: 0s - loss: 0.0641 - mae: 0.1735 - mse: 0.0641
72/72 [==============================] - 1s 8ms/step - loss: 0.0616 - mae: 0.1727 - mse: 0.0616 - val_loss: 0.0404 - val_mae: 0.1626 - val_mse: 0.0404
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0496 - mae: 0.1524 - mse: 0.0496
64/72 [=========================>....] - ETA: 0s - loss: 0.0530 - mae: 0.1587 - mse: 0.0530
72/72 [==============================] - 1s 8ms/step - loss: 0.0564 - mae: 0.1656 - mse: 0.0564 - val_loss: 0.0425 - val_mae: 0.1685 - val_mse: 0.0425
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0535 - mae: 0.1748 - mse: 0.0535
64/72 [=========================>....] - ETA: 0s - loss: 0.0565 - mae: 0.1631 - mse: 0.0565
72/72 [==============================] - 1s 8ms/step - loss: 0.0543 - mae: 0.1602 - mse: 0.0543 - val_loss: 0.0599 - val_mae: 0.2144 - val_mse: 0.0599
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0493 - mae: 0.1704 - mse: 0.0493
64/72 [=========================>....] - ETA: 0s - loss: 0.0525 - mae: 0.1759 - mse: 0.0525
72/72 [==============================] - 1s 8ms/step - loss: 0.0571 - mae: 0.1854 - mse: 0.0571 - val_loss: 0.0615 - val_mae: 0.2189 - val_mse: 0.0615
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0673 - mae: 0.1859 - mse: 0.0673
64/72 [=========================>....] - ETA: 0s - loss: 0.0580 - mae: 0.1743 - mse: 0.0580
72/72 [==============================] - 1s 8ms/step - loss: 0.0544 - mae: 0.1692 - mse: 0.0544 - val_loss: 0.0590 - val_mae: 0.2120 - val_mse: 0.0590
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0661 - mae: 0.1981 - mse: 0.0661
64/72 [=========================>....] - ETA: 0s - loss: 0.0585 - mae: 0.1839 - mse: 0.0585
72/72 [==============================] - 1s 8ms/step - loss: 0.0534 - mae: 0.1728 - mse: 0.0534 - val_loss: 0.0431 - val_mae: 0.1697 - val_mse: 0.0431
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0705 - mae: 0.1783 - mse: 0.0705
64/72 [=========================>....] - ETA: 0s - loss: 0.0575 - mae: 0.1602 - mse: 0.0575
72/72 [==============================] - 1s 8ms/step - loss: 0.0578 - mae: 0.1589 - mse: 0.0578 - val_loss: 0.0363 - val_mae: 0.1499 - val_mse: 0.0363
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0506 - mae: 0.1520 - mse: 0.0506
64/72 [=========================>....] - ETA: 0s - loss: 0.0461 - mae: 0.1512 - mse: 0.0461
72/72 [==============================] - 1s 8ms/step - loss: 0.0460 - mae: 0.1488 - mse: 0.0460 - val_loss: 0.0672 - val_mae: 0.2338 - val_mse: 0.0672
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0501 - mae: 0.1707 - mse: 0.0501
64/72 [=========================>....] - ETA: 0s - loss: 0.0432 - mae: 0.1524 - mse: 0.0432
72/72 [==============================] - 1s 8ms/step - loss: 0.0430 - mae: 0.1530 - mse: 0.0430 - val_loss: 0.0927 - val_mae: 0.2839 - val_mse: 0.0927
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0483 - mae: 0.1813 - mse: 0.0483
64/72 [=========================>....] - ETA: 0s - loss: 0.0537 - mae: 0.1800 - mse: 0.0537
72/72 [==============================] - 1s 8ms/step - loss: 0.0544 - mae: 0.1824 - mse: 0.0544 - val_loss: 0.0591 - val_mae: 0.2161 - val_mse: 0.0591
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0440 - mae: 0.1390 - mse: 0.0440
64/72 [=========================>....] - ETA: 0s - loss: 0.0429 - mae: 0.1472 - mse: 0.0429
72/72 [==============================] - 1s 8ms/step - loss: 0.0415 - mae: 0.1456 - mse: 0.0415 - val_loss: 0.0392 - val_mae: 0.1650 - val_mse: 0.0392
Saving trained model...
119
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         29.08018494]
average prediction= [3.9290226]
baseline= 8.404761904761905
eachuser= [0. 0. 0. 0. 0. 4.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 7.270046234130859
['train-height-16.py', '0']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
155 5
155 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
155 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
155 16
155 17
2_155_65_16_csi_a16_21.dat
155 19
155 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
155 25
2_155_65_16_csi_a16_10.dat
155 27
155 28
2_155_65_16_csi_a16_19.dat
155 30
170 31
2_170_60_16_csi_a16_3.dat
170 33
170 34
170 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
170 38
170 39
170 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_16_csi_a16_19.dat
165 90
2_165_50_16_csi_a16_3.dat
165 92
165 93
2_165_50_16_csi_a16_27.dat
165 95
165 96
165 97
165 98
165 99
165 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
175 105
1_175_70_16_csi_a16_23.dat
175 107
175 108
175 109
1_175_70_16_csi_a16_14.dat
175 111
175 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
175 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
175 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
175 123
175 124
1_175_70_16_csi_a16_5.dat
175 126
1_175_70_16_csi_a16_18.dat
175 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
180 132
1_180_85_16_csi_a16_7.dat
180 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
180 139
180 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
180 148
1_180_85_16_csi_a16_17.dat
180 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
180 155
180 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
180 161
180 162
180 163
1_180_75_16_csi_a16_18.dat
180 165
180 166
1_180_75_16_csi_a16_16.dat
180 168
1_180_75_16_csi_a16_9.dat
180 170
180 171
1_180_75_16_csi_a16_17.dat
180 173
180 174
1_180_75_16_csi_a16_13.dat
180 176
1_180_75_16_csi_a16_10.dat
180 178
180 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
180 183
1_180_75_16_csi_a16_5.dat
180 185
180 186
180 187
180 188
180 189
180 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
173 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
173 197
173 198
173 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
173 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
173 205
173 206
173 207
173 208
173 209
173 210
1_173_85_16_csi_a16_29.dat
173 212
1_173_85_16_csi_a16_17.dat
173 214
173 215
1_173_85_16_csi_a16_22.dat
173 217
1_173_85_16_csi_a16_26.dat
173 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173 173 173 173 173
 173 173 173 173 173 173 173 173 173 173 173]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.5436 - mae: 0.6585 - mse: 0.5436
64/72 [=========================>....] - ETA: 0s - loss: 0.4737 - mae: 0.6161 - mse: 0.4737
72/72 [==============================] - 1s 13ms/step - loss: 0.4474 - mae: 0.5942 - mse: 0.4474 - val_loss: 0.1306 - val_mae: 0.3071 - val_mse: 0.1306
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1949 - mae: 0.4006 - mse: 0.1949
64/72 [=========================>....] - ETA: 0s - loss: 0.2406 - mae: 0.4385 - mse: 0.2406
72/72 [==============================] - 0s 6ms/step - loss: 0.2183 - mae: 0.4089 - mse: 0.2183 - val_loss: 0.1919 - val_mae: 0.3910 - val_mse: 0.1919
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.2834 - mae: 0.4007 - mse: 0.2834
64/72 [=========================>....] - ETA: 0s - loss: 0.3017 - mae: 0.4304 - mse: 0.3017
72/72 [==============================] - 0s 6ms/step - loss: 0.2712 - mae: 0.3977 - mse: 0.2712 - val_loss: 0.1299 - val_mae: 0.3239 - val_mse: 0.1299
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.1681 - mae: 0.3484 - mse: 0.1681
64/72 [=========================>....] - ETA: 0s - loss: 0.1614 - mae: 0.3429 - mse: 0.1614
72/72 [==============================] - 0s 5ms/step - loss: 0.1648 - mae: 0.3513 - mse: 0.1648 - val_loss: 0.1060 - val_mae: 0.2751 - val_mse: 0.1060
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.1606 - mae: 0.3480 - mse: 0.1606
64/72 [=========================>....] - ETA: 0s - loss: 0.1539 - mae: 0.3457 - mse: 0.1539
72/72 [==============================] - 0s 6ms/step - loss: 0.1472 - mae: 0.3348 - mse: 0.1472 - val_loss: 0.1099 - val_mae: 0.2881 - val_mse: 0.1099
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.1270 - mae: 0.3114 - mse: 0.1270
64/72 [=========================>....] - ETA: 0s - loss: 0.1207 - mae: 0.2946 - mse: 0.1207
72/72 [==============================] - 0s 6ms/step - loss: 0.1248 - mae: 0.3007 - mse: 0.1248 - val_loss: 0.1061 - val_mae: 0.2814 - val_mse: 0.1061
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0985 - mae: 0.2551 - mse: 0.0985
64/72 [=========================>....] - ETA: 0s - loss: 0.1265 - mae: 0.3035 - mse: 0.1265
72/72 [==============================] - 0s 6ms/step - loss: 0.1188 - mae: 0.2928 - mse: 0.1188 - val_loss: 0.0925 - val_mae: 0.2564 - val_mse: 0.0925
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.1206 - mae: 0.2838 - mse: 0.1206
64/72 [=========================>....] - ETA: 0s - loss: 0.1037 - mae: 0.2633 - mse: 0.1037
72/72 [==============================] - 0s 6ms/step - loss: 0.1005 - mae: 0.2616 - mse: 0.1005 - val_loss: 0.0890 - val_mae: 0.2249 - val_mse: 0.0890
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.1138 - mae: 0.2597 - mse: 0.1138
64/72 [=========================>....] - ETA: 0s - loss: 0.0887 - mae: 0.2272 - mse: 0.0887
72/72 [==============================] - 0s 6ms/step - loss: 0.0880 - mae: 0.2274 - mse: 0.0880 - val_loss: 0.0939 - val_mae: 0.2305 - val_mse: 0.0939
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.1178 - mae: 0.2661 - mse: 0.1178
64/72 [=========================>....] - ETA: 0s - loss: 0.1176 - mae: 0.2606 - mse: 0.1176
72/72 [==============================] - 0s 6ms/step - loss: 0.1068 - mae: 0.2462 - mse: 0.1068 - val_loss: 0.0791 - val_mae: 0.2066 - val_mse: 0.0791
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0859 - mae: 0.2200 - mse: 0.0859
64/72 [=========================>....] - ETA: 0s - loss: 0.0784 - mae: 0.1965 - mse: 0.0784
72/72 [==============================] - 0s 6ms/step - loss: 0.0762 - mae: 0.1978 - mse: 0.0762 - val_loss: 0.0702 - val_mae: 0.2077 - val_mse: 0.0702
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0516 - mae: 0.1709 - mse: 0.0516
64/72 [=========================>....] - ETA: 0s - loss: 0.0695 - mae: 0.2046 - mse: 0.0695
72/72 [==============================] - 0s 6ms/step - loss: 0.0743 - mae: 0.2146 - mse: 0.0743 - val_loss: 0.0687 - val_mae: 0.2274 - val_mse: 0.0687
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0487 - mae: 0.1793 - mse: 0.0487
64/72 [=========================>....] - ETA: 0s - loss: 0.0624 - mae: 0.1928 - mse: 0.0624
72/72 [==============================] - 0s 6ms/step - loss: 0.0627 - mae: 0.1944 - mse: 0.0627 - val_loss: 0.0599 - val_mae: 0.2071 - val_mse: 0.0599
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0682 - mae: 0.1978 - mse: 0.0682
64/72 [=========================>....] - ETA: 0s - loss: 0.0579 - mae: 0.1794 - mse: 0.0579
72/72 [==============================] - 0s 6ms/step - loss: 0.0644 - mae: 0.1912 - mse: 0.0644 - val_loss: 0.0504 - val_mae: 0.1657 - val_mse: 0.0504
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0263 - mae: 0.1285 - mse: 0.0263
64/72 [=========================>....] - ETA: 0s - loss: 0.0506 - mae: 0.1674 - mse: 0.0506
72/72 [==============================] - 1s 8ms/step - loss: 0.0521 - mae: 0.1704 - mse: 0.0521 - val_loss: 0.0460 - val_mae: 0.1572 - val_mse: 0.0460
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0466 - mae: 0.1576 - mse: 0.0466
64/72 [=========================>....] - ETA: 0s - loss: 0.0626 - mae: 0.1780 - mse: 0.0626
72/72 [==============================] - 1s 9ms/step - loss: 0.0637 - mae: 0.1827 - mse: 0.0637 - val_loss: 0.0384 - val_mae: 0.1506 - val_mse: 0.0384
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0506 - mae: 0.1578 - mse: 0.0506
64/72 [=========================>....] - ETA: 0s - loss: 0.0529 - mae: 0.1630 - mse: 0.0529
72/72 [==============================] - 1s 8ms/step - loss: 0.0516 - mae: 0.1661 - mse: 0.0516 - val_loss: 0.0367 - val_mae: 0.1646 - val_mse: 0.0367
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0380 - mae: 0.1459 - mse: 0.0380
64/72 [=========================>....] - ETA: 0s - loss: 0.0413 - mae: 0.1561 - mse: 0.0413
72/72 [==============================] - 1s 8ms/step - loss: 0.0462 - mae: 0.1628 - mse: 0.0462 - val_loss: 0.0338 - val_mae: 0.1579 - val_mse: 0.0338
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0832 - mae: 0.2182 - mse: 0.0832
64/72 [=========================>....] - ETA: 0s - loss: 0.0619 - mae: 0.1864 - mse: 0.0619
72/72 [==============================] - 1s 8ms/step - loss: 0.0612 - mae: 0.1892 - mse: 0.0612 - val_loss: 0.0294 - val_mae: 0.1378 - val_mse: 0.0294
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0553 - mae: 0.1577 - mse: 0.0553
64/72 [=========================>....] - ETA: 0s - loss: 0.0492 - mae: 0.1543 - mse: 0.0492
72/72 [==============================] - 1s 8ms/step - loss: 0.0524 - mae: 0.1625 - mse: 0.0524 - val_loss: 0.0295 - val_mae: 0.1410 - val_mse: 0.0295
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0562 - mae: 0.1744 - mse: 0.0562
64/72 [=========================>....] - ETA: 0s - loss: 0.0484 - mae: 0.1592 - mse: 0.0484
72/72 [==============================] - 1s 8ms/step - loss: 0.0442 - mae: 0.1519 - mse: 0.0442 - val_loss: 0.0370 - val_mae: 0.1790 - val_mse: 0.0370
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0482 - mae: 0.1662 - mse: 0.0482
64/72 [=========================>....] - ETA: 0s - loss: 0.0487 - mae: 0.1701 - mse: 0.0487
72/72 [==============================] - 1s 8ms/step - loss: 0.0489 - mae: 0.1716 - mse: 0.0489 - val_loss: 0.0347 - val_mae: 0.1718 - val_mse: 0.0347
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0692 - mae: 0.2043 - mse: 0.0692
64/72 [=========================>....] - ETA: 0s - loss: 0.0515 - mae: 0.1736 - mse: 0.0515
72/72 [==============================] - 1s 8ms/step - loss: 0.0474 - mae: 0.1637 - mse: 0.0474 - val_loss: 0.0301 - val_mae: 0.1518 - val_mse: 0.0301
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0474 - mae: 0.1631 - mse: 0.0474
64/72 [=========================>....] - ETA: 0s - loss: 0.0546 - mae: 0.1677 - mse: 0.0546
72/72 [==============================] - 1s 8ms/step - loss: 0.0541 - mae: 0.1712 - mse: 0.0541 - val_loss: 0.0340 - val_mae: 0.1694 - val_mse: 0.0340
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0586 - mae: 0.1842 - mse: 0.0586
64/72 [=========================>....] - ETA: 0s - loss: 0.0459 - mae: 0.1596 - mse: 0.0459
72/72 [==============================] - 1s 8ms/step - loss: 0.0481 - mae: 0.1620 - mse: 0.0481 - val_loss: 0.0346 - val_mae: 0.1721 - val_mse: 0.0346
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0420 - mae: 0.1494 - mse: 0.0420
64/72 [=========================>....] - ETA: 0s - loss: 0.0402 - mae: 0.1479 - mse: 0.0402
72/72 [==============================] - 1s 8ms/step - loss: 0.0424 - mae: 0.1532 - mse: 0.0424 - val_loss: 0.0295 - val_mae: 0.1517 - val_mse: 0.0295
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0473 - mae: 0.1588 - mse: 0.0473
64/72 [=========================>....] - ETA: 0s - loss: 0.0408 - mae: 0.1538 - mse: 0.0408
72/72 [==============================] - 1s 8ms/step - loss: 0.0409 - mae: 0.1520 - mse: 0.0409 - val_loss: 0.0291 - val_mae: 0.1486 - val_mse: 0.0291
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0572 - mae: 0.1705 - mse: 0.0572
64/72 [=========================>....] - ETA: 0s - loss: 0.0420 - mae: 0.1476 - mse: 0.0420
72/72 [==============================] - 1s 8ms/step - loss: 0.0418 - mae: 0.1456 - mse: 0.0418 - val_loss: 0.0337 - val_mae: 0.1654 - val_mse: 0.0337
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0481 - mae: 0.1746 - mse: 0.0481
64/72 [=========================>....] - ETA: 0s - loss: 0.0416 - mae: 0.1626 - mse: 0.0416
72/72 [==============================] - 1s 8ms/step - loss: 0.0391 - mae: 0.1576 - mse: 0.0391 - val_loss: 0.0337 - val_mae: 0.1645 - val_mse: 0.0337
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0281 - mae: 0.1258 - mse: 0.0281
64/72 [=========================>....] - ETA: 0s - loss: 0.0432 - mae: 0.1554 - mse: 0.0432
72/72 [==============================] - 1s 8ms/step - loss: 0.0426 - mae: 0.1520 - mse: 0.0426 - val_loss: 0.0333 - val_mae: 0.1630 - val_mse: 0.0333
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         0.         0.         6.64952087]
average prediction= [5.427178]
baseline= 6.309523809523809
eachuser= [0. 0. 0. 0. 0. 1.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 6.6495208740234375
['train-height-16.py', '0']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
155 5
155 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
155 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
155 16
155 17
2_155_65_16_csi_a16_21.dat
155 19
155 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
155 25
2_155_65_16_csi_a16_10.dat
155 27
155 28
2_155_65_16_csi_a16_19.dat
155 30
170 31
2_170_60_16_csi_a16_3.dat
170 33
170 34
170 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
170 38
170 39
170 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_16_csi_a16_19.dat
165 90
2_165_50_16_csi_a16_3.dat
165 92
165 93
2_165_50_16_csi_a16_27.dat
165 95
165 96
165 97
165 98
165 99
165 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
175 105
1_175_70_16_csi_a16_23.dat
175 107
175 108
175 109
1_175_70_16_csi_a16_14.dat
175 111
175 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
175 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
175 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
175 123
175 124
1_175_70_16_csi_a16_5.dat
175 126
1_175_70_16_csi_a16_18.dat
175 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
180 132
1_180_85_16_csi_a16_7.dat
180 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
180 139
180 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
180 148
1_180_85_16_csi_a16_17.dat
180 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
180 155
180 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
180 161
180 162
180 163
1_180_75_16_csi_a16_18.dat
180 165
180 166
1_180_75_16_csi_a16_16.dat
180 168
1_180_75_16_csi_a16_9.dat
180 170
180 171
1_180_75_16_csi_a16_17.dat
180 173
180 174
1_180_75_16_csi_a16_13.dat
180 176
1_180_75_16_csi_a16_10.dat
180 178
180 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
180 183
1_180_75_16_csi_a16_5.dat
180 185
180 186
180 187
180 188
180 189
180 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
173 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
173 197
173 198
173 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
173 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
173 205
173 206
173 207
173 208
173 209
173 210
1_173_85_16_csi_a16_29.dat
173 212
1_173_85_16_csi_a16_17.dat
173 214
173 215
1_173_85_16_csi_a16_22.dat
173 217
1_173_85_16_csi_a16_26.dat
173 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173 173 173 173 173
 173 173 173 173 173 173 173 173 173 173 173]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.4750 - mae: 0.6096 - mse: 0.4750
64/72 [=========================>....] - ETA: 0s - loss: 0.3614 - mae: 0.5101 - mse: 0.3614
72/72 [==============================] - 1s 11ms/step - loss: 0.3523 - mae: 0.4991 - mse: 0.3523 - val_loss: 0.0854 - val_mae: 0.2763 - val_mse: 0.0854
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1458 - mae: 0.3200 - mse: 0.1458
64/72 [=========================>....] - ETA: 0s - loss: 0.1273 - mae: 0.2968 - mse: 0.1273
72/72 [==============================] - 0s 6ms/step - loss: 0.1295 - mae: 0.2958 - mse: 0.1295 - val_loss: 0.0674 - val_mae: 0.2417 - val_mse: 0.0674
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1442 - mae: 0.3076 - mse: 0.1442
64/72 [=========================>....] - ETA: 0s - loss: 0.1205 - mae: 0.2815 - mse: 0.1205
72/72 [==============================] - 0s 6ms/step - loss: 0.1216 - mae: 0.2841 - mse: 0.1216 - val_loss: 0.0926 - val_mae: 0.2126 - val_mse: 0.0926
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.1230 - mae: 0.3031 - mse: 0.1230
64/72 [=========================>....] - ETA: 0s - loss: 0.1081 - mae: 0.2736 - mse: 0.1081
72/72 [==============================] - 0s 6ms/step - loss: 0.1153 - mae: 0.2839 - mse: 0.1153 - val_loss: 0.1106 - val_mae: 0.2558 - val_mse: 0.1106
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.1100 - mae: 0.2883 - mse: 0.1100
64/72 [=========================>....] - ETA: 0s - loss: 0.1235 - mae: 0.2998 - mse: 0.1235
72/72 [==============================] - 0s 6ms/step - loss: 0.1234 - mae: 0.2956 - mse: 0.1234 - val_loss: 0.0621 - val_mae: 0.1687 - val_mse: 0.0621
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0907 - mae: 0.2453 - mse: 0.0907
64/72 [=========================>....] - ETA: 0s - loss: 0.0753 - mae: 0.2183 - mse: 0.0753
72/72 [==============================] - 0s 6ms/step - loss: 0.0730 - mae: 0.2160 - mse: 0.0730 - val_loss: 0.0296 - val_mae: 0.1284 - val_mse: 0.0296
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.1330 - mae: 0.2906 - mse: 0.1330
64/72 [=========================>....] - ETA: 0s - loss: 0.0991 - mae: 0.2393 - mse: 0.0991
72/72 [==============================] - 0s 6ms/step - loss: 0.0927 - mae: 0.2269 - mse: 0.0927 - val_loss: 0.0270 - val_mae: 0.0999 - val_mse: 0.0270
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0786 - mae: 0.2034 - mse: 0.0786
64/72 [=========================>....] - ETA: 0s - loss: 0.0697 - mae: 0.1927 - mse: 0.0697
72/72 [==============================] - 0s 6ms/step - loss: 0.0747 - mae: 0.2008 - mse: 0.0747 - val_loss: 0.0396 - val_mae: 0.1508 - val_mse: 0.0396
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0965 - mae: 0.2417 - mse: 0.0965
64/72 [=========================>....] - ETA: 0s - loss: 0.0780 - mae: 0.2180 - mse: 0.0780
72/72 [==============================] - 0s 6ms/step - loss: 0.0749 - mae: 0.2109 - mse: 0.0749 - val_loss: 0.0524 - val_mae: 0.1899 - val_mse: 0.0524
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0639 - mae: 0.1814 - mse: 0.0639
64/72 [=========================>....] - ETA: 0s - loss: 0.0712 - mae: 0.2027 - mse: 0.0712
72/72 [==============================] - 0s 6ms/step - loss: 0.0798 - mae: 0.2095 - mse: 0.0798 - val_loss: 0.0441 - val_mae: 0.1679 - val_mse: 0.0441
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0716 - mae: 0.1902 - mse: 0.0716
64/72 [=========================>....] - ETA: 0s - loss: 0.0731 - mae: 0.2041 - mse: 0.0731
72/72 [==============================] - 0s 5ms/step - loss: 0.0740 - mae: 0.2056 - mse: 0.0740 - val_loss: 0.0440 - val_mae: 0.1662 - val_mse: 0.0440
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0812 - mae: 0.1958 - mse: 0.0812
64/72 [=========================>....] - ETA: 0s - loss: 0.0696 - mae: 0.1868 - mse: 0.0696
72/72 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.1810 - mse: 0.0649 - val_loss: 0.0330 - val_mae: 0.1274 - val_mse: 0.0330
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0893 - mae: 0.1965 - mse: 0.0893
64/72 [=========================>....] - ETA: 0s - loss: 0.0771 - mae: 0.1935 - mse: 0.0771
72/72 [==============================] - 0s 6ms/step - loss: 0.0708 - mae: 0.1849 - mse: 0.0708 - val_loss: 0.0306 - val_mae: 0.1103 - val_mse: 0.0306
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0722 - mae: 0.2079 - mse: 0.0722
64/72 [=========================>....] - ETA: 0s - loss: 0.0601 - mae: 0.1848 - mse: 0.0601
72/72 [==============================] - 0s 6ms/step - loss: 0.0551 - mae: 0.1752 - mse: 0.0551 - val_loss: 0.0466 - val_mae: 0.1615 - val_mse: 0.0466
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0452 - mae: 0.1475 - mse: 0.0452
64/72 [=========================>....] - ETA: 0s - loss: 0.0630 - mae: 0.1844 - mse: 0.0630
72/72 [==============================] - 0s 6ms/step - loss: 0.0615 - mae: 0.1824 - mse: 0.0615 - val_loss: 0.0641 - val_mae: 0.2015 - val_mse: 0.0641
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0508 - mae: 0.1886 - mse: 0.0508
64/72 [=========================>....] - ETA: 0s - loss: 0.0497 - mae: 0.1750 - mse: 0.0497
72/72 [==============================] - 0s 6ms/step - loss: 0.0502 - mae: 0.1765 - mse: 0.0502 - val_loss: 0.0418 - val_mae: 0.1323 - val_mse: 0.0418
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0509 - mae: 0.1684 - mse: 0.0509
64/72 [=========================>....] - ETA: 0s - loss: 0.0488 - mae: 0.1622 - mse: 0.0488
72/72 [==============================] - 0s 5ms/step - loss: 0.0492 - mae: 0.1632 - mse: 0.0492 - val_loss: 0.0334 - val_mae: 0.1059 - val_mse: 0.0334
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0715 - mae: 0.2160 - mse: 0.0715
64/72 [=========================>....] - ETA: 0s - loss: 0.0551 - mae: 0.1788 - mse: 0.0551
72/72 [==============================] - 0s 6ms/step - loss: 0.0536 - mae: 0.1769 - mse: 0.0536 - val_loss: 0.0586 - val_mae: 0.1843 - val_mse: 0.0586
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0426 - mae: 0.1587 - mse: 0.0426
64/72 [=========================>....] - ETA: 0s - loss: 0.0521 - mae: 0.1703 - mse: 0.0521
72/72 [==============================] - 0s 6ms/step - loss: 0.0507 - mae: 0.1692 - mse: 0.0507 - val_loss: 0.0519 - val_mae: 0.1671 - val_mse: 0.0519
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0416 - mae: 0.1623 - mse: 0.0416
64/72 [=========================>....] - ETA: 0s - loss: 0.0339 - mae: 0.1460 - mse: 0.0339
72/72 [==============================] - 0s 6ms/step - loss: 0.0353 - mae: 0.1456 - mse: 0.0353 - val_loss: 0.0469 - val_mae: 0.1535 - val_mse: 0.0469
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0439 - mae: 0.1587 - mse: 0.0439
64/72 [=========================>....] - ETA: 0s - loss: 0.0486 - mae: 0.1655 - mse: 0.0486
72/72 [==============================] - 0s 6ms/step - loss: 0.0475 - mae: 0.1639 - mse: 0.0475 - val_loss: 0.0387 - val_mae: 0.1261 - val_mse: 0.0387
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0379 - mae: 0.1462 - mse: 0.0379
64/72 [=========================>....] - ETA: 0s - loss: 0.0441 - mae: 0.1591 - mse: 0.0441
72/72 [==============================] - 0s 6ms/step - loss: 0.0463 - mae: 0.1622 - mse: 0.0463 - val_loss: 0.0432 - val_mae: 0.1399 - val_mse: 0.0432
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0447 - mae: 0.1631 - mse: 0.0447
64/72 [=========================>....] - ETA: 0s - loss: 0.0383 - mae: 0.1509 - mse: 0.0383
72/72 [==============================] - 0s 7ms/step - loss: 0.0396 - mae: 0.1519 - mse: 0.0396 - val_loss: 0.0688 - val_mae: 0.2065 - val_mse: 0.0688
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0361 - mae: 0.1477 - mse: 0.0361
64/72 [=========================>....] - ETA: 0s - loss: 0.0478 - mae: 0.1681 - mse: 0.0478
72/72 [==============================] - 0s 5ms/step - loss: 0.0469 - mae: 0.1659 - mse: 0.0469 - val_loss: 0.0433 - val_mae: 0.1399 - val_mse: 0.0433
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0391 - mae: 0.1412 - mse: 0.0391
64/72 [=========================>....] - ETA: 0s - loss: 0.0436 - mae: 0.1509 - mse: 0.0436
72/72 [==============================] - 0s 5ms/step - loss: 0.0399 - mae: 0.1432 - mse: 0.0399 - val_loss: 0.0300 - val_mae: 0.1017 - val_mse: 0.0300
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0429 - mae: 0.1382 - mse: 0.0429
64/72 [=========================>....] - ETA: 0s - loss: 0.0433 - mae: 0.1467 - mse: 0.0433
72/72 [==============================] - 0s 6ms/step - loss: 0.0405 - mae: 0.1429 - mse: 0.0405 - val_loss: 0.0331 - val_mae: 0.1223 - val_mse: 0.0331
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0363 - mae: 0.1494 - mse: 0.0363
64/72 [=========================>....] - ETA: 0s - loss: 0.0356 - mae: 0.1436 - mse: 0.0356
72/72 [==============================] - 0s 5ms/step - loss: 0.0362 - mae: 0.1441 - mse: 0.0362 - val_loss: 0.0351 - val_mae: 0.1367 - val_mse: 0.0351
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0251 - mae: 0.1263 - mse: 0.0251
64/72 [=========================>....] - ETA: 0s - loss: 0.0335 - mae: 0.1406 - mse: 0.0335
72/72 [==============================] - 0s 5ms/step - loss: 0.0302 - mae: 0.1313 - mse: 0.0302 - val_loss: 0.0300 - val_mae: 0.1259 - val_mse: 0.0300
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0407 - mae: 0.1539 - mse: 0.0407
64/72 [=========================>....] - ETA: 0s - loss: 0.0389 - mae: 0.1481 - mse: 0.0389
72/72 [==============================] - 0s 5ms/step - loss: 0.0363 - mae: 0.1423 - mse: 0.0363 - val_loss: 0.0278 - val_mae: 0.1182 - val_mse: 0.0278
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0237 - mae: 0.1138 - mse: 0.0237
64/72 [=========================>....] - ETA: 0s - loss: 0.0254 - mae: 0.1242 - mse: 0.0254
72/72 [==============================] - 0s 5ms/step - loss: 0.0270 - mae: 0.1282 - mse: 0.0270 - val_loss: 0.0377 - val_mae: 0.1389 - val_mse: 0.0377
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         0.         0.         3.24360657]
average prediction= [4.509858]
baseline= 6.595238095238095
eachuser= [0. 0. 0. 0. 0. 2.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 1.6218032836914062
['train-height-16.py', '0']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
155 5
155 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
155 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
155 16
155 17
2_155_65_16_csi_a16_21.dat
155 19
155 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
155 25
2_155_65_16_csi_a16_10.dat
155 27
155 28
2_155_65_16_csi_a16_19.dat
155 30
170 31
2_170_60_16_csi_a16_3.dat
170 33
170 34
170 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
170 38
170 39
170 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_16_csi_a16_19.dat
165 90
2_165_50_16_csi_a16_3.dat
165 92
165 93
2_165_50_16_csi_a16_27.dat
165 95
165 96
165 97
165 98
165 99
165 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
175 105
1_175_70_16_csi_a16_23.dat
175 107
175 108
175 109
1_175_70_16_csi_a16_14.dat
175 111
175 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
175 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
175 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
175 123
175 124
1_175_70_16_csi_a16_5.dat
175 126
1_175_70_16_csi_a16_18.dat
175 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
180 132
1_180_85_16_csi_a16_7.dat
180 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
180 139
180 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
180 148
1_180_85_16_csi_a16_17.dat
180 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
180 155
180 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
180 161
180 162
180 163
1_180_75_16_csi_a16_18.dat
180 165
180 166
1_180_75_16_csi_a16_16.dat
180 168
1_180_75_16_csi_a16_9.dat
180 170
180 171
1_180_75_16_csi_a16_17.dat
180 173
180 174
1_180_75_16_csi_a16_13.dat
180 176
1_180_75_16_csi_a16_10.dat
180 178
180 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
180 183
1_180_75_16_csi_a16_5.dat
180 185
180 186
180 187
180 188
180 189
180 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
173 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
173 197
173 198
173 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
173 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
173 205
173 206
173 207
173 208
173 209
173 210
1_173_85_16_csi_a16_29.dat
173 212
1_173_85_16_csi_a16_17.dat
173 214
173 215
1_173_85_16_csi_a16_22.dat
173 217
1_173_85_16_csi_a16_26.dat
173 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173 173 173 173 173
 173 173 173 173 173 173 173 173 173 173 173]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.4879 - mae: 0.6171 - mse: 0.4879
64/72 [=========================>....] - ETA: 0s - loss: 0.3754 - mae: 0.5357 - mse: 0.3754
72/72 [==============================] - 1s 12ms/step - loss: 0.3624 - mae: 0.5268 - mse: 0.3624 - val_loss: 0.2158 - val_mae: 0.4365 - val_mse: 0.2158
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1241 - mae: 0.2765 - mse: 0.1241
64/72 [=========================>....] - ETA: 0s - loss: 0.1430 - mae: 0.2953 - mse: 0.1430
72/72 [==============================] - 0s 7ms/step - loss: 0.1500 - mae: 0.3070 - mse: 0.1500 - val_loss: 0.2194 - val_mae: 0.3654 - val_mse: 0.2194
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1204 - mae: 0.2574 - mse: 0.1204
64/72 [=========================>....] - ETA: 0s - loss: 0.1358 - mae: 0.2641 - mse: 0.1358
72/72 [==============================] - 0s 6ms/step - loss: 0.1404 - mae: 0.2731 - mse: 0.1404 - val_loss: 0.1898 - val_mae: 0.3955 - val_mse: 0.1898
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.0916 - mae: 0.2445 - mse: 0.0916
64/72 [=========================>....] - ETA: 0s - loss: 0.0886 - mae: 0.2295 - mse: 0.0886
72/72 [==============================] - 0s 6ms/step - loss: 0.1036 - mae: 0.2496 - mse: 0.1036 - val_loss: 0.2165 - val_mae: 0.4421 - val_mse: 0.2165
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.1265 - mae: 0.2742 - mse: 0.1265
64/72 [=========================>....] - ETA: 0s - loss: 0.1272 - mae: 0.2810 - mse: 0.1272
72/72 [==============================] - 0s 5ms/step - loss: 0.1204 - mae: 0.2728 - mse: 0.1204 - val_loss: 0.2226 - val_mae: 0.4520 - val_mse: 0.2226
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0911 - mae: 0.2346 - mse: 0.0911
64/72 [=========================>....] - ETA: 0s - loss: 0.0884 - mae: 0.2364 - mse: 0.0884
72/72 [==============================] - 0s 5ms/step - loss: 0.0906 - mae: 0.2410 - mse: 0.0906 - val_loss: 0.1901 - val_mae: 0.4171 - val_mse: 0.1901
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.1077 - mae: 0.2584 - mse: 0.1077
64/72 [=========================>....] - ETA: 0s - loss: 0.1090 - mae: 0.2479 - mse: 0.1090
72/72 [==============================] - 0s 5ms/step - loss: 0.1050 - mae: 0.2427 - mse: 0.1050 - val_loss: 0.1584 - val_mae: 0.3660 - val_mse: 0.1584
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0646 - mae: 0.1835 - mse: 0.0646
64/72 [=========================>....] - ETA: 0s - loss: 0.0894 - mae: 0.2143 - mse: 0.0894
72/72 [==============================] - 0s 6ms/step - loss: 0.0851 - mae: 0.2117 - mse: 0.0851 - val_loss: 0.1452 - val_mae: 0.3409 - val_mse: 0.1452
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.1158 - mae: 0.2175 - mse: 0.1158
64/72 [=========================>....] - ETA: 0s - loss: 0.0984 - mae: 0.2025 - mse: 0.0984
72/72 [==============================] - 0s 6ms/step - loss: 0.0896 - mae: 0.1927 - mse: 0.0896 - val_loss: 0.1394 - val_mae: 0.3428 - val_mse: 0.1394
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0885 - mae: 0.2058 - mse: 0.0885
64/72 [=========================>....] - ETA: 0s - loss: 0.0732 - mae: 0.1853 - mse: 0.0732
72/72 [==============================] - 0s 5ms/step - loss: 0.0744 - mae: 0.1881 - mse: 0.0744 - val_loss: 0.1445 - val_mae: 0.3650 - val_mse: 0.1445
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.1004 - mae: 0.2529 - mse: 0.1004
64/72 [=========================>....] - ETA: 0s - loss: 0.0793 - mae: 0.2192 - mse: 0.0793
72/72 [==============================] - 0s 5ms/step - loss: 0.0764 - mae: 0.2178 - mse: 0.0764 - val_loss: 0.1515 - val_mae: 0.3788 - val_mse: 0.1515
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0625 - mae: 0.1767 - mse: 0.0625
64/72 [=========================>....] - ETA: 0s - loss: 0.0640 - mae: 0.1974 - mse: 0.0640
72/72 [==============================] - 0s 5ms/step - loss: 0.0643 - mae: 0.1975 - mse: 0.0643 - val_loss: 0.1297 - val_mae: 0.3476 - val_mse: 0.1297
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0432 - mae: 0.1730 - mse: 0.0432
64/72 [=========================>....] - ETA: 0s - loss: 0.0517 - mae: 0.1692 - mse: 0.0517
72/72 [==============================] - 0s 6ms/step - loss: 0.0617 - mae: 0.1810 - mse: 0.0617 - val_loss: 0.1143 - val_mae: 0.3232 - val_mse: 0.1143
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0334 - mae: 0.1297 - mse: 0.0334
64/72 [=========================>....] - ETA: 0s - loss: 0.0554 - mae: 0.1718 - mse: 0.0554
72/72 [==============================] - 0s 6ms/step - loss: 0.0517 - mae: 0.1666 - mse: 0.0517 - val_loss: 0.1111 - val_mae: 0.3198 - val_mse: 0.1111
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0783 - mae: 0.2089 - mse: 0.0783
64/72 [=========================>....] - ETA: 0s - loss: 0.0715 - mae: 0.2023 - mse: 0.0715
72/72 [==============================] - 0s 5ms/step - loss: 0.0674 - mae: 0.1942 - mse: 0.0674 - val_loss: 0.0970 - val_mae: 0.2945 - val_mse: 0.0970
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0700 - mae: 0.2105 - mse: 0.0700
64/72 [=========================>....] - ETA: 0s - loss: 0.0570 - mae: 0.1835 - mse: 0.0570
72/72 [==============================] - 0s 6ms/step - loss: 0.0604 - mae: 0.1876 - mse: 0.0604 - val_loss: 0.0900 - val_mae: 0.2834 - val_mse: 0.0900
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0656 - mae: 0.1911 - mse: 0.0656
64/72 [=========================>....] - ETA: 0s - loss: 0.0553 - mae: 0.1704 - mse: 0.0553
72/72 [==============================] - 0s 5ms/step - loss: 0.0526 - mae: 0.1682 - mse: 0.0526 - val_loss: 0.0870 - val_mae: 0.2804 - val_mse: 0.0870
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0536 - mae: 0.1738 - mse: 0.0536
64/72 [=========================>....] - ETA: 0s - loss: 0.0423 - mae: 0.1570 - mse: 0.0423
72/72 [==============================] - 0s 5ms/step - loss: 0.0510 - mae: 0.1736 - mse: 0.0510 - val_loss: 0.0818 - val_mae: 0.2718 - val_mse: 0.0818
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0336 - mae: 0.1464 - mse: 0.0336
64/72 [=========================>....] - ETA: 0s - loss: 0.0446 - mae: 0.1663 - mse: 0.0446
72/72 [==============================] - 0s 6ms/step - loss: 0.0466 - mae: 0.1669 - mse: 0.0466 - val_loss: 0.0768 - val_mae: 0.2613 - val_mse: 0.0768
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0348 - mae: 0.1471 - mse: 0.0348
64/72 [=========================>....] - ETA: 0s - loss: 0.0448 - mae: 0.1559 - mse: 0.0448
72/72 [==============================] - 0s 6ms/step - loss: 0.0457 - mae: 0.1576 - mse: 0.0457 - val_loss: 0.0702 - val_mae: 0.2441 - val_mse: 0.0702
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0401 - mae: 0.1460 - mse: 0.0401
64/72 [=========================>....] - ETA: 0s - loss: 0.0363 - mae: 0.1380 - mse: 0.0363
72/72 [==============================] - 0s 5ms/step - loss: 0.0396 - mae: 0.1428 - mse: 0.0396 - val_loss: 0.0519 - val_mae: 0.2005 - val_mse: 0.0519
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0461 - mae: 0.1536 - mse: 0.0461
64/72 [=========================>....] - ETA: 0s - loss: 0.0474 - mae: 0.1577 - mse: 0.0474
72/72 [==============================] - 0s 5ms/step - loss: 0.0488 - mae: 0.1608 - mse: 0.0488 - val_loss: 0.0538 - val_mae: 0.2101 - val_mse: 0.0538
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0236 - mae: 0.1197 - mse: 0.0236
64/72 [=========================>....] - ETA: 0s - loss: 0.0336 - mae: 0.1363 - mse: 0.0336
72/72 [==============================] - 0s 5ms/step - loss: 0.0387 - mae: 0.1455 - mse: 0.0387 - val_loss: 0.0837 - val_mae: 0.2571 - val_mse: 0.0837
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0692 - mae: 0.1805 - mse: 0.0692
64/72 [=========================>....] - ETA: 0s - loss: 0.0642 - mae: 0.1869 - mse: 0.0642
72/72 [==============================] - 0s 6ms/step - loss: 0.0625 - mae: 0.1858 - mse: 0.0625 - val_loss: 0.0595 - val_mae: 0.2176 - val_mse: 0.0595
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0371 - mae: 0.1508 - mse: 0.0371
64/72 [=========================>....] - ETA: 0s - loss: 0.0427 - mae: 0.1569 - mse: 0.0427
72/72 [==============================] - 0s 5ms/step - loss: 0.0435 - mae: 0.1575 - mse: 0.0435 - val_loss: 0.0328 - val_mae: 0.1562 - val_mse: 0.0328
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0244 - mae: 0.1261 - mse: 0.0244
64/72 [=========================>....] - ETA: 0s - loss: 0.0352 - mae: 0.1427 - mse: 0.0352
72/72 [==============================] - 0s 5ms/step - loss: 0.0335 - mae: 0.1374 - mse: 0.0335 - val_loss: 0.0432 - val_mae: 0.1827 - val_mse: 0.0432
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0372 - mae: 0.1217 - mse: 0.0372
64/72 [=========================>....] - ETA: 0s - loss: 0.0467 - mae: 0.1514 - mse: 0.0467
72/72 [==============================] - 0s 5ms/step - loss: 0.0434 - mae: 0.1472 - mse: 0.0434 - val_loss: 0.0536 - val_mae: 0.2008 - val_mse: 0.0536
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0268 - mae: 0.1269 - mse: 0.0268
64/72 [=========================>....] - ETA: 0s - loss: 0.0285 - mae: 0.1247 - mse: 0.0285
72/72 [==============================] - 0s 5ms/step - loss: 0.0318 - mae: 0.1300 - mse: 0.0318 - val_loss: 0.0445 - val_mae: 0.1807 - val_mse: 0.0445
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0277 - mae: 0.1254 - mse: 0.0277
64/72 [=========================>....] - ETA: 0s - loss: 0.0357 - mae: 0.1416 - mse: 0.0357
72/72 [==============================] - 0s 5ms/step - loss: 0.0330 - mae: 0.1347 - mse: 0.0330 - val_loss: 0.0272 - val_mae: 0.1355 - val_mse: 0.0272
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0238 - mae: 0.1142 - mse: 0.0238
64/72 [=========================>....] - ETA: 0s - loss: 0.0281 - mae: 0.1250 - mse: 0.0281
72/72 [==============================] - 0s 5ms/step - loss: 0.0258 - mae: 0.1184 - mse: 0.0258 - val_loss: 0.0306 - val_mae: 0.1443 - val_mse: 0.0306
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         0.         0.         5.61665344]
average prediction= [3.2910848]
baseline= 5.595238095238095
eachuser= [0. 0. 0. 0. 0. 1.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 5.6166534423828125
['train-height-16.py', '0']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
155 5
155 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
155 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
155 16
155 17
2_155_65_16_csi_a16_21.dat
155 19
155 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
155 25
2_155_65_16_csi_a16_10.dat
155 27
155 28
2_155_65_16_csi_a16_19.dat
155 30
170 31
2_170_60_16_csi_a16_3.dat
170 33
170 34
170 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
170 38
170 39
170 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_16_csi_a16_19.dat
165 90
2_165_50_16_csi_a16_3.dat
165 92
165 93
2_165_50_16_csi_a16_27.dat
165 95
165 96
165 97
165 98
165 99
165 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
175 105
1_175_70_16_csi_a16_23.dat
175 107
175 108
175 109
1_175_70_16_csi_a16_14.dat
175 111
175 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
175 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
175 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
175 123
175 124
1_175_70_16_csi_a16_5.dat
175 126
1_175_70_16_csi_a16_18.dat
175 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
180 132
1_180_85_16_csi_a16_7.dat
180 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
180 139
180 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
180 148
1_180_85_16_csi_a16_17.dat
180 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
180 155
180 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
180 161
180 162
180 163
1_180_75_16_csi_a16_18.dat
180 165
180 166
1_180_75_16_csi_a16_16.dat
180 168
1_180_75_16_csi_a16_9.dat
180 170
180 171
1_180_75_16_csi_a16_17.dat
180 173
180 174
1_180_75_16_csi_a16_13.dat
180 176
1_180_75_16_csi_a16_10.dat
180 178
180 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
180 183
1_180_75_16_csi_a16_5.dat
180 185
180 186
180 187
180 188
180 189
180 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
173 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
173 197
173 198
173 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
173 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
173 205
173 206
173 207
173 208
173 209
173 210
1_173_85_16_csi_a16_29.dat
173 212
1_173_85_16_csi_a16_17.dat
173 214
173 215
1_173_85_16_csi_a16_22.dat
173 217
1_173_85_16_csi_a16_26.dat
173 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173 173 173 173 173
 173 173 173 173 173 173 173 173 173 173 173]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.4382 - mae: 0.5854 - mse: 0.4382
64/72 [=========================>....] - ETA: 0s - loss: 0.3377 - mae: 0.5099 - mse: 0.3377
72/72 [==============================] - 1s 12ms/step - loss: 0.3121 - mae: 0.4841 - mse: 0.3121 - val_loss: 0.0990 - val_mae: 0.2725 - val_mse: 0.0990
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1052 - mae: 0.2866 - mse: 0.1052
64/72 [=========================>....] - ETA: 0s - loss: 0.1613 - mae: 0.3284 - mse: 0.1613
72/72 [==============================] - 1s 7ms/step - loss: 0.1555 - mae: 0.3189 - mse: 0.1555 - val_loss: 0.0217 - val_mae: 0.1392 - val_mse: 0.0217
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1506 - mae: 0.2838 - mse: 0.1506
64/72 [=========================>....] - ETA: 0s - loss: 0.1560 - mae: 0.2857 - mse: 0.1560
72/72 [==============================] - 0s 6ms/step - loss: 0.1505 - mae: 0.2807 - mse: 0.1505 - val_loss: 0.1066 - val_mae: 0.2887 - val_mse: 0.1066
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.0794 - mae: 0.2216 - mse: 0.0794
64/72 [=========================>....] - ETA: 0s - loss: 0.0929 - mae: 0.2532 - mse: 0.0929
72/72 [==============================] - 0s 6ms/step - loss: 0.0875 - mae: 0.2437 - mse: 0.0875 - val_loss: 0.1825 - val_mae: 0.3941 - val_mse: 0.1825
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.1217 - mae: 0.2898 - mse: 0.1217
64/72 [=========================>....] - ETA: 0s - loss: 0.1033 - mae: 0.2634 - mse: 0.1033
72/72 [==============================] - 0s 6ms/step - loss: 0.1049 - mae: 0.2678 - mse: 0.1049 - val_loss: 0.1636 - val_mae: 0.3747 - val_mse: 0.1636
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0859 - mae: 0.2419 - mse: 0.0859
64/72 [=========================>....] - ETA: 0s - loss: 0.1036 - mae: 0.2650 - mse: 0.1036
72/72 [==============================] - 0s 5ms/step - loss: 0.1010 - mae: 0.2594 - mse: 0.1010 - val_loss: 0.0910 - val_mae: 0.2754 - val_mse: 0.0910
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0802 - mae: 0.2108 - mse: 0.0802
64/72 [=========================>....] - ETA: 0s - loss: 0.0819 - mae: 0.2148 - mse: 0.0819
72/72 [==============================] - 0s 6ms/step - loss: 0.0904 - mae: 0.2272 - mse: 0.0904 - val_loss: 0.0433 - val_mae: 0.1845 - val_mse: 0.0433
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.1255 - mae: 0.2546 - mse: 0.1255
64/72 [=========================>....] - ETA: 0s - loss: 0.0991 - mae: 0.2275 - mse: 0.0991
72/72 [==============================] - 0s 6ms/step - loss: 0.0908 - mae: 0.2166 - mse: 0.0908 - val_loss: 0.0677 - val_mae: 0.2325 - val_mse: 0.0677
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0544 - mae: 0.1779 - mse: 0.0544
64/72 [=========================>....] - ETA: 0s - loss: 0.0680 - mae: 0.1976 - mse: 0.0680
72/72 [==============================] - 0s 6ms/step - loss: 0.0692 - mae: 0.1989 - mse: 0.0692 - val_loss: 0.0904 - val_mae: 0.2737 - val_mse: 0.0904
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0750 - mae: 0.2193 - mse: 0.0750
64/72 [=========================>....] - ETA: 0s - loss: 0.0772 - mae: 0.2179 - mse: 0.0772
72/72 [==============================] - 0s 5ms/step - loss: 0.0740 - mae: 0.2105 - mse: 0.0740 - val_loss: 0.0929 - val_mae: 0.2772 - val_mse: 0.0929
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0767 - mae: 0.2151 - mse: 0.0767
64/72 [=========================>....] - ETA: 0s - loss: 0.0713 - mae: 0.2106 - mse: 0.0713
72/72 [==============================] - 0s 6ms/step - loss: 0.0745 - mae: 0.2138 - mse: 0.0745 - val_loss: 0.0854 - val_mae: 0.2633 - val_mse: 0.0854
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0363 - mae: 0.1497 - mse: 0.0363
64/72 [=========================>....] - ETA: 0s - loss: 0.0601 - mae: 0.1890 - mse: 0.0601
72/72 [==============================] - 0s 6ms/step - loss: 0.0598 - mae: 0.1918 - mse: 0.0598 - val_loss: 0.0629 - val_mae: 0.2211 - val_mse: 0.0629
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0530 - mae: 0.1807 - mse: 0.0530
64/72 [=========================>....] - ETA: 0s - loss: 0.0634 - mae: 0.1969 - mse: 0.0634
72/72 [==============================] - 0s 7ms/step - loss: 0.0598 - mae: 0.1910 - mse: 0.0598 - val_loss: 0.0365 - val_mae: 0.1749 - val_mse: 0.0365
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0506 - mae: 0.1674 - mse: 0.0506
64/72 [=========================>....] - ETA: 0s - loss: 0.0601 - mae: 0.1812 - mse: 0.0601
72/72 [==============================] - 0s 6ms/step - loss: 0.0598 - mae: 0.1769 - mse: 0.0598 - val_loss: 0.0483 - val_mae: 0.1963 - val_mse: 0.0483
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0555 - mae: 0.1789 - mse: 0.0555
64/72 [=========================>....] - ETA: 0s - loss: 0.0572 - mae: 0.1868 - mse: 0.0572
72/72 [==============================] - 0s 6ms/step - loss: 0.0581 - mae: 0.1879 - mse: 0.0581 - val_loss: 0.0780 - val_mae: 0.2469 - val_mse: 0.0780
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0644 - mae: 0.1933 - mse: 0.0644
64/72 [=========================>....] - ETA: 0s - loss: 0.0712 - mae: 0.2146 - mse: 0.0712
72/72 [==============================] - 0s 7ms/step - loss: 0.0685 - mae: 0.2078 - mse: 0.0685 - val_loss: 0.0746 - val_mae: 0.2369 - val_mse: 0.0746
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0774 - mae: 0.2133 - mse: 0.0774
64/72 [=========================>....] - ETA: 0s - loss: 0.0605 - mae: 0.1870 - mse: 0.0605
72/72 [==============================] - 0s 6ms/step - loss: 0.0554 - mae: 0.1761 - mse: 0.0554 - val_loss: 0.0446 - val_mae: 0.1782 - val_mse: 0.0446
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0729 - mae: 0.1992 - mse: 0.0729
64/72 [=========================>....] - ETA: 0s - loss: 0.0582 - mae: 0.1786 - mse: 0.0582
72/72 [==============================] - 0s 6ms/step - loss: 0.0536 - mae: 0.1703 - mse: 0.0536 - val_loss: 0.0360 - val_mae: 0.1557 - val_mse: 0.0360
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0541 - mae: 0.1748 - mse: 0.0541
64/72 [=========================>....] - ETA: 0s - loss: 0.0536 - mae: 0.1729 - mse: 0.0536
72/72 [==============================] - 0s 6ms/step - loss: 0.0497 - mae: 0.1660 - mse: 0.0497 - val_loss: 0.0308 - val_mae: 0.1408 - val_mse: 0.0308
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0558 - mae: 0.1758 - mse: 0.0558
64/72 [=========================>....] - ETA: 0s - loss: 0.0509 - mae: 0.1661 - mse: 0.0509
72/72 [==============================] - 0s 6ms/step - loss: 0.0503 - mae: 0.1657 - mse: 0.0503 - val_loss: 0.0299 - val_mae: 0.1408 - val_mse: 0.0299
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0585 - mae: 0.1736 - mse: 0.0585
64/72 [=========================>....] - ETA: 0s - loss: 0.0458 - mae: 0.1620 - mse: 0.0458
72/72 [==============================] - 0s 6ms/step - loss: 0.0437 - mae: 0.1537 - mse: 0.0437 - val_loss: 0.0483 - val_mae: 0.1800 - val_mse: 0.0483
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0578 - mae: 0.1831 - mse: 0.0578
64/72 [=========================>....] - ETA: 0s - loss: 0.0427 - mae: 0.1541 - mse: 0.0427
72/72 [==============================] - 0s 6ms/step - loss: 0.0414 - mae: 0.1532 - mse: 0.0414 - val_loss: 0.0623 - val_mae: 0.2113 - val_mse: 0.0623
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0499 - mae: 0.1849 - mse: 0.0499
64/72 [=========================>....] - ETA: 0s - loss: 0.0498 - mae: 0.1812 - mse: 0.0498
72/72 [==============================] - 0s 6ms/step - loss: 0.0504 - mae: 0.1842 - mse: 0.0504 - val_loss: 0.0325 - val_mae: 0.1555 - val_mse: 0.0325
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0422 - mae: 0.1553 - mse: 0.0422
64/72 [=========================>....] - ETA: 0s - loss: 0.0341 - mae: 0.1360 - mse: 0.0341
72/72 [==============================] - 0s 5ms/step - loss: 0.0381 - mae: 0.1442 - mse: 0.0381 - val_loss: 0.0244 - val_mae: 0.1457 - val_mse: 0.0244
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0385 - mae: 0.1520 - mse: 0.0385
64/72 [=========================>....] - ETA: 0s - loss: 0.0363 - mae: 0.1475 - mse: 0.0363
72/72 [==============================] - 1s 7ms/step - loss: 0.0362 - mae: 0.1468 - mse: 0.0362 - val_loss: 0.0440 - val_mae: 0.1761 - val_mse: 0.0440
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0470 - mae: 0.1762 - mse: 0.0470
64/72 [=========================>....] - ETA: 0s - loss: 0.0387 - mae: 0.1601 - mse: 0.0387
72/72 [==============================] - 0s 7ms/step - loss: 0.0365 - mae: 0.1560 - mse: 0.0365 - val_loss: 0.0371 - val_mae: 0.1670 - val_mse: 0.0371
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0428 - mae: 0.1549 - mse: 0.0428
64/72 [=========================>....] - ETA: 0s - loss: 0.0406 - mae: 0.1545 - mse: 0.0406
72/72 [==============================] - 0s 5ms/step - loss: 0.0370 - mae: 0.1448 - mse: 0.0370 - val_loss: 0.0335 - val_mae: 0.1599 - val_mse: 0.0335
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0365 - mae: 0.1433 - mse: 0.0365
64/72 [=========================>....] - ETA: 0s - loss: 0.0343 - mae: 0.1421 - mse: 0.0343
72/72 [==============================] - 0s 5ms/step - loss: 0.0337 - mae: 0.1415 - mse: 0.0337 - val_loss: 0.0331 - val_mae: 0.1575 - val_mse: 0.0331
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0245 - mae: 0.1204 - mse: 0.0245
64/72 [=========================>....] - ETA: 0s - loss: 0.0287 - mae: 0.1331 - mse: 0.0287
72/72 [==============================] - 0s 7ms/step - loss: 0.0358 - mae: 0.1441 - mse: 0.0358 - val_loss: 0.0475 - val_mae: 0.1879 - val_mse: 0.0475
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0277 - mae: 0.1305 - mse: 0.0277
64/72 [=========================>....] - ETA: 0s - loss: 0.0467 - mae: 0.1660 - mse: 0.0467
72/72 [==============================] - 1s 7ms/step - loss: 0.0484 - mae: 0.1735 - mse: 0.0484 - val_loss: 0.0802 - val_mae: 0.2580 - val_mse: 0.0802
Saving trained model...
119
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         11.46560669]
average prediction= [7.0770764]
baseline= 5.738095238095238
eachuser= [0. 0. 0. 0. 0. 1.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 11.465606689453125
['train-height-16.py', '0']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
155 5
155 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
155 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
155 16
155 17
2_155_65_16_csi_a16_21.dat
155 19
155 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
155 25
2_155_65_16_csi_a16_10.dat
155 27
155 28
2_155_65_16_csi_a16_19.dat
155 30
170 31
2_170_60_16_csi_a16_3.dat
170 33
170 34
170 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
170 38
170 39
170 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_16_csi_a16_19.dat
165 90
2_165_50_16_csi_a16_3.dat
165 92
165 93
2_165_50_16_csi_a16_27.dat
165 95
165 96
165 97
165 98
165 99
165 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
175 105
1_175_70_16_csi_a16_23.dat
175 107
175 108
175 109
1_175_70_16_csi_a16_14.dat
175 111
175 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
175 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
175 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
175 123
175 124
1_175_70_16_csi_a16_5.dat
175 126
1_175_70_16_csi_a16_18.dat
175 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
180 132
1_180_85_16_csi_a16_7.dat
180 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
180 139
180 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
180 148
1_180_85_16_csi_a16_17.dat
180 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
180 155
180 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
180 161
180 162
180 163
1_180_75_16_csi_a16_18.dat
180 165
180 166
1_180_75_16_csi_a16_16.dat
180 168
1_180_75_16_csi_a16_9.dat
180 170
180 171
1_180_75_16_csi_a16_17.dat
180 173
180 174
1_180_75_16_csi_a16_13.dat
180 176
1_180_75_16_csi_a16_10.dat
180 178
180 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
180 183
1_180_75_16_csi_a16_5.dat
180 185
180 186
180 187
180 188
180 189
180 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
173 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
173 197
173 198
173 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
173 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
173 205
173 206
173 207
173 208
173 209
173 210
1_173_85_16_csi_a16_29.dat
173 212
1_173_85_16_csi_a16_17.dat
173 214
173 215
1_173_85_16_csi_a16_22.dat
173 217
1_173_85_16_csi_a16_26.dat
173 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173 173 173 173 173
 173 173 173 173 173 173 173 173 173 173 173]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.5135 - mae: 0.6416 - mse: 0.5135
64/72 [=========================>....] - ETA: 0s - loss: 0.3783 - mae: 0.5334 - mse: 0.3783
72/72 [==============================] - 1s 13ms/step - loss: 0.3612 - mae: 0.5210 - mse: 0.3612 - val_loss: 0.1036 - val_mae: 0.2530 - val_mse: 0.1036
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1410 - mae: 0.3133 - mse: 0.1410
64/72 [=========================>....] - ETA: 0s - loss: 0.1463 - mae: 0.3283 - mse: 0.1463
72/72 [==============================] - 0s 6ms/step - loss: 0.1380 - mae: 0.3150 - mse: 0.1380 - val_loss: 0.0341 - val_mae: 0.1418 - val_mse: 0.0341
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1618 - mae: 0.3262 - mse: 0.1618
64/72 [=========================>....] - ETA: 0s - loss: 0.1614 - mae: 0.3175 - mse: 0.1614
72/72 [==============================] - 0s 7ms/step - loss: 0.1668 - mae: 0.3221 - mse: 0.1668 - val_loss: 0.0227 - val_mae: 0.1363 - val_mse: 0.0227
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.0510 - mae: 0.1710 - mse: 0.0510
64/72 [=========================>....] - ETA: 0s - loss: 0.1012 - mae: 0.2265 - mse: 0.1012
72/72 [==============================] - 0s 6ms/step - loss: 0.0929 - mae: 0.2162 - mse: 0.0929 - val_loss: 0.0666 - val_mae: 0.2056 - val_mse: 0.0666
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.1039 - mae: 0.2690 - mse: 0.1039
64/72 [=========================>....] - ETA: 0s - loss: 0.1035 - mae: 0.2714 - mse: 0.1035
72/72 [==============================] - 0s 6ms/step - loss: 0.0974 - mae: 0.2603 - mse: 0.0974 - val_loss: 0.1060 - val_mae: 0.2880 - val_mse: 0.1060
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.1114 - mae: 0.2869 - mse: 0.1114
64/72 [=========================>....] - ETA: 0s - loss: 0.1055 - mae: 0.2721 - mse: 0.1055
72/72 [==============================] - 0s 6ms/step - loss: 0.1063 - mae: 0.2738 - mse: 0.1063 - val_loss: 0.0914 - val_mae: 0.2696 - val_mse: 0.0914
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.1083 - mae: 0.2651 - mse: 0.1083
64/72 [=========================>....] - ETA: 0s - loss: 0.1032 - mae: 0.2589 - mse: 0.1032
72/72 [==============================] - 0s 6ms/step - loss: 0.1013 - mae: 0.2577 - mse: 0.1013 - val_loss: 0.0450 - val_mae: 0.1782 - val_mse: 0.0450
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0586 - mae: 0.1737 - mse: 0.0586
64/72 [=========================>....] - ETA: 0s - loss: 0.0751 - mae: 0.2029 - mse: 0.0751
72/72 [==============================] - 0s 6ms/step - loss: 0.0752 - mae: 0.2026 - mse: 0.0752 - val_loss: 0.0213 - val_mae: 0.1180 - val_mse: 0.0213
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0520 - mae: 0.1719 - mse: 0.0520
64/72 [=========================>....] - ETA: 0s - loss: 0.0791 - mae: 0.1986 - mse: 0.0791
72/72 [==============================] - 0s 6ms/step - loss: 0.0838 - mae: 0.1989 - mse: 0.0838 - val_loss: 0.0233 - val_mae: 0.1202 - val_mse: 0.0233
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0807 - mae: 0.2105 - mse: 0.0807
64/72 [=========================>....] - ETA: 0s - loss: 0.0624 - mae: 0.1895 - mse: 0.0624
72/72 [==============================] - 0s 6ms/step - loss: 0.0664 - mae: 0.1951 - mse: 0.0664 - val_loss: 0.0406 - val_mae: 0.1655 - val_mse: 0.0406
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0643 - mae: 0.1910 - mse: 0.0643
64/72 [=========================>....] - ETA: 0s - loss: 0.0620 - mae: 0.1893 - mse: 0.0620
72/72 [==============================] - 0s 6ms/step - loss: 0.0637 - mae: 0.1958 - mse: 0.0637 - val_loss: 0.0496 - val_mae: 0.1853 - val_mse: 0.0496
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0828 - mae: 0.2244 - mse: 0.0828
64/72 [=========================>....] - ETA: 0s - loss: 0.0646 - mae: 0.1944 - mse: 0.0646
72/72 [==============================] - 0s 6ms/step - loss: 0.0608 - mae: 0.1897 - mse: 0.0608 - val_loss: 0.0383 - val_mae: 0.1489 - val_mse: 0.0383
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0550 - mae: 0.1679 - mse: 0.0550
64/72 [=========================>....] - ETA: 0s - loss: 0.0524 - mae: 0.1714 - mse: 0.0524
72/72 [==============================] - 0s 5ms/step - loss: 0.0551 - mae: 0.1770 - mse: 0.0551 - val_loss: 0.0290 - val_mae: 0.1058 - val_mse: 0.0290
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0552 - mae: 0.1873 - mse: 0.0552
64/72 [=========================>....] - ETA: 0s - loss: 0.0465 - mae: 0.1660 - mse: 0.0465
72/72 [==============================] - 0s 6ms/step - loss: 0.0422 - mae: 0.1574 - mse: 0.0422 - val_loss: 0.0281 - val_mae: 0.0880 - val_mse: 0.0281
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0601 - mae: 0.1900 - mse: 0.0601
64/72 [=========================>....] - ETA: 0s - loss: 0.0509 - mae: 0.1681 - mse: 0.0509
72/72 [==============================] - 0s 6ms/step - loss: 0.0492 - mae: 0.1673 - mse: 0.0492 - val_loss: 0.0341 - val_mae: 0.1044 - val_mse: 0.0341
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0318 - mae: 0.1408 - mse: 0.0318
64/72 [=========================>....] - ETA: 0s - loss: 0.0373 - mae: 0.1463 - mse: 0.0373
72/72 [==============================] - 0s 6ms/step - loss: 0.0385 - mae: 0.1499 - mse: 0.0385 - val_loss: 0.0418 - val_mae: 0.1265 - val_mse: 0.0418
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0492 - mae: 0.1678 - mse: 0.0492
64/72 [=========================>....] - ETA: 0s - loss: 0.0458 - mae: 0.1595 - mse: 0.0458
72/72 [==============================] - 0s 6ms/step - loss: 0.0413 - mae: 0.1478 - mse: 0.0413 - val_loss: 0.0383 - val_mae: 0.1123 - val_mse: 0.0383
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0331 - mae: 0.1324 - mse: 0.0331
64/72 [=========================>....] - ETA: 0s - loss: 0.0347 - mae: 0.1419 - mse: 0.0347
72/72 [==============================] - 0s 5ms/step - loss: 0.0348 - mae: 0.1441 - mse: 0.0348 - val_loss: 0.0358 - val_mae: 0.1044 - val_mse: 0.0358
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0260 - mae: 0.1168 - mse: 0.0260
64/72 [=========================>....] - ETA: 0s - loss: 0.0236 - mae: 0.1112 - mse: 0.0236
72/72 [==============================] - 0s 5ms/step - loss: 0.0249 - mae: 0.1145 - mse: 0.0249 - val_loss: 0.0386 - val_mae: 0.1252 - val_mse: 0.0386
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0404 - mae: 0.1559 - mse: 0.0404
64/72 [=========================>....] - ETA: 0s - loss: 0.0322 - mae: 0.1372 - mse: 0.0322
72/72 [==============================] - 0s 5ms/step - loss: 0.0328 - mae: 0.1372 - mse: 0.0328 - val_loss: 0.0418 - val_mae: 0.1486 - val_mse: 0.0418
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0300 - mae: 0.1269 - mse: 0.0300
64/72 [=========================>....] - ETA: 0s - loss: 0.0301 - mae: 0.1326 - mse: 0.0301
72/72 [==============================] - 0s 6ms/step - loss: 0.0299 - mae: 0.1348 - mse: 0.0299 - val_loss: 0.0543 - val_mae: 0.1864 - val_mse: 0.0543
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0320 - mae: 0.1349 - mse: 0.0320
64/72 [=========================>....] - ETA: 0s - loss: 0.0393 - mae: 0.1501 - mse: 0.0393
72/72 [==============================] - 0s 6ms/step - loss: 0.0396 - mae: 0.1494 - mse: 0.0396 - val_loss: 0.0476 - val_mae: 0.1665 - val_mse: 0.0476
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0410 - mae: 0.1431 - mse: 0.0410
64/72 [=========================>....] - ETA: 0s - loss: 0.0361 - mae: 0.1360 - mse: 0.0361
72/72 [==============================] - 0s 6ms/step - loss: 0.0333 - mae: 0.1304 - mse: 0.0333 - val_loss: 0.0267 - val_mae: 0.0951 - val_mse: 0.0267
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0348 - mae: 0.1365 - mse: 0.0348
64/72 [=========================>....] - ETA: 0s - loss: 0.0315 - mae: 0.1364 - mse: 0.0315
72/72 [==============================] - 0s 5ms/step - loss: 0.0336 - mae: 0.1423 - mse: 0.0336 - val_loss: 0.0199 - val_mae: 0.0842 - val_mse: 0.0199
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0237 - mae: 0.1115 - mse: 0.0237
64/72 [=========================>....] - ETA: 0s - loss: 0.0423 - mae: 0.1506 - mse: 0.0423
72/72 [==============================] - 0s 6ms/step - loss: 0.0406 - mae: 0.1478 - mse: 0.0406 - val_loss: 0.0236 - val_mae: 0.0986 - val_mse: 0.0236
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0226 - mae: 0.1148 - mse: 0.0226
64/72 [=========================>....] - ETA: 0s - loss: 0.0262 - mae: 0.1223 - mse: 0.0262
72/72 [==============================] - 0s 5ms/step - loss: 0.0253 - mae: 0.1227 - mse: 0.0253 - val_loss: 0.0382 - val_mae: 0.1586 - val_mse: 0.0382
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0313 - mae: 0.1432 - mse: 0.0313
64/72 [=========================>....] - ETA: 0s - loss: 0.0254 - mae: 0.1260 - mse: 0.0254
72/72 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1231 - mse: 0.0241 - val_loss: 0.0383 - val_mae: 0.1630 - val_mse: 0.0383
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0274 - mae: 0.1331 - mse: 0.0274
64/72 [=========================>....] - ETA: 0s - loss: 0.0287 - mae: 0.1358 - mse: 0.0287
72/72 [==============================] - 0s 6ms/step - loss: 0.0271 - mae: 0.1319 - mse: 0.0271 - val_loss: 0.0237 - val_mae: 0.1197 - val_mse: 0.0237
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0258 - mae: 0.1289 - mse: 0.0258
64/72 [=========================>....] - ETA: 0s - loss: 0.0236 - mae: 0.1208 - mse: 0.0236
72/72 [==============================] - 0s 6ms/step - loss: 0.0230 - mae: 0.1192 - mse: 0.0230 - val_loss: 0.0175 - val_mae: 0.0935 - val_mse: 0.0175
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0245 - mae: 0.1166 - mse: 0.0245
64/72 [=========================>....] - ETA: 0s - loss: 0.0200 - mae: 0.1087 - mse: 0.0200
72/72 [==============================] - 0s 6ms/step - loss: 0.0225 - mae: 0.1147 - mse: 0.0225 - val_loss: 0.0282 - val_mae: 0.1299 - val_mse: 0.0282
Saving trained model...
119
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         11.02201843]
average prediction= [3.5626118]
baseline= 7.928571428571429
eachuser= [0. 0. 0. 0. 0. 3.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 3.674006144205729
['train-height-16.py', '0']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
155 5
155 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
155 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
155 16
155 17
2_155_65_16_csi_a16_21.dat
155 19
155 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
155 25
2_155_65_16_csi_a16_10.dat
155 27
155 28
2_155_65_16_csi_a16_19.dat
155 30
170 31
2_170_60_16_csi_a16_3.dat
170 33
170 34
170 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
170 38
170 39
170 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_16_csi_a16_19.dat
165 90
2_165_50_16_csi_a16_3.dat
165 92
165 93
2_165_50_16_csi_a16_27.dat
165 95
165 96
165 97
165 98
165 99
165 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
175 105
1_175_70_16_csi_a16_23.dat
175 107
175 108
175 109
1_175_70_16_csi_a16_14.dat
175 111
175 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
175 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
175 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
175 123
175 124
1_175_70_16_csi_a16_5.dat
175 126
1_175_70_16_csi_a16_18.dat
175 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
180 132
1_180_85_16_csi_a16_7.dat
180 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
180 139
180 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
180 148
1_180_85_16_csi_a16_17.dat
180 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
180 155
180 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
180 161
180 162
180 163
1_180_75_16_csi_a16_18.dat
180 165
180 166
1_180_75_16_csi_a16_16.dat
180 168
1_180_75_16_csi_a16_9.dat
180 170
180 171
1_180_75_16_csi_a16_17.dat
180 173
180 174
1_180_75_16_csi_a16_13.dat
180 176
1_180_75_16_csi_a16_10.dat
180 178
180 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
180 183
1_180_75_16_csi_a16_5.dat
180 185
180 186
180 187
180 188
180 189
180 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
173 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
173 197
173 198
173 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
173 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
173 205
173 206
173 207
173 208
173 209
173 210
1_173_85_16_csi_a16_29.dat
173 212
1_173_85_16_csi_a16_17.dat
173 214
173 215
1_173_85_16_csi_a16_22.dat
173 217
1_173_85_16_csi_a16_26.dat
173 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 173 173 173 173 173
 173 173 173 173 173 173 173 173 173 173 173]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.3167 - mae: 0.4670 - mse: 0.3167
64/72 [=========================>....] - ETA: 0s - loss: 0.2723 - mae: 0.4349 - mse: 0.2723
72/72 [==============================] - 1s 12ms/step - loss: 0.2543 - mae: 0.4194 - mse: 0.2543 - val_loss: 0.0544 - val_mae: 0.1991 - val_mse: 0.0544
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.2586 - mae: 0.4058 - mse: 0.2586
64/72 [=========================>....] - ETA: 0s - loss: 0.2070 - mae: 0.3476 - mse: 0.2070
72/72 [==============================] - 0s 6ms/step - loss: 0.1892 - mae: 0.3303 - mse: 0.1892 - val_loss: 0.0443 - val_mae: 0.1859 - val_mse: 0.0443
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1307 - mae: 0.2867 - mse: 0.1307
64/72 [=========================>....] - ETA: 0s - loss: 0.1233 - mae: 0.2814 - mse: 0.1233
72/72 [==============================] - 0s 6ms/step - loss: 0.1188 - mae: 0.2797 - mse: 0.1188 - val_loss: 0.0726 - val_mae: 0.2401 - val_mse: 0.0726
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.1007 - mae: 0.2361 - mse: 0.1007
64/72 [=========================>....] - ETA: 0s - loss: 0.1187 - mae: 0.2759 - mse: 0.1187
72/72 [==============================] - 0s 6ms/step - loss: 0.1176 - mae: 0.2779 - mse: 0.1176 - val_loss: 0.1007 - val_mae: 0.2730 - val_mse: 0.1007
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.0929 - mae: 0.2290 - mse: 0.0929
64/72 [=========================>....] - ETA: 0s - loss: 0.1077 - mae: 0.2649 - mse: 0.1077
72/72 [==============================] - 0s 6ms/step - loss: 0.1108 - mae: 0.2719 - mse: 0.1108 - val_loss: 0.0850 - val_mae: 0.2455 - val_mse: 0.0850
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.1016 - mae: 0.2603 - mse: 0.1016
64/72 [=========================>....] - ETA: 0s - loss: 0.1186 - mae: 0.2707 - mse: 0.1186
72/72 [==============================] - 0s 6ms/step - loss: 0.1090 - mae: 0.2564 - mse: 0.1090 - val_loss: 0.0485 - val_mae: 0.1896 - val_mse: 0.0485
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.1122 - mae: 0.2677 - mse: 0.1122
64/72 [=========================>....] - ETA: 0s - loss: 0.0915 - mae: 0.2252 - mse: 0.0915
72/72 [==============================] - 0s 5ms/step - loss: 0.0854 - mae: 0.2180 - mse: 0.0854 - val_loss: 0.0246 - val_mae: 0.1430 - val_mse: 0.0246
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0969 - mae: 0.2033 - mse: 0.0969
64/72 [=========================>....] - ETA: 0s - loss: 0.1256 - mae: 0.2239 - mse: 0.1256
72/72 [==============================] - 0s 5ms/step - loss: 0.1387 - mae: 0.2341 - mse: 0.1387 - val_loss: 0.0254 - val_mae: 0.1434 - val_mse: 0.0254
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.1163 - mae: 0.2244 - mse: 0.1163
64/72 [=========================>....] - ETA: 0s - loss: 0.1202 - mae: 0.2352 - mse: 0.1202
72/72 [==============================] - 0s 6ms/step - loss: 0.1103 - mae: 0.2227 - mse: 0.1103 - val_loss: 0.0601 - val_mae: 0.2093 - val_mse: 0.0601
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0603 - mae: 0.2023 - mse: 0.0603
64/72 [=========================>....] - ETA: 0s - loss: 0.0743 - mae: 0.2227 - mse: 0.0743
72/72 [==============================] - 0s 6ms/step - loss: 0.0828 - mae: 0.2351 - mse: 0.0828 - val_loss: 0.0783 - val_mae: 0.2530 - val_mse: 0.0783
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0861 - mae: 0.2310 - mse: 0.0861
64/72 [=========================>....] - ETA: 0s - loss: 0.0889 - mae: 0.2440 - mse: 0.0889
72/72 [==============================] - 0s 6ms/step - loss: 0.0847 - mae: 0.2358 - mse: 0.0847 - val_loss: 0.0590 - val_mae: 0.2154 - val_mse: 0.0590
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0725 - mae: 0.2246 - mse: 0.0725
64/72 [=========================>....] - ETA: 0s - loss: 0.0822 - mae: 0.2285 - mse: 0.0822
72/72 [==============================] - 0s 5ms/step - loss: 0.0794 - mae: 0.2186 - mse: 0.0794 - val_loss: 0.0217 - val_mae: 0.1348 - val_mse: 0.0217
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.1167 - mae: 0.2257 - mse: 0.1167
64/72 [=========================>....] - ETA: 0s - loss: 0.0918 - mae: 0.2066 - mse: 0.0918
72/72 [==============================] - 0s 6ms/step - loss: 0.0828 - mae: 0.1933 - mse: 0.0828 - val_loss: 0.0147 - val_mae: 0.1145 - val_mse: 0.0147
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0838 - mae: 0.2146 - mse: 0.0838
64/72 [=========================>....] - ETA: 0s - loss: 0.0643 - mae: 0.1736 - mse: 0.0643
72/72 [==============================] - 0s 6ms/step - loss: 0.0704 - mae: 0.1826 - mse: 0.0704 - val_loss: 0.0224 - val_mae: 0.1365 - val_mse: 0.0224
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0651 - mae: 0.1766 - mse: 0.0651
64/72 [=========================>....] - ETA: 0s - loss: 0.0550 - mae: 0.1756 - mse: 0.0550
72/72 [==============================] - 0s 6ms/step - loss: 0.0597 - mae: 0.1813 - mse: 0.0597 - val_loss: 0.0487 - val_mae: 0.1974 - val_mse: 0.0487
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0524 - mae: 0.1823 - mse: 0.0524
64/72 [=========================>....] - ETA: 0s - loss: 0.0627 - mae: 0.1999 - mse: 0.0627
72/72 [==============================] - 1s 7ms/step - loss: 0.0632 - mae: 0.2038 - mse: 0.0632 - val_loss: 0.0381 - val_mae: 0.1711 - val_mse: 0.0381
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0838 - mae: 0.2337 - mse: 0.0838
64/72 [=========================>....] - ETA: 0s - loss: 0.0717 - mae: 0.2096 - mse: 0.0717
72/72 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.1954 - mse: 0.0647 - val_loss: 0.0127 - val_mae: 0.1030 - val_mse: 0.0127
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0508 - mae: 0.1728 - mse: 0.0508
64/72 [=========================>....] - ETA: 0s - loss: 0.0515 - mae: 0.1623 - mse: 0.0515
72/72 [==============================] - 0s 6ms/step - loss: 0.0572 - mae: 0.1719 - mse: 0.0572 - val_loss: 0.0072 - val_mae: 0.0788 - val_mse: 0.0072
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0485 - mae: 0.1512 - mse: 0.0485
64/72 [=========================>....] - ETA: 0s - loss: 0.0526 - mae: 0.1563 - mse: 0.0526
72/72 [==============================] - 0s 6ms/step - loss: 0.0558 - mae: 0.1591 - mse: 0.0558 - val_loss: 0.0228 - val_mae: 0.1340 - val_mse: 0.0228
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0402 - mae: 0.1408 - mse: 0.0402
64/72 [=========================>....] - ETA: 0s - loss: 0.0495 - mae: 0.1587 - mse: 0.0495
72/72 [==============================] - 0s 6ms/step - loss: 0.0471 - mae: 0.1568 - mse: 0.0471 - val_loss: 0.0401 - val_mae: 0.1889 - val_mse: 0.0401
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0626 - mae: 0.1948 - mse: 0.0626
64/72 [=========================>....] - ETA: 0s - loss: 0.0549 - mae: 0.1748 - mse: 0.0549
72/72 [==============================] - 0s 6ms/step - loss: 0.0524 - mae: 0.1722 - mse: 0.0524 - val_loss: 0.0237 - val_mae: 0.1433 - val_mse: 0.0237
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0603 - mae: 0.1845 - mse: 0.0603
64/72 [=========================>....] - ETA: 0s - loss: 0.0485 - mae: 0.1587 - mse: 0.0485
72/72 [==============================] - 0s 6ms/step - loss: 0.0470 - mae: 0.1569 - mse: 0.0470 - val_loss: 0.0057 - val_mae: 0.0593 - val_mse: 0.0057
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0505 - mae: 0.1758 - mse: 0.0505
64/72 [=========================>....] - ETA: 0s - loss: 0.0428 - mae: 0.1592 - mse: 0.0428
72/72 [==============================] - 0s 6ms/step - loss: 0.0408 - mae: 0.1568 - mse: 0.0408 - val_loss: 0.0022 - val_mae: 0.0409 - val_mse: 0.0022
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0440 - mae: 0.1467 - mse: 0.0440
64/72 [=========================>....] - ETA: 0s - loss: 0.0354 - mae: 0.1344 - mse: 0.0354
72/72 [==============================] - 0s 6ms/step - loss: 0.0465 - mae: 0.1537 - mse: 0.0465 - val_loss: 0.0135 - val_mae: 0.1097 - val_mse: 0.0135
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0387 - mae: 0.1341 - mse: 0.0387
64/72 [=========================>....] - ETA: 0s - loss: 0.0478 - mae: 0.1670 - mse: 0.0478
72/72 [==============================] - 0s 6ms/step - loss: 0.0545 - mae: 0.1731 - mse: 0.0545 - val_loss: 0.0511 - val_mae: 0.2192 - val_mse: 0.0511
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0640 - mae: 0.2053 - mse: 0.0640
64/72 [=========================>....] - ETA: 0s - loss: 0.0633 - mae: 0.2027 - mse: 0.0633
72/72 [==============================] - 0s 6ms/step - loss: 0.0604 - mae: 0.1956 - mse: 0.0604 - val_loss: 0.0321 - val_mae: 0.1713 - val_mse: 0.0321
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0519 - mae: 0.1865 - mse: 0.0519
64/72 [=========================>....] - ETA: 0s - loss: 0.0487 - mae: 0.1695 - mse: 0.0487
72/72 [==============================] - 0s 5ms/step - loss: 0.0468 - mae: 0.1661 - mse: 0.0468 - val_loss: 0.0029 - val_mae: 0.0400 - val_mse: 0.0029
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0360 - mae: 0.1509 - mse: 0.0360
64/72 [=========================>....] - ETA: 0s - loss: 0.0419 - mae: 0.1642 - mse: 0.0419
72/72 [==============================] - 0s 5ms/step - loss: 0.0583 - mae: 0.1853 - mse: 0.0583 - val_loss: 0.0017 - val_mae: 0.0305 - val_mse: 0.0017
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0324 - mae: 0.1289 - mse: 0.0324
64/72 [=========================>....] - ETA: 0s - loss: 0.0537 - mae: 0.1699 - mse: 0.0537
72/72 [==============================] - 0s 6ms/step - loss: 0.0485 - mae: 0.1591 - mse: 0.0485 - val_loss: 0.0307 - val_mae: 0.1719 - val_mse: 0.0307
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0440 - mae: 0.1560 - mse: 0.0440
64/72 [=========================>....] - ETA: 0s - loss: 0.0427 - mae: 0.1585 - mse: 0.0427
72/72 [==============================] - 0s 6ms/step - loss: 0.0450 - mae: 0.1668 - mse: 0.0450 - val_loss: 0.0466 - val_mae: 0.2119 - val_mse: 0.0466
Saving trained model...
119
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         22.55517578]
average prediction= [5.883813]
baseline= 7.880952380952381
eachuser= [0. 0. 0. 0. 0. 4.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 5.6387939453125
