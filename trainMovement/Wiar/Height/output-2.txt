['train-height-2.py', '0']
2_155_65_2_csi_a2_20.dat
155 2
2_155_65_2_csi_a2_23.dat
155 4
2_155_65_2_csi_a2_16.dat
155 6
155 7
155 8
155 9
2_155_65_2_csi_a2_1.dat
2_155_65_2_csi_a2_21.dat
155 12
155 13
155 14
2_155_65_2_csi_a2_25.dat
2_155_65_2_csi_a2_18.dat
2_155_65_2_csi_a2_17.dat
2_155_65_2_csi_a2_14.dat
155 19
155 20
155 21
155 22
155 23
155 24
2_155_65_2_csi_a2_27.dat
2_155_65_2_csi_a2_24.dat
155 27
155 28
2_155_65_2_csi_a2_7.dat
155 30
170 31
170 32
170 33
2_170_60_2_csi_a2_2.dat
170 35
170 36
170 37
170 38
2_170_60_2_csi_a2_10.dat
170 40
1_165_65_2_csi_a2_30.dat
1_165_65_2_csi_a2_15.dat
1_165_65_2_csi_a2_10.dat
1_165_65_2_csi_a2_21.dat
1_165_65_2_csi_a2_9.dat
1_165_65_2_csi_a2_13.dat
1_165_65_2_csi_a2_19.dat
1_165_65_2_csi_a2_18.dat
1_165_65_2_csi_a2_7.dat
1_165_65_2_csi_a2_20.dat
1_165_65_2_csi_a2_11.dat
1_165_65_2_csi_a2_26.dat
1_165_65_2_csi_a2_16.dat
1_165_65_2_csi_a2_28.dat
1_165_65_2_csi_a2_5.dat
1_165_65_2_csi_a2_6.dat
1_165_65_2_csi_a2_25.dat
1_165_65_2_csi_a2_29.dat
1_165_65_2_csi_a2_23.dat
1_165_65_2_csi_a2_3.dat
1_165_65_2_csi_a2_27.dat
1_165_65_2_csi_a2_2.dat
1_165_65_2_csi_a2_17.dat
1_165_65_2_csi_a2_8.dat
1_165_65_2_csi_a2_14.dat
1_165_65_2_csi_a2_1.dat
1_165_65_2_csi_a2_24.dat
1_165_65_2_csi_a2_4.dat
1_165_65_2_csi_a2_22.dat
1_165_65_2_csi_a2_12.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
2_165_50_2_csi_a2_13.dat
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
2_165_50_2_csi_a2_24.dat
165 100
1_175_70_2_csi_a2_25.dat
175 102
1_175_70_2_csi_a2_20.dat
1_175_70_2_csi_a2_28.dat
175 105
1_175_70_2_csi_a2_22.dat
1_175_70_2_csi_a2_5.dat
1_175_70_2_csi_a2_11.dat
1_175_70_2_csi_a2_6.dat
1_175_70_2_csi_a2_8.dat
1_175_70_2_csi_a2_3.dat
1_175_70_2_csi_a2_16.dat
1_175_70_2_csi_a2_19.dat
1_175_70_2_csi_a2_1.dat
1_175_70_2_csi_a2_27.dat
175 116
1_175_70_2_csi_a2_18.dat
1_175_70_2_csi_a2_23.dat
1_175_70_2_csi_a2_21.dat
1_175_70_2_csi_a2_12.dat
1_175_70_2_csi_a2_30.dat
1_175_70_2_csi_a2_15.dat
1_175_70_2_csi_a2_17.dat
1_175_70_2_csi_a2_14.dat
1_175_70_2_csi_a2_9.dat
1_175_70_2_csi_a2_24.dat
1_175_70_2_csi_a2_10.dat
1_175_70_2_csi_a2_4.dat
1_175_70_2_csi_a2_29.dat
1_175_70_2_csi_a2_13.dat
1_180_85_2_csi_a2_22.dat
1_180_85_2_csi_a2_17.dat
1_180_85_2_csi_a2_15.dat
1_180_85_2_csi_a2_21.dat
180 135
1_180_85_2_csi_a2_7.dat
1_180_85_2_csi_a2_25.dat
1_180_85_2_csi_a2_19.dat
180 139
1_180_85_2_csi_a2_6.dat
1_180_85_2_csi_a2_1.dat
1_180_85_2_csi_a2_16.dat
1_180_85_2_csi_a2_14.dat
1_180_85_2_csi_a2_23.dat
1_180_85_2_csi_a2_24.dat
1_180_85_2_csi_a2_8.dat
180 147
1_180_85_2_csi_a2_13.dat
1_180_85_2_csi_a2_29.dat
1_180_85_2_csi_a2_9.dat
1_180_85_2_csi_a2_18.dat
1_180_85_2_csi_a2_28.dat
1_180_85_2_csi_a2_12.dat
1_180_85_2_csi_a2_30.dat
1_180_85_2_csi_a2_20.dat
180 156
1_180_85_2_csi_a2_26.dat
1_180_85_2_csi_a2_11.dat
180 159
1_180_85_2_csi_a2_10.dat
180 161
180 162
180 163
180 164
180 165
180 166
1_180_75_2_csi_a2_9.dat
180 168
180 169
180 170
180 171
180 172
180 173
180 174
180 175
180 176
180 177
180 178
180 179
180 180
180 181
1_180_75_2_csi_a2_1.dat
180 183
180 184
180 185
1_180_75_2_csi_a2_4.dat
180 187
180 188
180 189
180 190
173 191
173 192
173 193
173 194
173 195
173 196
173 197
173 198
173 199
1_173_85_2_csi_a2_23.dat
173 201
173 202
173 203
173 204
173 205
173 206
173 207
1_173_85_2_csi_a2_13.dat
173 209
173 210
173 211
173 212
173 213
173 214
173 215
173 216
1_173_85_2_csi_a2_28.dat
173 218
1_173_85_2_csi_a2_4.dat
173 220
(115, 30, 3)
(115, 421, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155
 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 173
 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173
 173 173 173 173 173 173 173]
(115, 421, 30, 3, 1)

Loaded dataset of 115 samples, each sized (421, 30, 3, 1)


Train on 92 samples
Test on 23 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 421, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 421, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 421, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 421, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 421, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 421, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 421, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 82 samples, validate on 10 samples
Epoch 1/30

32/82 [==========>...................] - ETA: 0s - loss: 0.4980 - mae: 0.6174 - mse: 0.4980
64/82 [======================>.......] - ETA: 0s - loss: 0.4136 - mae: 0.5628 - mse: 0.4136
82/82 [==============================] - 1s 9ms/step - loss: 0.3685 - mae: 0.5267 - mse: 0.3685 - val_loss: 0.2108 - val_mae: 0.4144 - val_mse: 0.2108
Epoch 2/30

32/82 [==========>...................] - ETA: 0s - loss: 0.2618 - mae: 0.3856 - mse: 0.2618
64/82 [======================>.......] - ETA: 0s - loss: 0.2134 - mae: 0.3559 - mse: 0.2134
82/82 [==============================] - 0s 5ms/step - loss: 0.2021 - mae: 0.3496 - mse: 0.2021 - val_loss: 0.1668 - val_mae: 0.3601 - val_mse: 0.1668
Epoch 3/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1186 - mae: 0.2791 - mse: 0.1186
64/82 [======================>.......] - ETA: 0s - loss: 0.1442 - mae: 0.3125 - mse: 0.1442
82/82 [==============================] - 0s 5ms/step - loss: 0.1458 - mae: 0.3192 - mse: 0.1458 - val_loss: 0.0983 - val_mae: 0.2654 - val_mse: 0.0983
Epoch 4/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0968 - mae: 0.2672 - mse: 0.0968
64/82 [======================>.......] - ETA: 0s - loss: 0.1156 - mae: 0.2883 - mse: 0.1156
82/82 [==============================] - 0s 5ms/step - loss: 0.1079 - mae: 0.2752 - mse: 0.1079 - val_loss: 0.0714 - val_mae: 0.2278 - val_mse: 0.0714
Epoch 5/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1023 - mae: 0.2474 - mse: 0.1023
64/82 [======================>.......] - ETA: 0s - loss: 0.0871 - mae: 0.2326 - mse: 0.0871
82/82 [==============================] - 0s 5ms/step - loss: 0.0851 - mae: 0.2332 - mse: 0.0851 - val_loss: 0.0476 - val_mae: 0.1830 - val_mse: 0.0476
Epoch 6/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0612 - mae: 0.1876 - mse: 0.0612
64/82 [======================>.......] - ETA: 0s - loss: 0.0613 - mae: 0.1922 - mse: 0.0613
82/82 [==============================] - 0s 5ms/step - loss: 0.0696 - mae: 0.2079 - mse: 0.0696 - val_loss: 0.0336 - val_mae: 0.1477 - val_mse: 0.0336
Epoch 7/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0542 - mae: 0.1753 - mse: 0.0542
64/82 [======================>.......] - ETA: 0s - loss: 0.0614 - mae: 0.1913 - mse: 0.0614
82/82 [==============================] - 0s 5ms/step - loss: 0.0619 - mae: 0.1877 - mse: 0.0619 - val_loss: 0.0271 - val_mae: 0.1370 - val_mse: 0.0271
Epoch 8/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0816 - mae: 0.2214 - mse: 0.0816
64/82 [======================>.......] - ETA: 0s - loss: 0.0605 - mae: 0.1869 - mse: 0.0605
82/82 [==============================] - 0s 5ms/step - loss: 0.0608 - mae: 0.1913 - mse: 0.0608 - val_loss: 0.0263 - val_mae: 0.1491 - val_mse: 0.0263
Epoch 9/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0441 - mae: 0.1820 - mse: 0.0441
64/82 [======================>.......] - ETA: 0s - loss: 0.0591 - mae: 0.1920 - mse: 0.0591
82/82 [==============================] - 0s 5ms/step - loss: 0.0663 - mae: 0.2036 - mse: 0.0663 - val_loss: 0.0268 - val_mae: 0.1533 - val_mse: 0.0268
Epoch 10/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0845 - mae: 0.2137 - mse: 0.0845
64/82 [======================>.......] - ETA: 0s - loss: 0.0717 - mae: 0.2062 - mse: 0.0717
82/82 [==============================] - 1s 8ms/step - loss: 0.0641 - mae: 0.1961 - mse: 0.0641 - val_loss: 0.0258 - val_mae: 0.1494 - val_mse: 0.0258
Epoch 11/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0704 - mae: 0.2017 - mse: 0.0704
64/82 [======================>.......] - ETA: 0s - loss: 0.0560 - mae: 0.1770 - mse: 0.0560
82/82 [==============================] - 1s 7ms/step - loss: 0.0555 - mae: 0.1743 - mse: 0.0555 - val_loss: 0.0247 - val_mae: 0.1429 - val_mse: 0.0247
Epoch 12/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0755 - mae: 0.2153 - mse: 0.0755
64/82 [======================>.......] - ETA: 0s - loss: 0.0729 - mae: 0.2070 - mse: 0.0729
82/82 [==============================] - 1s 8ms/step - loss: 0.0618 - mae: 0.1879 - mse: 0.0618 - val_loss: 0.0195 - val_mae: 0.1203 - val_mse: 0.0195
Epoch 13/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0627 - mae: 0.1650 - mse: 0.0627
64/82 [======================>.......] - ETA: 0s - loss: 0.0555 - mae: 0.1688 - mse: 0.0555
82/82 [==============================] - 1s 8ms/step - loss: 0.0571 - mae: 0.1739 - mse: 0.0571 - val_loss: 0.0186 - val_mae: 0.1125 - val_mse: 0.0186
Epoch 14/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0440 - mae: 0.1476 - mse: 0.0440
64/82 [======================>.......] - ETA: 0s - loss: 0.0527 - mae: 0.1692 - mse: 0.0527
82/82 [==============================] - 1s 7ms/step - loss: 0.0510 - mae: 0.1620 - mse: 0.0510 - val_loss: 0.0200 - val_mae: 0.1209 - val_mse: 0.0200
Epoch 15/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0344 - mae: 0.1235 - mse: 0.0344
64/82 [======================>.......] - ETA: 0s - loss: 0.0490 - mae: 0.1536 - mse: 0.0490
82/82 [==============================] - 1s 7ms/step - loss: 0.0520 - mae: 0.1586 - mse: 0.0520 - val_loss: 0.0198 - val_mae: 0.1220 - val_mse: 0.0198
Epoch 16/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0413 - mae: 0.1628 - mse: 0.0413
64/82 [======================>.......] - ETA: 0s - loss: 0.0568 - mae: 0.1758 - mse: 0.0568
82/82 [==============================] - 1s 7ms/step - loss: 0.0547 - mae: 0.1730 - mse: 0.0547 - val_loss: 0.0175 - val_mae: 0.1129 - val_mse: 0.0175
Epoch 17/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0331 - mae: 0.1213 - mse: 0.0331
64/82 [======================>.......] - ETA: 0s - loss: 0.0393 - mae: 0.1414 - mse: 0.0393
82/82 [==============================] - 1s 7ms/step - loss: 0.0476 - mae: 0.1531 - mse: 0.0476 - val_loss: 0.0158 - val_mae: 0.1032 - val_mse: 0.0158
Epoch 18/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0492 - mae: 0.1506 - mse: 0.0492
64/82 [======================>.......] - ETA: 0s - loss: 0.0564 - mae: 0.1730 - mse: 0.0564
82/82 [==============================] - 1s 7ms/step - loss: 0.0510 - mae: 0.1680 - mse: 0.0510 - val_loss: 0.0156 - val_mae: 0.1028 - val_mse: 0.0156
Epoch 19/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0470 - mae: 0.1612 - mse: 0.0470
64/82 [======================>.......] - ETA: 0s - loss: 0.0549 - mae: 0.1782 - mse: 0.0549
82/82 [==============================] - 1s 7ms/step - loss: 0.0541 - mae: 0.1735 - mse: 0.0541 - val_loss: 0.0164 - val_mae: 0.1119 - val_mse: 0.0164
Epoch 20/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0309 - mae: 0.1455 - mse: 0.0309
64/82 [======================>.......] - ETA: 0s - loss: 0.0548 - mae: 0.1742 - mse: 0.0548
82/82 [==============================] - 1s 7ms/step - loss: 0.0526 - mae: 0.1695 - mse: 0.0526 - val_loss: 0.0145 - val_mae: 0.1044 - val_mse: 0.0145
Epoch 21/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0273 - mae: 0.1361 - mse: 0.0273
64/82 [======================>.......] - ETA: 0s - loss: 0.0506 - mae: 0.1595 - mse: 0.0506
82/82 [==============================] - 1s 7ms/step - loss: 0.0435 - mae: 0.1465 - mse: 0.0435 - val_loss: 0.0119 - val_mae: 0.0941 - val_mse: 0.0119
Epoch 22/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0418 - mae: 0.1370 - mse: 0.0418
64/82 [======================>.......] - ETA: 0s - loss: 0.0513 - mae: 0.1567 - mse: 0.0513
82/82 [==============================] - 1s 7ms/step - loss: 0.0493 - mae: 0.1530 - mse: 0.0493 - val_loss: 0.0117 - val_mae: 0.0930 - val_mse: 0.0117
Epoch 23/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0507 - mae: 0.1616 - mse: 0.0507
64/82 [======================>.......] - ETA: 0s - loss: 0.0554 - mae: 0.1708 - mse: 0.0554
82/82 [==============================] - 1s 7ms/step - loss: 0.0458 - mae: 0.1509 - mse: 0.0458 - val_loss: 0.0148 - val_mae: 0.1086 - val_mse: 0.0148
Epoch 24/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0469 - mae: 0.1414 - mse: 0.0469
64/82 [======================>.......] - ETA: 0s - loss: 0.0515 - mae: 0.1609 - mse: 0.0515
82/82 [==============================] - 1s 7ms/step - loss: 0.0443 - mae: 0.1473 - mse: 0.0443 - val_loss: 0.0171 - val_mae: 0.1180 - val_mse: 0.0171
Epoch 25/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0426 - mae: 0.1270 - mse: 0.0426
64/82 [======================>.......] - ETA: 0s - loss: 0.0484 - mae: 0.1486 - mse: 0.0484
82/82 [==============================] - 1s 7ms/step - loss: 0.0435 - mae: 0.1443 - mse: 0.0435 - val_loss: 0.0156 - val_mae: 0.1064 - val_mse: 0.0156
Epoch 26/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0483 - mae: 0.1696 - mse: 0.0483
64/82 [======================>.......] - ETA: 0s - loss: 0.0341 - mae: 0.1375 - mse: 0.0341
82/82 [==============================] - 1s 6ms/step - loss: 0.0442 - mae: 0.1511 - mse: 0.0442 - val_loss: 0.0139 - val_mae: 0.0984 - val_mse: 0.0139
Epoch 27/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0587 - mae: 0.1879 - mse: 0.0587
64/82 [======================>.......] - ETA: 0s - loss: 0.0539 - mae: 0.1797 - mse: 0.0539
82/82 [==============================] - 1s 7ms/step - loss: 0.0457 - mae: 0.1636 - mse: 0.0457 - val_loss: 0.0136 - val_mae: 0.0912 - val_mse: 0.0136
Epoch 28/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0384 - mae: 0.1651 - mse: 0.0384
64/82 [======================>.......] - ETA: 0s - loss: 0.0507 - mae: 0.1805 - mse: 0.0507
82/82 [==============================] - 1s 7ms/step - loss: 0.0523 - mae: 0.1791 - mse: 0.0523 - val_loss: 0.0123 - val_mae: 0.0941 - val_mse: 0.0123
Epoch 29/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0460 - mae: 0.1367 - mse: 0.0460
64/82 [======================>.......] - ETA: 0s - loss: 0.0456 - mae: 0.1439 - mse: 0.0456
82/82 [==============================] - 1s 7ms/step - loss: 0.0405 - mae: 0.1388 - mse: 0.0405 - val_loss: 0.0120 - val_mae: 0.0940 - val_mse: 0.0120
Epoch 30/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0499 - mae: 0.1494 - mse: 0.0499
64/82 [======================>.......] - ETA: 0s - loss: 0.0495 - mae: 0.1487 - mse: 0.0495
82/82 [==============================] - 1s 7ms/step - loss: 0.0430 - mae: 0.1418 - mse: 0.0430 - val_loss: 0.0117 - val_mae: 0.0961 - val_mse: 0.0117
Saving trained model...
105
Testing...
heightdiff= [0.         0.         0.         0.         0.         8.95730591]
average prediction= [2.4652364]
baseline= 5.804347826086956
eachuser= [0. 0. 0. 0. 0. 2.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 4.4786529541015625
['train-height-2.py', '0']
2_155_65_2_csi_a2_20.dat
155 2
2_155_65_2_csi_a2_23.dat
155 4
2_155_65_2_csi_a2_16.dat
155 6
155 7
155 8
155 9
2_155_65_2_csi_a2_1.dat
2_155_65_2_csi_a2_21.dat
155 12
155 13
155 14
2_155_65_2_csi_a2_25.dat
2_155_65_2_csi_a2_18.dat
2_155_65_2_csi_a2_17.dat
2_155_65_2_csi_a2_14.dat
155 19
155 20
155 21
155 22
155 23
155 24
2_155_65_2_csi_a2_27.dat
2_155_65_2_csi_a2_24.dat
155 27
155 28
2_155_65_2_csi_a2_7.dat
155 30
170 31
170 32
170 33
2_170_60_2_csi_a2_2.dat
170 35
170 36
170 37
170 38
2_170_60_2_csi_a2_10.dat
170 40
1_165_65_2_csi_a2_30.dat
1_165_65_2_csi_a2_15.dat
1_165_65_2_csi_a2_10.dat
1_165_65_2_csi_a2_21.dat
1_165_65_2_csi_a2_9.dat
1_165_65_2_csi_a2_13.dat
1_165_65_2_csi_a2_19.dat
1_165_65_2_csi_a2_18.dat
1_165_65_2_csi_a2_7.dat
1_165_65_2_csi_a2_20.dat
1_165_65_2_csi_a2_11.dat
1_165_65_2_csi_a2_26.dat
1_165_65_2_csi_a2_16.dat
1_165_65_2_csi_a2_28.dat
1_165_65_2_csi_a2_5.dat
1_165_65_2_csi_a2_6.dat
1_165_65_2_csi_a2_25.dat
1_165_65_2_csi_a2_29.dat
1_165_65_2_csi_a2_23.dat
1_165_65_2_csi_a2_3.dat
1_165_65_2_csi_a2_27.dat
1_165_65_2_csi_a2_2.dat
1_165_65_2_csi_a2_17.dat
1_165_65_2_csi_a2_8.dat
1_165_65_2_csi_a2_14.dat
1_165_65_2_csi_a2_1.dat
1_165_65_2_csi_a2_24.dat
1_165_65_2_csi_a2_4.dat
1_165_65_2_csi_a2_22.dat
1_165_65_2_csi_a2_12.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
2_165_50_2_csi_a2_13.dat
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
2_165_50_2_csi_a2_24.dat
165 100
1_175_70_2_csi_a2_25.dat
175 102
1_175_70_2_csi_a2_20.dat
1_175_70_2_csi_a2_28.dat
175 105
1_175_70_2_csi_a2_22.dat
1_175_70_2_csi_a2_5.dat
1_175_70_2_csi_a2_11.dat
1_175_70_2_csi_a2_6.dat
1_175_70_2_csi_a2_8.dat
1_175_70_2_csi_a2_3.dat
1_175_70_2_csi_a2_16.dat
1_175_70_2_csi_a2_19.dat
1_175_70_2_csi_a2_1.dat
1_175_70_2_csi_a2_27.dat
175 116
1_175_70_2_csi_a2_18.dat
1_175_70_2_csi_a2_23.dat
1_175_70_2_csi_a2_21.dat
1_175_70_2_csi_a2_12.dat
1_175_70_2_csi_a2_30.dat
1_175_70_2_csi_a2_15.dat
1_175_70_2_csi_a2_17.dat
1_175_70_2_csi_a2_14.dat
1_175_70_2_csi_a2_9.dat
1_175_70_2_csi_a2_24.dat
1_175_70_2_csi_a2_10.dat
1_175_70_2_csi_a2_4.dat
1_175_70_2_csi_a2_29.dat
1_175_70_2_csi_a2_13.dat
1_180_85_2_csi_a2_22.dat
1_180_85_2_csi_a2_17.dat
1_180_85_2_csi_a2_15.dat
1_180_85_2_csi_a2_21.dat
180 135
1_180_85_2_csi_a2_7.dat
1_180_85_2_csi_a2_25.dat
1_180_85_2_csi_a2_19.dat
180 139
1_180_85_2_csi_a2_6.dat
1_180_85_2_csi_a2_1.dat
1_180_85_2_csi_a2_16.dat
1_180_85_2_csi_a2_14.dat
1_180_85_2_csi_a2_23.dat
1_180_85_2_csi_a2_24.dat
1_180_85_2_csi_a2_8.dat
180 147
1_180_85_2_csi_a2_13.dat
1_180_85_2_csi_a2_29.dat
1_180_85_2_csi_a2_9.dat
1_180_85_2_csi_a2_18.dat
1_180_85_2_csi_a2_28.dat
1_180_85_2_csi_a2_12.dat
1_180_85_2_csi_a2_30.dat
1_180_85_2_csi_a2_20.dat
180 156
1_180_85_2_csi_a2_26.dat
1_180_85_2_csi_a2_11.dat
180 159
1_180_85_2_csi_a2_10.dat
180 161
180 162
180 163
180 164
180 165
180 166
1_180_75_2_csi_a2_9.dat
180 168
180 169
180 170
180 171
180 172
180 173
180 174
180 175
180 176
180 177
180 178
180 179
180 180
180 181
1_180_75_2_csi_a2_1.dat
180 183
180 184
180 185
1_180_75_2_csi_a2_4.dat
180 187
180 188
180 189
180 190
173 191
173 192
173 193
173 194
173 195
173 196
173 197
173 198
173 199
1_173_85_2_csi_a2_23.dat
173 201
173 202
173 203
173 204
173 205
173 206
173 207
1_173_85_2_csi_a2_13.dat
173 209
173 210
173 211
173 212
173 213
173 214
173 215
173 216
1_173_85_2_csi_a2_28.dat
173 218
1_173_85_2_csi_a2_4.dat
173 220
(115, 30, 3)
(115, 421, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155
 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 173
 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173
 173 173 173 173 173 173 173]
(115, 421, 30, 3, 1)

Loaded dataset of 115 samples, each sized (421, 30, 3, 1)


Train on 92 samples
Test on 23 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 421, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 421, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 421, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 421, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 421, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 421, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 421, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 82 samples, validate on 10 samples
Epoch 1/30

32/82 [==========>...................] - ETA: 0s - loss: 0.5440 - mae: 0.6564 - mse: 0.5440
64/82 [======================>.......] - ETA: 0s - loss: 0.4407 - mae: 0.5769 - mse: 0.4407
82/82 [==============================] - 1s 11ms/step - loss: 0.3992 - mae: 0.5520 - mse: 0.3992 - val_loss: 0.0669 - val_mae: 0.1874 - val_mse: 0.0669
Epoch 2/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1125 - mae: 0.2748 - mse: 0.1125
64/82 [======================>.......] - ETA: 0s - loss: 0.1219 - mae: 0.2768 - mse: 0.1219
82/82 [==============================] - 1s 7ms/step - loss: 0.1167 - mae: 0.2667 - mse: 0.1167 - val_loss: 0.0321 - val_mae: 0.1445 - val_mse: 0.0321
Epoch 3/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1104 - mae: 0.2726 - mse: 0.1104
64/82 [======================>.......] - ETA: 0s - loss: 0.1671 - mae: 0.3145 - mse: 0.1671
82/82 [==============================] - 1s 7ms/step - loss: 0.1500 - mae: 0.2971 - mse: 0.1500 - val_loss: 0.0206 - val_mae: 0.1209 - val_mse: 0.0206
Epoch 4/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1243 - mae: 0.2759 - mse: 0.1243
64/82 [======================>.......] - ETA: 0s - loss: 0.1023 - mae: 0.2526 - mse: 0.1023
82/82 [==============================] - 1s 7ms/step - loss: 0.0964 - mae: 0.2422 - mse: 0.0964 - val_loss: 0.0607 - val_mae: 0.2020 - val_mse: 0.0607
Epoch 5/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0570 - mae: 0.1854 - mse: 0.0570
64/82 [======================>.......] - ETA: 0s - loss: 0.0846 - mae: 0.2247 - mse: 0.0846
82/82 [==============================] - 1s 6ms/step - loss: 0.0843 - mae: 0.2265 - mse: 0.0843 - val_loss: 0.0763 - val_mae: 0.2429 - val_mse: 0.0763
Epoch 6/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0841 - mae: 0.2326 - mse: 0.0841
64/82 [======================>.......] - ETA: 0s - loss: 0.0926 - mae: 0.2429 - mse: 0.0926
82/82 [==============================] - 1s 7ms/step - loss: 0.0930 - mae: 0.2439 - mse: 0.0930 - val_loss: 0.0571 - val_mae: 0.2120 - val_mse: 0.0571
Epoch 7/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0845 - mae: 0.2377 - mse: 0.0845
64/82 [======================>.......] - ETA: 0s - loss: 0.0721 - mae: 0.2163 - mse: 0.0721
82/82 [==============================] - 1s 7ms/step - loss: 0.0666 - mae: 0.2015 - mse: 0.0666 - val_loss: 0.0305 - val_mae: 0.1536 - val_mse: 0.0305
Epoch 8/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0709 - mae: 0.1939 - mse: 0.0709
64/82 [======================>.......] - ETA: 0s - loss: 0.0645 - mae: 0.1887 - mse: 0.0645
82/82 [==============================] - 1s 7ms/step - loss: 0.0631 - mae: 0.1877 - mse: 0.0631 - val_loss: 0.0177 - val_mae: 0.1176 - val_mse: 0.0177
Epoch 9/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0820 - mae: 0.2099 - mse: 0.0820
64/82 [======================>.......] - ETA: 0s - loss: 0.0836 - mae: 0.2196 - mse: 0.0836
82/82 [==============================] - 1s 7ms/step - loss: 0.0697 - mae: 0.1965 - mse: 0.0697 - val_loss: 0.0255 - val_mae: 0.1438 - val_mse: 0.0255
Epoch 10/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0607 - mae: 0.1967 - mse: 0.0607
64/82 [======================>.......] - ETA: 0s - loss: 0.0635 - mae: 0.1854 - mse: 0.0635
82/82 [==============================] - 1s 8ms/step - loss: 0.0624 - mae: 0.1823 - mse: 0.0624 - val_loss: 0.0554 - val_mae: 0.2246 - val_mse: 0.0554
Epoch 11/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0541 - mae: 0.1771 - mse: 0.0541
64/82 [======================>.......] - ETA: 0s - loss: 0.0627 - mae: 0.1815 - mse: 0.0627
82/82 [==============================] - 1s 7ms/step - loss: 0.0606 - mae: 0.1798 - mse: 0.0606 - val_loss: 0.0623 - val_mae: 0.2397 - val_mse: 0.0623
Epoch 12/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0760 - mae: 0.1962 - mse: 0.0760
64/82 [======================>.......] - ETA: 0s - loss: 0.0802 - mae: 0.2155 - mse: 0.0802
82/82 [==============================] - 1s 7ms/step - loss: 0.0720 - mae: 0.2044 - mse: 0.0720 - val_loss: 0.0392 - val_mae: 0.1909 - val_mse: 0.0392
Epoch 13/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0738 - mae: 0.2034 - mse: 0.0738
64/82 [======================>.......] - ETA: 0s - loss: 0.0634 - mae: 0.1887 - mse: 0.0634
82/82 [==============================] - 1s 7ms/step - loss: 0.0605 - mae: 0.1891 - mse: 0.0605 - val_loss: 0.0193 - val_mae: 0.1283 - val_mse: 0.0193
Epoch 14/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0522 - mae: 0.1755 - mse: 0.0522
64/82 [======================>.......] - ETA: 0s - loss: 0.0694 - mae: 0.1968 - mse: 0.0694
82/82 [==============================] - 1s 7ms/step - loss: 0.0739 - mae: 0.1990 - mse: 0.0739 - val_loss: 0.0269 - val_mae: 0.1579 - val_mse: 0.0269
Epoch 15/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0614 - mae: 0.1786 - mse: 0.0614
64/82 [======================>.......] - ETA: 0s - loss: 0.0454 - mae: 0.1584 - mse: 0.0454
82/82 [==============================] - 1s 8ms/step - loss: 0.0503 - mae: 0.1575 - mse: 0.0503 - val_loss: 0.0482 - val_mae: 0.2097 - val_mse: 0.0482
Epoch 16/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0365 - mae: 0.1550 - mse: 0.0365
64/82 [======================>.......] - ETA: 0s - loss: 0.0539 - mae: 0.1692 - mse: 0.0539
82/82 [==============================] - 1s 7ms/step - loss: 0.0634 - mae: 0.1827 - mse: 0.0634 - val_loss: 0.0489 - val_mae: 0.2092 - val_mse: 0.0489
Epoch 17/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0732 - mae: 0.1932 - mse: 0.0732
64/82 [======================>.......] - ETA: 0s - loss: 0.0623 - mae: 0.1791 - mse: 0.0623
82/82 [==============================] - 1s 7ms/step - loss: 0.0592 - mae: 0.1741 - mse: 0.0592 - val_loss: 0.0276 - val_mae: 0.1569 - val_mse: 0.0276
Epoch 18/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0576 - mae: 0.1571 - mse: 0.0576
64/82 [======================>.......] - ETA: 0s - loss: 0.0472 - mae: 0.1641 - mse: 0.0472
82/82 [==============================] - 1s 6ms/step - loss: 0.0573 - mae: 0.1763 - mse: 0.0573 - val_loss: 0.0182 - val_mae: 0.1270 - val_mse: 0.0182
Epoch 19/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0601 - mae: 0.1932 - mse: 0.0601
64/82 [======================>.......] - ETA: 0s - loss: 0.0648 - mae: 0.1945 - mse: 0.0648
82/82 [==============================] - 1s 6ms/step - loss: 0.0577 - mae: 0.1842 - mse: 0.0577 - val_loss: 0.0263 - val_mae: 0.1540 - val_mse: 0.0263
Epoch 20/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0535 - mae: 0.1709 - mse: 0.0535
64/82 [======================>.......] - ETA: 0s - loss: 0.0484 - mae: 0.1624 - mse: 0.0484
82/82 [==============================] - 1s 7ms/step - loss: 0.0494 - mae: 0.1619 - mse: 0.0494 - val_loss: 0.0357 - val_mae: 0.1795 - val_mse: 0.0357
Epoch 21/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0554 - mae: 0.1861 - mse: 0.0554
64/82 [======================>.......] - ETA: 0s - loss: 0.0530 - mae: 0.1642 - mse: 0.0530
82/82 [==============================] - 1s 7ms/step - loss: 0.0532 - mae: 0.1654 - mse: 0.0532 - val_loss: 0.0379 - val_mae: 0.1864 - val_mse: 0.0379
Epoch 22/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0610 - mae: 0.1706 - mse: 0.0610
64/82 [======================>.......] - ETA: 0s - loss: 0.0436 - mae: 0.1547 - mse: 0.0436
82/82 [==============================] - 1s 6ms/step - loss: 0.0500 - mae: 0.1584 - mse: 0.0500 - val_loss: 0.0292 - val_mae: 0.1657 - val_mse: 0.0292
Epoch 23/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0421 - mae: 0.1487 - mse: 0.0421
64/82 [======================>.......] - ETA: 0s - loss: 0.0461 - mae: 0.1484 - mse: 0.0461
82/82 [==============================] - 1s 7ms/step - loss: 0.0456 - mae: 0.1492 - mse: 0.0456 - val_loss: 0.0302 - val_mae: 0.1691 - val_mse: 0.0302
Epoch 24/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0544 - mae: 0.1829 - mse: 0.0544
64/82 [======================>.......] - ETA: 0s - loss: 0.0440 - mae: 0.1573 - mse: 0.0440
82/82 [==============================] - 0s 5ms/step - loss: 0.0487 - mae: 0.1589 - mse: 0.0487 - val_loss: 0.0442 - val_mae: 0.2020 - val_mse: 0.0442
Epoch 25/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0380 - mae: 0.1475 - mse: 0.0380
64/82 [======================>.......] - ETA: 0s - loss: 0.0566 - mae: 0.1685 - mse: 0.0566
82/82 [==============================] - 0s 5ms/step - loss: 0.0525 - mae: 0.1661 - mse: 0.0525 - val_loss: 0.0473 - val_mae: 0.2080 - val_mse: 0.0473
Epoch 26/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0542 - mae: 0.1695 - mse: 0.0542
64/82 [======================>.......] - ETA: 0s - loss: 0.0460 - mae: 0.1588 - mse: 0.0460
82/82 [==============================] - 0s 5ms/step - loss: 0.0470 - mae: 0.1576 - mse: 0.0470 - val_loss: 0.0324 - val_mae: 0.1744 - val_mse: 0.0324
Epoch 27/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0597 - mae: 0.1687 - mse: 0.0597
64/82 [======================>.......] - ETA: 0s - loss: 0.0468 - mae: 0.1487 - mse: 0.0468
82/82 [==============================] - 0s 5ms/step - loss: 0.0429 - mae: 0.1436 - mse: 0.0429 - val_loss: 0.0285 - val_mae: 0.1636 - val_mse: 0.0285
Epoch 28/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0480 - mae: 0.1387 - mse: 0.0480
64/82 [======================>.......] - ETA: 0s - loss: 0.0492 - mae: 0.1426 - mse: 0.0492
82/82 [==============================] - 0s 5ms/step - loss: 0.0479 - mae: 0.1432 - mse: 0.0479 - val_loss: 0.0323 - val_mae: 0.1735 - val_mse: 0.0323
Epoch 29/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0352 - mae: 0.1381 - mse: 0.0352
64/82 [======================>.......] - ETA: 0s - loss: 0.0394 - mae: 0.1438 - mse: 0.0394
82/82 [==============================] - 0s 5ms/step - loss: 0.0461 - mae: 0.1493 - mse: 0.0461 - val_loss: 0.0416 - val_mae: 0.1962 - val_mse: 0.0416
Epoch 30/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0564 - mae: 0.1499 - mse: 0.0564
64/82 [======================>.......] - ETA: 0s - loss: 0.0471 - mae: 0.1443 - mse: 0.0471
82/82 [==============================] - 0s 5ms/step - loss: 0.0437 - mae: 0.1468 - mse: 0.0437 - val_loss: 0.0372 - val_mae: 0.1864 - val_mse: 0.0372
Saving trained model...
105
Testing...
heightdiff= [ 0.         0.         0.         0.         0.        27.3790741]
average prediction= [4.8529634]
baseline= 8.152173913043478
eachuser= [0. 0. 0. 0. 0. 6.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 4.563179016113281
['train-height-2.py', '0']
2_155_65_2_csi_a2_20.dat
155 2
2_155_65_2_csi_a2_23.dat
155 4
2_155_65_2_csi_a2_16.dat
155 6
155 7
155 8
155 9
2_155_65_2_csi_a2_1.dat
2_155_65_2_csi_a2_21.dat
155 12
155 13
155 14
2_155_65_2_csi_a2_25.dat
2_155_65_2_csi_a2_18.dat
2_155_65_2_csi_a2_17.dat
2_155_65_2_csi_a2_14.dat
155 19
155 20
155 21
155 22
155 23
155 24
2_155_65_2_csi_a2_27.dat
2_155_65_2_csi_a2_24.dat
155 27
155 28
2_155_65_2_csi_a2_7.dat
155 30
170 31
170 32
170 33
2_170_60_2_csi_a2_2.dat
170 35
170 36
170 37
170 38
2_170_60_2_csi_a2_10.dat
170 40
1_165_65_2_csi_a2_30.dat
1_165_65_2_csi_a2_15.dat
1_165_65_2_csi_a2_10.dat
1_165_65_2_csi_a2_21.dat
1_165_65_2_csi_a2_9.dat
1_165_65_2_csi_a2_13.dat
1_165_65_2_csi_a2_19.dat
1_165_65_2_csi_a2_18.dat
1_165_65_2_csi_a2_7.dat
1_165_65_2_csi_a2_20.dat
1_165_65_2_csi_a2_11.dat
1_165_65_2_csi_a2_26.dat
1_165_65_2_csi_a2_16.dat
1_165_65_2_csi_a2_28.dat
1_165_65_2_csi_a2_5.dat
1_165_65_2_csi_a2_6.dat
1_165_65_2_csi_a2_25.dat
1_165_65_2_csi_a2_29.dat
1_165_65_2_csi_a2_23.dat
1_165_65_2_csi_a2_3.dat
1_165_65_2_csi_a2_27.dat
1_165_65_2_csi_a2_2.dat
1_165_65_2_csi_a2_17.dat
1_165_65_2_csi_a2_8.dat
1_165_65_2_csi_a2_14.dat
1_165_65_2_csi_a2_1.dat
1_165_65_2_csi_a2_24.dat
1_165_65_2_csi_a2_4.dat
1_165_65_2_csi_a2_22.dat
1_165_65_2_csi_a2_12.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
2_165_50_2_csi_a2_13.dat
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
2_165_50_2_csi_a2_24.dat
165 100
1_175_70_2_csi_a2_25.dat
175 102
1_175_70_2_csi_a2_20.dat
1_175_70_2_csi_a2_28.dat
175 105
1_175_70_2_csi_a2_22.dat
1_175_70_2_csi_a2_5.dat
1_175_70_2_csi_a2_11.dat
1_175_70_2_csi_a2_6.dat
1_175_70_2_csi_a2_8.dat
1_175_70_2_csi_a2_3.dat
1_175_70_2_csi_a2_16.dat
1_175_70_2_csi_a2_19.dat
1_175_70_2_csi_a2_1.dat
1_175_70_2_csi_a2_27.dat
175 116
1_175_70_2_csi_a2_18.dat
1_175_70_2_csi_a2_23.dat
1_175_70_2_csi_a2_21.dat
1_175_70_2_csi_a2_12.dat
1_175_70_2_csi_a2_30.dat
1_175_70_2_csi_a2_15.dat
1_175_70_2_csi_a2_17.dat
1_175_70_2_csi_a2_14.dat
1_175_70_2_csi_a2_9.dat
1_175_70_2_csi_a2_24.dat
1_175_70_2_csi_a2_10.dat
1_175_70_2_csi_a2_4.dat
1_175_70_2_csi_a2_29.dat
1_175_70_2_csi_a2_13.dat
1_180_85_2_csi_a2_22.dat
1_180_85_2_csi_a2_17.dat
1_180_85_2_csi_a2_15.dat
1_180_85_2_csi_a2_21.dat
180 135
1_180_85_2_csi_a2_7.dat
1_180_85_2_csi_a2_25.dat
1_180_85_2_csi_a2_19.dat
180 139
1_180_85_2_csi_a2_6.dat
1_180_85_2_csi_a2_1.dat
1_180_85_2_csi_a2_16.dat
1_180_85_2_csi_a2_14.dat
1_180_85_2_csi_a2_23.dat
1_180_85_2_csi_a2_24.dat
1_180_85_2_csi_a2_8.dat
180 147
1_180_85_2_csi_a2_13.dat
1_180_85_2_csi_a2_29.dat
1_180_85_2_csi_a2_9.dat
1_180_85_2_csi_a2_18.dat
1_180_85_2_csi_a2_28.dat
1_180_85_2_csi_a2_12.dat
1_180_85_2_csi_a2_30.dat
1_180_85_2_csi_a2_20.dat
180 156
1_180_85_2_csi_a2_26.dat
1_180_85_2_csi_a2_11.dat
180 159
1_180_85_2_csi_a2_10.dat
180 161
180 162
180 163
180 164
180 165
180 166
1_180_75_2_csi_a2_9.dat
180 168
180 169
180 170
180 171
180 172
180 173
180 174
180 175
180 176
180 177
180 178
180 179
180 180
180 181
1_180_75_2_csi_a2_1.dat
180 183
180 184
180 185
1_180_75_2_csi_a2_4.dat
180 187
180 188
180 189
180 190
173 191
173 192
173 193
173 194
173 195
173 196
173 197
173 198
173 199
1_173_85_2_csi_a2_23.dat
173 201
173 202
173 203
173 204
173 205
173 206
173 207
1_173_85_2_csi_a2_13.dat
173 209
173 210
173 211
173 212
173 213
173 214
173 215
173 216
1_173_85_2_csi_a2_28.dat
173 218
1_173_85_2_csi_a2_4.dat
173 220
(115, 30, 3)
(115, 421, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155
 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 173
 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173
 173 173 173 173 173 173 173]
(115, 421, 30, 3, 1)

Loaded dataset of 115 samples, each sized (421, 30, 3, 1)


Train on 92 samples
Test on 23 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 421, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 421, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 421, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 421, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 421, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 421, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 421, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 82 samples, validate on 10 samples
Epoch 1/30

32/82 [==========>...................] - ETA: 0s - loss: 0.3511 - mae: 0.5158 - mse: 0.3511
64/82 [======================>.......] - ETA: 0s - loss: 0.2849 - mae: 0.4586 - mse: 0.2849
82/82 [==============================] - 1s 11ms/step - loss: 0.2515 - mae: 0.4190 - mse: 0.2515 - val_loss: 0.1043 - val_mae: 0.2842 - val_mse: 0.1043
Epoch 2/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1480 - mae: 0.2799 - mse: 0.1480
64/82 [======================>.......] - ETA: 0s - loss: 0.1874 - mae: 0.3278 - mse: 0.1874
82/82 [==============================] - 1s 7ms/step - loss: 0.2061 - mae: 0.3500 - mse: 0.2061 - val_loss: 0.0735 - val_mae: 0.2331 - val_mse: 0.0735
Epoch 3/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0986 - mae: 0.2387 - mse: 0.0986
64/82 [======================>.......] - ETA: 0s - loss: 0.1059 - mae: 0.2588 - mse: 0.1059
82/82 [==============================] - 1s 8ms/step - loss: 0.1022 - mae: 0.2565 - mse: 0.1022 - val_loss: 0.1089 - val_mae: 0.2454 - val_mse: 0.1089
Epoch 4/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0908 - mae: 0.2430 - mse: 0.0908
64/82 [======================>.......] - ETA: 0s - loss: 0.1120 - mae: 0.2761 - mse: 0.1120
82/82 [==============================] - 1s 7ms/step - loss: 0.1245 - mae: 0.2901 - mse: 0.1245 - val_loss: 0.1159 - val_mae: 0.2668 - val_mse: 0.1159
Epoch 5/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1376 - mae: 0.3087 - mse: 0.1376
64/82 [======================>.......] - ETA: 0s - loss: 0.1111 - mae: 0.2722 - mse: 0.1111
82/82 [==============================] - 1s 7ms/step - loss: 0.1075 - mae: 0.2661 - mse: 0.1075 - val_loss: 0.0714 - val_mae: 0.1643 - val_mse: 0.0714
Epoch 6/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0693 - mae: 0.2050 - mse: 0.0693
64/82 [======================>.......] - ETA: 0s - loss: 0.0643 - mae: 0.1955 - mse: 0.0643
82/82 [==============================] - 1s 7ms/step - loss: 0.0627 - mae: 0.1959 - mse: 0.0627 - val_loss: 0.0628 - val_mae: 0.1730 - val_mse: 0.0628
Epoch 7/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0762 - mae: 0.2319 - mse: 0.0762
64/82 [======================>.......] - ETA: 0s - loss: 0.0728 - mae: 0.2158 - mse: 0.0728
82/82 [==============================] - 1s 7ms/step - loss: 0.0691 - mae: 0.2109 - mse: 0.0691 - val_loss: 0.0644 - val_mae: 0.1731 - val_mse: 0.0644
Epoch 8/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0808 - mae: 0.2182 - mse: 0.0808
64/82 [======================>.......] - ETA: 0s - loss: 0.0709 - mae: 0.2058 - mse: 0.0709
82/82 [==============================] - 1s 7ms/step - loss: 0.0687 - mae: 0.2010 - mse: 0.0687 - val_loss: 0.0681 - val_mae: 0.1306 - val_mse: 0.0681
Epoch 9/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0548 - mae: 0.1694 - mse: 0.0548
64/82 [======================>.......] - ETA: 0s - loss: 0.0486 - mae: 0.1686 - mse: 0.0486
82/82 [==============================] - 0s 6ms/step - loss: 0.0482 - mae: 0.1722 - mse: 0.0482 - val_loss: 0.0889 - val_mae: 0.2088 - val_mse: 0.0889
Epoch 10/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0558 - mae: 0.1792 - mse: 0.0558
64/82 [======================>.......] - ETA: 0s - loss: 0.0542 - mae: 0.1883 - mse: 0.0542
82/82 [==============================] - 0s 5ms/step - loss: 0.0637 - mae: 0.1988 - mse: 0.0637 - val_loss: 0.0922 - val_mae: 0.2148 - val_mse: 0.0922
Epoch 11/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0320 - mae: 0.1553 - mse: 0.0320
64/82 [======================>.......] - ETA: 0s - loss: 0.0498 - mae: 0.1724 - mse: 0.0498
82/82 [==============================] - 0s 6ms/step - loss: 0.0559 - mae: 0.1782 - mse: 0.0559 - val_loss: 0.0807 - val_mae: 0.1700 - val_mse: 0.0807
Epoch 12/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0318 - mae: 0.1454 - mse: 0.0318
64/82 [======================>.......] - ETA: 0s - loss: 0.0634 - mae: 0.1917 - mse: 0.0634
82/82 [==============================] - 0s 5ms/step - loss: 0.0587 - mae: 0.1865 - mse: 0.0587 - val_loss: 0.0771 - val_mae: 0.1728 - val_mse: 0.0771
Epoch 13/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0676 - mae: 0.1834 - mse: 0.0676
64/82 [======================>.......] - ETA: 0s - loss: 0.0610 - mae: 0.1727 - mse: 0.0610
82/82 [==============================] - 0s 5ms/step - loss: 0.0505 - mae: 0.1561 - mse: 0.0505 - val_loss: 0.0775 - val_mae: 0.1665 - val_mse: 0.0775
Epoch 14/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0645 - mae: 0.1706 - mse: 0.0645
64/82 [======================>.......] - ETA: 0s - loss: 0.0474 - mae: 0.1517 - mse: 0.0474
82/82 [==============================] - 0s 5ms/step - loss: 0.0506 - mae: 0.1656 - mse: 0.0506 - val_loss: 0.0786 - val_mae: 0.1869 - val_mse: 0.0786
Epoch 15/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0297 - mae: 0.1459 - mse: 0.0297
64/82 [======================>.......] - ETA: 0s - loss: 0.0494 - mae: 0.1735 - mse: 0.0494
82/82 [==============================] - 0s 4ms/step - loss: 0.0538 - mae: 0.1766 - mse: 0.0538 - val_loss: 0.0823 - val_mae: 0.1994 - val_mse: 0.0823
Epoch 16/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0494 - mae: 0.1654 - mse: 0.0494
64/82 [======================>.......] - ETA: 0s - loss: 0.0518 - mae: 0.1624 - mse: 0.0518
82/82 [==============================] - 0s 4ms/step - loss: 0.0443 - mae: 0.1508 - mse: 0.0443 - val_loss: 0.0782 - val_mae: 0.1856 - val_mse: 0.0782
Epoch 17/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0474 - mae: 0.1664 - mse: 0.0474
64/82 [======================>.......] - ETA: 0s - loss: 0.0433 - mae: 0.1534 - mse: 0.0433
82/82 [==============================] - 0s 5ms/step - loss: 0.0482 - mae: 0.1596 - mse: 0.0482 - val_loss: 0.0709 - val_mae: 0.1539 - val_mse: 0.0709
Epoch 18/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0509 - mae: 0.1646 - mse: 0.0509
64/82 [======================>.......] - ETA: 0s - loss: 0.0436 - mae: 0.1522 - mse: 0.0436
82/82 [==============================] - 0s 5ms/step - loss: 0.0451 - mae: 0.1538 - mse: 0.0451 - val_loss: 0.0683 - val_mae: 0.1288 - val_mse: 0.0683
Epoch 19/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0585 - mae: 0.1757 - mse: 0.0585
64/82 [======================>.......] - ETA: 0s - loss: 0.0515 - mae: 0.1688 - mse: 0.0515
82/82 [==============================] - 0s 5ms/step - loss: 0.0450 - mae: 0.1589 - mse: 0.0450 - val_loss: 0.0724 - val_mae: 0.1387 - val_mse: 0.0724
Epoch 20/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0254 - mae: 0.1269 - mse: 0.0254
64/82 [======================>.......] - ETA: 0s - loss: 0.0425 - mae: 0.1403 - mse: 0.0425
82/82 [==============================] - 0s 5ms/step - loss: 0.0472 - mae: 0.1497 - mse: 0.0472 - val_loss: 0.0840 - val_mae: 0.1786 - val_mse: 0.0840
Epoch 21/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0369 - mae: 0.1213 - mse: 0.0369
64/82 [======================>.......] - ETA: 0s - loss: 0.0377 - mae: 0.1294 - mse: 0.0377
82/82 [==============================] - 0s 5ms/step - loss: 0.0405 - mae: 0.1353 - mse: 0.0405 - val_loss: 0.0894 - val_mae: 0.2019 - val_mse: 0.0894
Epoch 22/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0401 - mae: 0.1387 - mse: 0.0401
64/82 [======================>.......] - ETA: 0s - loss: 0.0512 - mae: 0.1520 - mse: 0.0512
82/82 [==============================] - 0s 5ms/step - loss: 0.0459 - mae: 0.1471 - mse: 0.0459 - val_loss: 0.0858 - val_mae: 0.1894 - val_mse: 0.0858
Epoch 23/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0397 - mae: 0.1303 - mse: 0.0397
64/82 [======================>.......] - ETA: 0s - loss: 0.0326 - mae: 0.1310 - mse: 0.0326
82/82 [==============================] - 0s 5ms/step - loss: 0.0456 - mae: 0.1461 - mse: 0.0456 - val_loss: 0.0823 - val_mae: 0.1734 - val_mse: 0.0823
Epoch 24/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0554 - mae: 0.1608 - mse: 0.0554
64/82 [======================>.......] - ETA: 0s - loss: 0.0420 - mae: 0.1467 - mse: 0.0420
82/82 [==============================] - 0s 5ms/step - loss: 0.0449 - mae: 0.1504 - mse: 0.0449 - val_loss: 0.0807 - val_mae: 0.1888 - val_mse: 0.0807
Epoch 25/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0308 - mae: 0.1256 - mse: 0.0308
64/82 [======================>.......] - ETA: 0s - loss: 0.0433 - mae: 0.1413 - mse: 0.0433
82/82 [==============================] - 0s 5ms/step - loss: 0.0397 - mae: 0.1379 - mse: 0.0397 - val_loss: 0.0812 - val_mae: 0.2001 - val_mse: 0.0812
Epoch 26/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0489 - mae: 0.1372 - mse: 0.0489
64/82 [======================>.......] - ETA: 0s - loss: 0.0422 - mae: 0.1345 - mse: 0.0422
82/82 [==============================] - 0s 4ms/step - loss: 0.0406 - mae: 0.1387 - mse: 0.0406 - val_loss: 0.0770 - val_mae: 0.1901 - val_mse: 0.0770
Epoch 27/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0333 - mae: 0.1300 - mse: 0.0333
64/82 [======================>.......] - ETA: 0s - loss: 0.0407 - mae: 0.1390 - mse: 0.0407
82/82 [==============================] - 0s 5ms/step - loss: 0.0351 - mae: 0.1280 - mse: 0.0351 - val_loss: 0.0769 - val_mae: 0.1827 - val_mse: 0.0769
Epoch 28/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0702 - mae: 0.1907 - mse: 0.0702
64/82 [======================>.......] - ETA: 0s - loss: 0.0467 - mae: 0.1580 - mse: 0.0467
82/82 [==============================] - 0s 5ms/step - loss: 0.0404 - mae: 0.1455 - mse: 0.0404 - val_loss: 0.0812 - val_mae: 0.1989 - val_mse: 0.0812
Epoch 29/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0282 - mae: 0.1110 - mse: 0.0282
64/82 [======================>.......] - ETA: 0s - loss: 0.0324 - mae: 0.1178 - mse: 0.0324
82/82 [==============================] - 0s 5ms/step - loss: 0.0331 - mae: 0.1159 - mse: 0.0331 - val_loss: 0.0812 - val_mae: 0.1968 - val_mse: 0.0812
Epoch 30/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0173 - mae: 0.1013 - mse: 0.0173
64/82 [======================>.......] - ETA: 0s - loss: 0.0452 - mae: 0.1390 - mse: 0.0452
82/82 [==============================] - 0s 5ms/step - loss: 0.0380 - mae: 0.1292 - mse: 0.0380 - val_loss: 0.0754 - val_mae: 0.1771 - val_mse: 0.0754
Saving trained model...
105
Testing...
heightdiff= [ 0.         0.         0.         0.         0.        18.7925415]
average prediction= [3.1382267]
baseline= 7.1521739130434785
eachuser= [0. 0. 0. 0. 0. 5.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 3.75850830078125
['train-height-2.py', '0']
2_155_65_2_csi_a2_20.dat
155 2
2_155_65_2_csi_a2_23.dat
155 4
2_155_65_2_csi_a2_16.dat
155 6
155 7
155 8
155 9
2_155_65_2_csi_a2_1.dat
2_155_65_2_csi_a2_21.dat
155 12
155 13
155 14
2_155_65_2_csi_a2_25.dat
2_155_65_2_csi_a2_18.dat
2_155_65_2_csi_a2_17.dat
2_155_65_2_csi_a2_14.dat
155 19
155 20
155 21
155 22
155 23
155 24
2_155_65_2_csi_a2_27.dat
2_155_65_2_csi_a2_24.dat
155 27
155 28
2_155_65_2_csi_a2_7.dat
155 30
170 31
170 32
170 33
2_170_60_2_csi_a2_2.dat
170 35
170 36
170 37
170 38
2_170_60_2_csi_a2_10.dat
170 40
1_165_65_2_csi_a2_30.dat
1_165_65_2_csi_a2_15.dat
1_165_65_2_csi_a2_10.dat
1_165_65_2_csi_a2_21.dat
1_165_65_2_csi_a2_9.dat
1_165_65_2_csi_a2_13.dat
1_165_65_2_csi_a2_19.dat
1_165_65_2_csi_a2_18.dat
1_165_65_2_csi_a2_7.dat
1_165_65_2_csi_a2_20.dat
1_165_65_2_csi_a2_11.dat
1_165_65_2_csi_a2_26.dat
1_165_65_2_csi_a2_16.dat
1_165_65_2_csi_a2_28.dat
1_165_65_2_csi_a2_5.dat
1_165_65_2_csi_a2_6.dat
1_165_65_2_csi_a2_25.dat
1_165_65_2_csi_a2_29.dat
1_165_65_2_csi_a2_23.dat
1_165_65_2_csi_a2_3.dat
1_165_65_2_csi_a2_27.dat
1_165_65_2_csi_a2_2.dat
1_165_65_2_csi_a2_17.dat
1_165_65_2_csi_a2_8.dat
1_165_65_2_csi_a2_14.dat
1_165_65_2_csi_a2_1.dat
1_165_65_2_csi_a2_24.dat
1_165_65_2_csi_a2_4.dat
1_165_65_2_csi_a2_22.dat
1_165_65_2_csi_a2_12.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
2_165_50_2_csi_a2_13.dat
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
2_165_50_2_csi_a2_24.dat
165 100
1_175_70_2_csi_a2_25.dat
175 102
1_175_70_2_csi_a2_20.dat
1_175_70_2_csi_a2_28.dat
175 105
1_175_70_2_csi_a2_22.dat
1_175_70_2_csi_a2_5.dat
1_175_70_2_csi_a2_11.dat
1_175_70_2_csi_a2_6.dat
1_175_70_2_csi_a2_8.dat
1_175_70_2_csi_a2_3.dat
1_175_70_2_csi_a2_16.dat
1_175_70_2_csi_a2_19.dat
1_175_70_2_csi_a2_1.dat
1_175_70_2_csi_a2_27.dat
175 116
1_175_70_2_csi_a2_18.dat
1_175_70_2_csi_a2_23.dat
1_175_70_2_csi_a2_21.dat
1_175_70_2_csi_a2_12.dat
1_175_70_2_csi_a2_30.dat
1_175_70_2_csi_a2_15.dat
1_175_70_2_csi_a2_17.dat
1_175_70_2_csi_a2_14.dat
1_175_70_2_csi_a2_9.dat
1_175_70_2_csi_a2_24.dat
1_175_70_2_csi_a2_10.dat
1_175_70_2_csi_a2_4.dat
1_175_70_2_csi_a2_29.dat
1_175_70_2_csi_a2_13.dat
1_180_85_2_csi_a2_22.dat
1_180_85_2_csi_a2_17.dat
1_180_85_2_csi_a2_15.dat
1_180_85_2_csi_a2_21.dat
180 135
1_180_85_2_csi_a2_7.dat
1_180_85_2_csi_a2_25.dat
1_180_85_2_csi_a2_19.dat
180 139
1_180_85_2_csi_a2_6.dat
1_180_85_2_csi_a2_1.dat
1_180_85_2_csi_a2_16.dat
1_180_85_2_csi_a2_14.dat
1_180_85_2_csi_a2_23.dat
1_180_85_2_csi_a2_24.dat
1_180_85_2_csi_a2_8.dat
180 147
1_180_85_2_csi_a2_13.dat
1_180_85_2_csi_a2_29.dat
1_180_85_2_csi_a2_9.dat
1_180_85_2_csi_a2_18.dat
1_180_85_2_csi_a2_28.dat
1_180_85_2_csi_a2_12.dat
1_180_85_2_csi_a2_30.dat
1_180_85_2_csi_a2_20.dat
180 156
1_180_85_2_csi_a2_26.dat
1_180_85_2_csi_a2_11.dat
180 159
1_180_85_2_csi_a2_10.dat
180 161
180 162
180 163
180 164
180 165
180 166
1_180_75_2_csi_a2_9.dat
180 168
180 169
180 170
180 171
180 172
180 173
180 174
180 175
180 176
180 177
180 178
180 179
180 180
180 181
1_180_75_2_csi_a2_1.dat
180 183
180 184
180 185
1_180_75_2_csi_a2_4.dat
180 187
180 188
180 189
180 190
173 191
173 192
173 193
173 194
173 195
173 196
173 197
173 198
173 199
1_173_85_2_csi_a2_23.dat
173 201
173 202
173 203
173 204
173 205
173 206
173 207
1_173_85_2_csi_a2_13.dat
173 209
173 210
173 211
173 212
173 213
173 214
173 215
173 216
1_173_85_2_csi_a2_28.dat
173 218
1_173_85_2_csi_a2_4.dat
173 220
(115, 30, 3)
(115, 421, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155
 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 173
 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173
 173 173 173 173 173 173 173]
(115, 421, 30, 3, 1)

Loaded dataset of 115 samples, each sized (421, 30, 3, 1)


Train on 92 samples
Test on 23 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 421, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 421, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 421, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 421, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 421, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 421, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 421, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 82 samples, validate on 10 samples
Epoch 1/30

32/82 [==========>...................] - ETA: 0s - loss: 0.3691 - mae: 0.5271 - mse: 0.3691
64/82 [======================>.......] - ETA: 0s - loss: 0.3419 - mae: 0.5150 - mse: 0.3419
82/82 [==============================] - 1s 9ms/step - loss: 0.3166 - mae: 0.4893 - mse: 0.3166 - val_loss: 0.0864 - val_mae: 0.2426 - val_mse: 0.0864
Epoch 2/30

32/82 [==========>...................] - ETA: 0s - loss: 0.2174 - mae: 0.3792 - mse: 0.2174
64/82 [======================>.......] - ETA: 0s - loss: 0.1827 - mae: 0.3403 - mse: 0.1827
82/82 [==============================] - 0s 5ms/step - loss: 0.1907 - mae: 0.3462 - mse: 0.1907 - val_loss: 0.0486 - val_mae: 0.1923 - val_mse: 0.0486
Epoch 3/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1656 - mae: 0.3145 - mse: 0.1656
64/82 [======================>.......] - ETA: 0s - loss: 0.1238 - mae: 0.2703 - mse: 0.1238
82/82 [==============================] - 1s 8ms/step - loss: 0.1255 - mae: 0.2763 - mse: 0.1255 - val_loss: 0.0822 - val_mae: 0.2222 - val_mse: 0.0822
Epoch 4/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0724 - mae: 0.2011 - mse: 0.0724
64/82 [======================>.......] - ETA: 0s - loss: 0.0756 - mae: 0.2075 - mse: 0.0756
82/82 [==============================] - 1s 7ms/step - loss: 0.0720 - mae: 0.2065 - mse: 0.0720 - val_loss: 0.1135 - val_mae: 0.2852 - val_mse: 0.1135
Epoch 5/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0410 - mae: 0.1734 - mse: 0.0410
64/82 [======================>.......] - ETA: 0s - loss: 0.0582 - mae: 0.1986 - mse: 0.0582
82/82 [==============================] - 1s 7ms/step - loss: 0.0637 - mae: 0.2024 - mse: 0.0637 - val_loss: 0.0973 - val_mae: 0.2359 - val_mse: 0.0973
Epoch 6/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0766 - mae: 0.2080 - mse: 0.0766
64/82 [======================>.......] - ETA: 0s - loss: 0.0583 - mae: 0.1836 - mse: 0.0583
82/82 [==============================] - 1s 7ms/step - loss: 0.0586 - mae: 0.1863 - mse: 0.0586 - val_loss: 0.0906 - val_mae: 0.2035 - val_mse: 0.0906
Epoch 7/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0575 - mae: 0.1911 - mse: 0.0575
64/82 [======================>.......] - ETA: 0s - loss: 0.0558 - mae: 0.1891 - mse: 0.0558
82/82 [==============================] - 1s 7ms/step - loss: 0.0581 - mae: 0.1882 - mse: 0.0581 - val_loss: 0.1194 - val_mae: 0.2546 - val_mse: 0.1194
Epoch 8/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0337 - mae: 0.1534 - mse: 0.0337
64/82 [======================>.......] - ETA: 0s - loss: 0.0632 - mae: 0.1898 - mse: 0.0632
82/82 [==============================] - 1s 7ms/step - loss: 0.0587 - mae: 0.1841 - mse: 0.0587 - val_loss: 0.1248 - val_mae: 0.2584 - val_mse: 0.1248
Epoch 9/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0396 - mae: 0.1538 - mse: 0.0396
64/82 [======================>.......] - ETA: 0s - loss: 0.0470 - mae: 0.1597 - mse: 0.0470
82/82 [==============================] - 1s 7ms/step - loss: 0.0560 - mae: 0.1688 - mse: 0.0560 - val_loss: 0.1001 - val_mae: 0.2003 - val_mse: 0.1001
Epoch 10/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0259 - mae: 0.1378 - mse: 0.0259
64/82 [======================>.......] - ETA: 0s - loss: 0.0474 - mae: 0.1606 - mse: 0.0474
82/82 [==============================] - 1s 7ms/step - loss: 0.0451 - mae: 0.1608 - mse: 0.0451 - val_loss: 0.0783 - val_mae: 0.1412 - val_mse: 0.0783
Epoch 11/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0599 - mae: 0.1870 - mse: 0.0599
64/82 [======================>.......] - ETA: 0s - loss: 0.0540 - mae: 0.1713 - mse: 0.0540
82/82 [==============================] - 1s 6ms/step - loss: 0.0493 - mae: 0.1680 - mse: 0.0493 - val_loss: 0.0877 - val_mae: 0.1671 - val_mse: 0.0877
Epoch 12/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0399 - mae: 0.1414 - mse: 0.0399
64/82 [======================>.......] - ETA: 0s - loss: 0.0390 - mae: 0.1562 - mse: 0.0390
82/82 [==============================] - 1s 7ms/step - loss: 0.0482 - mae: 0.1669 - mse: 0.0482 - val_loss: 0.1166 - val_mae: 0.2484 - val_mse: 0.1166
Epoch 13/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0520 - mae: 0.1563 - mse: 0.0520
64/82 [======================>.......] - ETA: 0s - loss: 0.0410 - mae: 0.1522 - mse: 0.0410
82/82 [==============================] - 1s 9ms/step - loss: 0.0504 - mae: 0.1632 - mse: 0.0504 - val_loss: 0.1036 - val_mae: 0.2127 - val_mse: 0.1036
Epoch 14/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0221 - mae: 0.1205 - mse: 0.0221
64/82 [======================>.......] - ETA: 0s - loss: 0.0396 - mae: 0.1554 - mse: 0.0396
82/82 [==============================] - 1s 8ms/step - loss: 0.0422 - mae: 0.1512 - mse: 0.0422 - val_loss: 0.0851 - val_mae: 0.1475 - val_mse: 0.0851
Epoch 15/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0609 - mae: 0.1795 - mse: 0.0609
64/82 [======================>.......] - ETA: 0s - loss: 0.0415 - mae: 0.1505 - mse: 0.0415
82/82 [==============================] - 1s 7ms/step - loss: 0.0446 - mae: 0.1486 - mse: 0.0446 - val_loss: 0.0919 - val_mae: 0.1751 - val_mse: 0.0919
Epoch 16/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0628 - mae: 0.1664 - mse: 0.0628
64/82 [======================>.......] - ETA: 0s - loss: 0.0420 - mae: 0.1451 - mse: 0.0420
82/82 [==============================] - 1s 7ms/step - loss: 0.0370 - mae: 0.1395 - mse: 0.0370 - val_loss: 0.0872 - val_mae: 0.1624 - val_mse: 0.0872
Epoch 17/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0202 - mae: 0.1142 - mse: 0.0202
64/82 [======================>.......] - ETA: 0s - loss: 0.0416 - mae: 0.1380 - mse: 0.0416
82/82 [==============================] - 1s 7ms/step - loss: 0.0358 - mae: 0.1317 - mse: 0.0358 - val_loss: 0.0960 - val_mae: 0.1919 - val_mse: 0.0960
Epoch 18/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0301 - mae: 0.1049 - mse: 0.0301
64/82 [======================>.......] - ETA: 0s - loss: 0.0317 - mae: 0.1142 - mse: 0.0317
82/82 [==============================] - 1s 7ms/step - loss: 0.0283 - mae: 0.1142 - mse: 0.0283 - val_loss: 0.0885 - val_mae: 0.1625 - val_mse: 0.0885
Epoch 19/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0354 - mae: 0.1161 - mse: 0.0354
64/82 [======================>.......] - ETA: 0s - loss: 0.0375 - mae: 0.1262 - mse: 0.0375
82/82 [==============================] - 1s 6ms/step - loss: 0.0337 - mae: 0.1224 - mse: 0.0337 - val_loss: 0.0845 - val_mae: 0.1441 - val_mse: 0.0845
Epoch 20/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0523 - mae: 0.1734 - mse: 0.0523
64/82 [======================>.......] - ETA: 0s - loss: 0.0414 - mae: 0.1395 - mse: 0.0414
82/82 [==============================] - 1s 6ms/step - loss: 0.0372 - mae: 0.1362 - mse: 0.0372 - val_loss: 0.0977 - val_mae: 0.1890 - val_mse: 0.0977
Epoch 21/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0171 - mae: 0.1044 - mse: 0.0171
64/82 [======================>.......] - ETA: 0s - loss: 0.0202 - mae: 0.1143 - mse: 0.0202
82/82 [==============================] - 1s 6ms/step - loss: 0.0395 - mae: 0.1292 - mse: 0.0395 - val_loss: 0.0985 - val_mae: 0.1876 - val_mse: 0.0985
Epoch 22/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0152 - mae: 0.1019 - mse: 0.0152
64/82 [======================>.......] - ETA: 0s - loss: 0.0276 - mae: 0.1158 - mse: 0.0276
82/82 [==============================] - 1s 7ms/step - loss: 0.0333 - mae: 0.1179 - mse: 0.0333 - val_loss: 0.0844 - val_mae: 0.1387 - val_mse: 0.0844
Epoch 23/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0381 - mae: 0.1303 - mse: 0.0381
64/82 [======================>.......] - ETA: 0s - loss: 0.0276 - mae: 0.1153 - mse: 0.0276
82/82 [==============================] - 1s 7ms/step - loss: 0.0349 - mae: 0.1272 - mse: 0.0349 - val_loss: 0.0789 - val_mae: 0.1413 - val_mse: 0.0789
Epoch 24/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0531 - mae: 0.1401 - mse: 0.0531
64/82 [======================>.......] - ETA: 0s - loss: 0.0344 - mae: 0.1172 - mse: 0.0344
82/82 [==============================] - 1s 7ms/step - loss: 0.0314 - mae: 0.1190 - mse: 0.0314 - val_loss: 0.0777 - val_mae: 0.1608 - val_mse: 0.0777
Epoch 25/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0304 - mae: 0.1201 - mse: 0.0304
64/82 [======================>.......] - ETA: 0s - loss: 0.0316 - mae: 0.1221 - mse: 0.0316
82/82 [==============================] - 1s 7ms/step - loss: 0.0297 - mae: 0.1219 - mse: 0.0297 - val_loss: 0.0719 - val_mae: 0.1391 - val_mse: 0.0719
Epoch 26/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0181 - mae: 0.1126 - mse: 0.0181
64/82 [======================>.......] - ETA: 0s - loss: 0.0261 - mae: 0.1093 - mse: 0.0261
82/82 [==============================] - 1s 8ms/step - loss: 0.0307 - mae: 0.1137 - mse: 0.0307 - val_loss: 0.0757 - val_mae: 0.1373 - val_mse: 0.0757
Epoch 27/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0455 - mae: 0.1387 - mse: 0.0455
64/82 [======================>.......] - ETA: 0s - loss: 0.0425 - mae: 0.1363 - mse: 0.0425
82/82 [==============================] - 1s 8ms/step - loss: 0.0368 - mae: 0.1280 - mse: 0.0368 - val_loss: 0.0937 - val_mae: 0.2079 - val_mse: 0.0937
Epoch 28/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0436 - mae: 0.1275 - mse: 0.0436
64/82 [======================>.......] - ETA: 0s - loss: 0.0371 - mae: 0.1177 - mse: 0.0371
82/82 [==============================] - 1s 7ms/step - loss: 0.0318 - mae: 0.1103 - mse: 0.0318 - val_loss: 0.0793 - val_mae: 0.1638 - val_mse: 0.0793
Epoch 29/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0335 - mae: 0.1118 - mse: 0.0335
64/82 [======================>.......] - ETA: 0s - loss: 0.0235 - mae: 0.1025 - mse: 0.0235
82/82 [==============================] - 1s 7ms/step - loss: 0.0294 - mae: 0.1110 - mse: 0.0294 - val_loss: 0.0753 - val_mae: 0.1553 - val_mse: 0.0753
Epoch 30/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0292 - mae: 0.1034 - mse: 0.0292
64/82 [======================>.......] - ETA: 0s - loss: 0.0313 - mae: 0.1069 - mse: 0.0313
82/82 [==============================] - 1s 7ms/step - loss: 0.0273 - mae: 0.1053 - mse: 0.0273 - val_loss: 0.0821 - val_mae: 0.1843 - val_mse: 0.0821
Saving trained model...
105
Testing...
heightdiff= [0.         0.         0.         0.         0.         6.87562561]
average prediction= [4.5128565]
baseline= 6.891304347826087
eachuser= [0. 0. 0. 0. 0. 3.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 2.291875203450521
['train-height-2.py', '0']
2_155_65_2_csi_a2_20.dat
155 2
2_155_65_2_csi_a2_23.dat
155 4
2_155_65_2_csi_a2_16.dat
155 6
155 7
155 8
155 9
2_155_65_2_csi_a2_1.dat
2_155_65_2_csi_a2_21.dat
155 12
155 13
155 14
2_155_65_2_csi_a2_25.dat
2_155_65_2_csi_a2_18.dat
2_155_65_2_csi_a2_17.dat
2_155_65_2_csi_a2_14.dat
155 19
155 20
155 21
155 22
155 23
155 24
2_155_65_2_csi_a2_27.dat
2_155_65_2_csi_a2_24.dat
155 27
155 28
2_155_65_2_csi_a2_7.dat
155 30
170 31
170 32
170 33
2_170_60_2_csi_a2_2.dat
170 35
170 36
170 37
170 38
2_170_60_2_csi_a2_10.dat
170 40
1_165_65_2_csi_a2_30.dat
1_165_65_2_csi_a2_15.dat
1_165_65_2_csi_a2_10.dat
1_165_65_2_csi_a2_21.dat
1_165_65_2_csi_a2_9.dat
1_165_65_2_csi_a2_13.dat
1_165_65_2_csi_a2_19.dat
1_165_65_2_csi_a2_18.dat
1_165_65_2_csi_a2_7.dat
1_165_65_2_csi_a2_20.dat
1_165_65_2_csi_a2_11.dat
1_165_65_2_csi_a2_26.dat
1_165_65_2_csi_a2_16.dat
1_165_65_2_csi_a2_28.dat
1_165_65_2_csi_a2_5.dat
1_165_65_2_csi_a2_6.dat
1_165_65_2_csi_a2_25.dat
1_165_65_2_csi_a2_29.dat
1_165_65_2_csi_a2_23.dat
1_165_65_2_csi_a2_3.dat
1_165_65_2_csi_a2_27.dat
1_165_65_2_csi_a2_2.dat
1_165_65_2_csi_a2_17.dat
1_165_65_2_csi_a2_8.dat
1_165_65_2_csi_a2_14.dat
1_165_65_2_csi_a2_1.dat
1_165_65_2_csi_a2_24.dat
1_165_65_2_csi_a2_4.dat
1_165_65_2_csi_a2_22.dat
1_165_65_2_csi_a2_12.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
2_165_50_2_csi_a2_13.dat
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
2_165_50_2_csi_a2_24.dat
165 100
1_175_70_2_csi_a2_25.dat
175 102
1_175_70_2_csi_a2_20.dat
1_175_70_2_csi_a2_28.dat
175 105
1_175_70_2_csi_a2_22.dat
1_175_70_2_csi_a2_5.dat
1_175_70_2_csi_a2_11.dat
1_175_70_2_csi_a2_6.dat
1_175_70_2_csi_a2_8.dat
1_175_70_2_csi_a2_3.dat
1_175_70_2_csi_a2_16.dat
1_175_70_2_csi_a2_19.dat
1_175_70_2_csi_a2_1.dat
1_175_70_2_csi_a2_27.dat
175 116
1_175_70_2_csi_a2_18.dat
1_175_70_2_csi_a2_23.dat
1_175_70_2_csi_a2_21.dat
1_175_70_2_csi_a2_12.dat
1_175_70_2_csi_a2_30.dat
1_175_70_2_csi_a2_15.dat
1_175_70_2_csi_a2_17.dat
1_175_70_2_csi_a2_14.dat
1_175_70_2_csi_a2_9.dat
1_175_70_2_csi_a2_24.dat
1_175_70_2_csi_a2_10.dat
1_175_70_2_csi_a2_4.dat
1_175_70_2_csi_a2_29.dat
1_175_70_2_csi_a2_13.dat
1_180_85_2_csi_a2_22.dat
1_180_85_2_csi_a2_17.dat
1_180_85_2_csi_a2_15.dat
1_180_85_2_csi_a2_21.dat
180 135
1_180_85_2_csi_a2_7.dat
1_180_85_2_csi_a2_25.dat
1_180_85_2_csi_a2_19.dat
180 139
1_180_85_2_csi_a2_6.dat
1_180_85_2_csi_a2_1.dat
1_180_85_2_csi_a2_16.dat
1_180_85_2_csi_a2_14.dat
1_180_85_2_csi_a2_23.dat
1_180_85_2_csi_a2_24.dat
1_180_85_2_csi_a2_8.dat
180 147
1_180_85_2_csi_a2_13.dat
1_180_85_2_csi_a2_29.dat
1_180_85_2_csi_a2_9.dat
1_180_85_2_csi_a2_18.dat
1_180_85_2_csi_a2_28.dat
1_180_85_2_csi_a2_12.dat
1_180_85_2_csi_a2_30.dat
1_180_85_2_csi_a2_20.dat
180 156
1_180_85_2_csi_a2_26.dat
1_180_85_2_csi_a2_11.dat
180 159
1_180_85_2_csi_a2_10.dat
180 161
180 162
180 163
180 164
180 165
180 166
1_180_75_2_csi_a2_9.dat
180 168
180 169
180 170
180 171
180 172
180 173
180 174
180 175
180 176
180 177
180 178
180 179
180 180
180 181
1_180_75_2_csi_a2_1.dat
180 183
180 184
180 185
1_180_75_2_csi_a2_4.dat
180 187
180 188
180 189
180 190
173 191
173 192
173 193
173 194
173 195
173 196
173 197
173 198
173 199
1_173_85_2_csi_a2_23.dat
173 201
173 202
173 203
173 204
173 205
173 206
173 207
1_173_85_2_csi_a2_13.dat
173 209
173 210
173 211
173 212
173 213
173 214
173 215
173 216
1_173_85_2_csi_a2_28.dat
173 218
1_173_85_2_csi_a2_4.dat
173 220
(115, 30, 3)
(115, 421, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155
 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 173
 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173
 173 173 173 173 173 173 173]
(115, 421, 30, 3, 1)

Loaded dataset of 115 samples, each sized (421, 30, 3, 1)


Train on 92 samples
Test on 23 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 421, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 421, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 421, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 421, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 421, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 421, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 421, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 82 samples, validate on 10 samples
Epoch 1/30

32/82 [==========>...................] - ETA: 1s - loss: 0.4437 - mae: 0.5956 - mse: 0.4437
64/82 [======================>.......] - ETA: 0s - loss: 0.3713 - mae: 0.5330 - mse: 0.3713
82/82 [==============================] - 1s 15ms/step - loss: 0.3459 - mae: 0.5166 - mse: 0.3459 - val_loss: 0.2210 - val_mae: 0.3927 - val_mse: 0.2210
Epoch 2/30

32/82 [==========>...................] - ETA: 0s - loss: 0.2092 - mae: 0.3878 - mse: 0.2092
64/82 [======================>.......] - ETA: 0s - loss: 0.2029 - mae: 0.3916 - mse: 0.2029
82/82 [==============================] - 1s 9ms/step - loss: 0.1850 - mae: 0.3676 - mse: 0.1850 - val_loss: 0.1282 - val_mae: 0.3242 - val_mse: 0.1282
Epoch 3/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1633 - mae: 0.3167 - mse: 0.1633
64/82 [======================>.......] - ETA: 0s - loss: 0.1903 - mae: 0.3492 - mse: 0.1903
82/82 [==============================] - 1s 7ms/step - loss: 0.1730 - mae: 0.3299 - mse: 0.1730 - val_loss: 0.1134 - val_mae: 0.2935 - val_mse: 0.1134
Epoch 4/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1586 - mae: 0.3193 - mse: 0.1586
64/82 [======================>.......] - ETA: 0s - loss: 0.1369 - mae: 0.3022 - mse: 0.1369
82/82 [==============================] - 1s 6ms/step - loss: 0.1208 - mae: 0.2796 - mse: 0.1208 - val_loss: 0.1340 - val_mae: 0.2911 - val_mse: 0.1340
Epoch 5/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0795 - mae: 0.2288 - mse: 0.0795
64/82 [======================>.......] - ETA: 0s - loss: 0.1097 - mae: 0.2755 - mse: 0.1097
82/82 [==============================] - 1s 7ms/step - loss: 0.1039 - mae: 0.2671 - mse: 0.1039 - val_loss: 0.1606 - val_mae: 0.3305 - val_mse: 0.1606
Epoch 6/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0782 - mae: 0.2247 - mse: 0.0782
64/82 [======================>.......] - ETA: 0s - loss: 0.0926 - mae: 0.2531 - mse: 0.0926
82/82 [==============================] - 1s 7ms/step - loss: 0.0915 - mae: 0.2500 - mse: 0.0915 - val_loss: 0.1412 - val_mae: 0.3002 - val_mse: 0.1412
Epoch 7/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0530 - mae: 0.1680 - mse: 0.0530
64/82 [======================>.......] - ETA: 0s - loss: 0.0517 - mae: 0.1793 - mse: 0.0517
82/82 [==============================] - 1s 7ms/step - loss: 0.0558 - mae: 0.1827 - mse: 0.0558 - val_loss: 0.1062 - val_mae: 0.2389 - val_mse: 0.1062
Epoch 8/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0584 - mae: 0.1892 - mse: 0.0584
64/82 [======================>.......] - ETA: 0s - loss: 0.0613 - mae: 0.1949 - mse: 0.0613
82/82 [==============================] - 1s 7ms/step - loss: 0.0575 - mae: 0.1859 - mse: 0.0575 - val_loss: 0.0900 - val_mae: 0.2082 - val_mse: 0.0900
Epoch 9/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0659 - mae: 0.1941 - mse: 0.0659
64/82 [======================>.......] - ETA: 0s - loss: 0.0495 - mae: 0.1699 - mse: 0.0495
82/82 [==============================] - 1s 7ms/step - loss: 0.0521 - mae: 0.1781 - mse: 0.0521 - val_loss: 0.0940 - val_mae: 0.1940 - val_mse: 0.0940
Epoch 10/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0520 - mae: 0.1661 - mse: 0.0520
64/82 [======================>.......] - ETA: 0s - loss: 0.0512 - mae: 0.1699 - mse: 0.0512
82/82 [==============================] - 1s 7ms/step - loss: 0.0436 - mae: 0.1566 - mse: 0.0436 - val_loss: 0.1112 - val_mae: 0.2370 - val_mse: 0.1112
Epoch 11/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0424 - mae: 0.1664 - mse: 0.0424
64/82 [======================>.......] - ETA: 0s - loss: 0.0490 - mae: 0.1670 - mse: 0.0490
82/82 [==============================] - 1s 8ms/step - loss: 0.0443 - mae: 0.1569 - mse: 0.0443 - val_loss: 0.1240 - val_mae: 0.2683 - val_mse: 0.1240
Epoch 12/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0532 - mae: 0.1555 - mse: 0.0532
64/82 [======================>.......] - ETA: 0s - loss: 0.0370 - mae: 0.1365 - mse: 0.0370
82/82 [==============================] - 1s 6ms/step - loss: 0.0334 - mae: 0.1343 - mse: 0.0334 - val_loss: 0.1077 - val_mae: 0.2403 - val_mse: 0.1077
Epoch 13/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0197 - mae: 0.1112 - mse: 0.0197
64/82 [======================>.......] - ETA: 0s - loss: 0.0436 - mae: 0.1408 - mse: 0.0436
82/82 [==============================] - 0s 5ms/step - loss: 0.0406 - mae: 0.1388 - mse: 0.0406 - val_loss: 0.0910 - val_mae: 0.2043 - val_mse: 0.0910
Epoch 14/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0533 - mae: 0.1520 - mse: 0.0533
64/82 [======================>.......] - ETA: 0s - loss: 0.0384 - mae: 0.1374 - mse: 0.0384
82/82 [==============================] - 0s 5ms/step - loss: 0.0354 - mae: 0.1355 - mse: 0.0354 - val_loss: 0.0931 - val_mae: 0.1992 - val_mse: 0.0931
Epoch 15/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0586 - mae: 0.1742 - mse: 0.0586
64/82 [======================>.......] - ETA: 0s - loss: 0.0391 - mae: 0.1424 - mse: 0.0391
82/82 [==============================] - 0s 5ms/step - loss: 0.0424 - mae: 0.1439 - mse: 0.0424 - val_loss: 0.1019 - val_mae: 0.2047 - val_mse: 0.1019
Epoch 16/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0340 - mae: 0.1231 - mse: 0.0340
64/82 [======================>.......] - ETA: 0s - loss: 0.0420 - mae: 0.1398 - mse: 0.0420
82/82 [==============================] - 0s 5ms/step - loss: 0.0363 - mae: 0.1332 - mse: 0.0363 - val_loss: 0.1162 - val_mae: 0.2360 - val_mse: 0.1162
Epoch 17/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0224 - mae: 0.1192 - mse: 0.0224
64/82 [======================>.......] - ETA: 0s - loss: 0.0508 - mae: 0.1457 - mse: 0.0508
82/82 [==============================] - 0s 5ms/step - loss: 0.0418 - mae: 0.1313 - mse: 0.0418 - val_loss: 0.1183 - val_mae: 0.2395 - val_mse: 0.1183
Epoch 18/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0451 - mae: 0.1457 - mse: 0.0451
64/82 [======================>.......] - ETA: 0s - loss: 0.0337 - mae: 0.1341 - mse: 0.0337
82/82 [==============================] - 0s 5ms/step - loss: 0.0380 - mae: 0.1407 - mse: 0.0380 - val_loss: 0.1044 - val_mae: 0.2064 - val_mse: 0.1044
Epoch 19/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0216 - mae: 0.1066 - mse: 0.0216
64/82 [======================>.......] - ETA: 0s - loss: 0.0255 - mae: 0.1109 - mse: 0.0255
82/82 [==============================] - 0s 5ms/step - loss: 0.0330 - mae: 0.1200 - mse: 0.0330 - val_loss: 0.0988 - val_mae: 0.1931 - val_mse: 0.0988
Epoch 20/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0442 - mae: 0.1495 - mse: 0.0442
64/82 [======================>.......] - ETA: 0s - loss: 0.0387 - mae: 0.1358 - mse: 0.0387
82/82 [==============================] - 0s 5ms/step - loss: 0.0353 - mae: 0.1358 - mse: 0.0353 - val_loss: 0.1009 - val_mae: 0.2082 - val_mse: 0.1009
Epoch 21/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0133 - mae: 0.0920 - mse: 0.0133
64/82 [======================>.......] - ETA: 0s - loss: 0.0220 - mae: 0.1054 - mse: 0.0220
82/82 [==============================] - 0s 5ms/step - loss: 0.0291 - mae: 0.1126 - mse: 0.0291 - val_loss: 0.1121 - val_mae: 0.2384 - val_mse: 0.1121
Epoch 22/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0166 - mae: 0.1018 - mse: 0.0166
64/82 [======================>.......] - ETA: 0s - loss: 0.0393 - mae: 0.1370 - mse: 0.0393
82/82 [==============================] - 0s 4ms/step - loss: 0.0345 - mae: 0.1291 - mse: 0.0345 - val_loss: 0.0969 - val_mae: 0.2032 - val_mse: 0.0969
Epoch 23/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0329 - mae: 0.1215 - mse: 0.0329
64/82 [======================>.......] - ETA: 0s - loss: 0.0403 - mae: 0.1326 - mse: 0.0403
82/82 [==============================] - 0s 5ms/step - loss: 0.0355 - mae: 0.1247 - mse: 0.0355 - val_loss: 0.0900 - val_mae: 0.1701 - val_mse: 0.0900
Epoch 24/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0778 - mae: 0.1630 - mse: 0.0778
64/82 [======================>.......] - ETA: 0s - loss: 0.0440 - mae: 0.1216 - mse: 0.0440
82/82 [==============================] - 0s 4ms/step - loss: 0.0367 - mae: 0.1127 - mse: 0.0367 - val_loss: 0.0953 - val_mae: 0.1876 - val_mse: 0.0953
Epoch 25/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0355 - mae: 0.1132 - mse: 0.0355
64/82 [======================>.......] - ETA: 0s - loss: 0.0340 - mae: 0.1089 - mse: 0.0340
82/82 [==============================] - 0s 5ms/step - loss: 0.0307 - mae: 0.1114 - mse: 0.0307 - val_loss: 0.1034 - val_mae: 0.2149 - val_mse: 0.1034
Epoch 26/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0142 - mae: 0.1032 - mse: 0.0142
64/82 [======================>.......] - ETA: 0s - loss: 0.0260 - mae: 0.1119 - mse: 0.0260
82/82 [==============================] - 0s 5ms/step - loss: 0.0347 - mae: 0.1218 - mse: 0.0347 - val_loss: 0.0959 - val_mae: 0.1931 - val_mse: 0.0959
Epoch 27/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0109 - mae: 0.0875 - mse: 0.0109
64/82 [======================>.......] - ETA: 0s - loss: 0.0378 - mae: 0.1242 - mse: 0.0378
82/82 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1243 - mse: 0.0343 - val_loss: 0.0873 - val_mae: 0.1789 - val_mse: 0.0873
Epoch 28/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0162 - mae: 0.1109 - mse: 0.0162
64/82 [======================>.......] - ETA: 0s - loss: 0.0226 - mae: 0.1091 - mse: 0.0226
82/82 [==============================] - 0s 5ms/step - loss: 0.0277 - mae: 0.1143 - mse: 0.0277 - val_loss: 0.0930 - val_mae: 0.2028 - val_mse: 0.0930
Epoch 29/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0372 - mae: 0.1283 - mse: 0.0372
64/82 [======================>.......] - ETA: 0s - loss: 0.0354 - mae: 0.1249 - mse: 0.0354
82/82 [==============================] - 0s 5ms/step - loss: 0.0299 - mae: 0.1157 - mse: 0.0299 - val_loss: 0.1035 - val_mae: 0.2295 - val_mse: 0.1035
Epoch 30/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0397 - mae: 0.1335 - mse: 0.0397
64/82 [======================>.......] - ETA: 0s - loss: 0.0356 - mae: 0.1239 - mse: 0.0356
82/82 [==============================] - 0s 5ms/step - loss: 0.0321 - mae: 0.1195 - mse: 0.0321 - val_loss: 0.0935 - val_mae: 0.1992 - val_mse: 0.0935
Saving trained model...
105
Testing...
heightdiff= [0.        0.        0.        0.        0.        6.1782074]
average prediction= [4.7315373]
baseline= 8.23913043478261
eachuser= [0. 0. 0. 0. 0. 4.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 1.5445518493652344
['train-height-2.py', '0']
2_155_65_2_csi_a2_20.dat
155 2
2_155_65_2_csi_a2_23.dat
155 4
2_155_65_2_csi_a2_16.dat
155 6
155 7
155 8
155 9
2_155_65_2_csi_a2_1.dat
2_155_65_2_csi_a2_21.dat
155 12
155 13
155 14
2_155_65_2_csi_a2_25.dat
2_155_65_2_csi_a2_18.dat
2_155_65_2_csi_a2_17.dat
2_155_65_2_csi_a2_14.dat
155 19
155 20
155 21
155 22
155 23
155 24
2_155_65_2_csi_a2_27.dat
2_155_65_2_csi_a2_24.dat
155 27
155 28
2_155_65_2_csi_a2_7.dat
155 30
170 31
170 32
170 33
2_170_60_2_csi_a2_2.dat
170 35
170 36
170 37
170 38
2_170_60_2_csi_a2_10.dat
170 40
1_165_65_2_csi_a2_30.dat
1_165_65_2_csi_a2_15.dat
1_165_65_2_csi_a2_10.dat
1_165_65_2_csi_a2_21.dat
1_165_65_2_csi_a2_9.dat
1_165_65_2_csi_a2_13.dat
1_165_65_2_csi_a2_19.dat
1_165_65_2_csi_a2_18.dat
1_165_65_2_csi_a2_7.dat
1_165_65_2_csi_a2_20.dat
1_165_65_2_csi_a2_11.dat
1_165_65_2_csi_a2_26.dat
1_165_65_2_csi_a2_16.dat
1_165_65_2_csi_a2_28.dat
1_165_65_2_csi_a2_5.dat
1_165_65_2_csi_a2_6.dat
1_165_65_2_csi_a2_25.dat
1_165_65_2_csi_a2_29.dat
1_165_65_2_csi_a2_23.dat
1_165_65_2_csi_a2_3.dat
1_165_65_2_csi_a2_27.dat
1_165_65_2_csi_a2_2.dat
1_165_65_2_csi_a2_17.dat
1_165_65_2_csi_a2_8.dat
1_165_65_2_csi_a2_14.dat
1_165_65_2_csi_a2_1.dat
1_165_65_2_csi_a2_24.dat
1_165_65_2_csi_a2_4.dat
1_165_65_2_csi_a2_22.dat
1_165_65_2_csi_a2_12.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
2_165_50_2_csi_a2_13.dat
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
2_165_50_2_csi_a2_24.dat
165 100
1_175_70_2_csi_a2_25.dat
175 102
1_175_70_2_csi_a2_20.dat
1_175_70_2_csi_a2_28.dat
175 105
1_175_70_2_csi_a2_22.dat
1_175_70_2_csi_a2_5.dat
1_175_70_2_csi_a2_11.dat
1_175_70_2_csi_a2_6.dat
1_175_70_2_csi_a2_8.dat
1_175_70_2_csi_a2_3.dat
1_175_70_2_csi_a2_16.dat
1_175_70_2_csi_a2_19.dat
1_175_70_2_csi_a2_1.dat
1_175_70_2_csi_a2_27.dat
175 116
1_175_70_2_csi_a2_18.dat
1_175_70_2_csi_a2_23.dat
1_175_70_2_csi_a2_21.dat
1_175_70_2_csi_a2_12.dat
1_175_70_2_csi_a2_30.dat
1_175_70_2_csi_a2_15.dat
1_175_70_2_csi_a2_17.dat
1_175_70_2_csi_a2_14.dat
1_175_70_2_csi_a2_9.dat
1_175_70_2_csi_a2_24.dat
1_175_70_2_csi_a2_10.dat
1_175_70_2_csi_a2_4.dat
1_175_70_2_csi_a2_29.dat
1_175_70_2_csi_a2_13.dat
1_180_85_2_csi_a2_22.dat
1_180_85_2_csi_a2_17.dat
1_180_85_2_csi_a2_15.dat
1_180_85_2_csi_a2_21.dat
180 135
1_180_85_2_csi_a2_7.dat
1_180_85_2_csi_a2_25.dat
1_180_85_2_csi_a2_19.dat
180 139
1_180_85_2_csi_a2_6.dat
1_180_85_2_csi_a2_1.dat
1_180_85_2_csi_a2_16.dat
1_180_85_2_csi_a2_14.dat
1_180_85_2_csi_a2_23.dat
1_180_85_2_csi_a2_24.dat
1_180_85_2_csi_a2_8.dat
180 147
1_180_85_2_csi_a2_13.dat
1_180_85_2_csi_a2_29.dat
1_180_85_2_csi_a2_9.dat
1_180_85_2_csi_a2_18.dat
1_180_85_2_csi_a2_28.dat
1_180_85_2_csi_a2_12.dat
1_180_85_2_csi_a2_30.dat
1_180_85_2_csi_a2_20.dat
180 156
1_180_85_2_csi_a2_26.dat
1_180_85_2_csi_a2_11.dat
180 159
1_180_85_2_csi_a2_10.dat
180 161
180 162
180 163
180 164
180 165
180 166
1_180_75_2_csi_a2_9.dat
180 168
180 169
180 170
180 171
180 172
180 173
180 174
180 175
180 176
180 177
180 178
180 179
180 180
180 181
1_180_75_2_csi_a2_1.dat
180 183
180 184
180 185
1_180_75_2_csi_a2_4.dat
180 187
180 188
180 189
180 190
173 191
173 192
173 193
173 194
173 195
173 196
173 197
173 198
173 199
1_173_85_2_csi_a2_23.dat
173 201
173 202
173 203
173 204
173 205
173 206
173 207
1_173_85_2_csi_a2_13.dat
173 209
173 210
173 211
173 212
173 213
173 214
173 215
173 216
1_173_85_2_csi_a2_28.dat
173 218
1_173_85_2_csi_a2_4.dat
173 220
(115, 30, 3)
(115, 421, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155
 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 173
 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173
 173 173 173 173 173 173 173]
(115, 421, 30, 3, 1)

Loaded dataset of 115 samples, each sized (421, 30, 3, 1)


Train on 92 samples
Test on 23 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 421, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 421, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 421, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 421, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 421, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 421, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 421, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 82 samples, validate on 10 samples
Epoch 1/30

32/82 [==========>...................] - ETA: 0s - loss: 0.3847 - mae: 0.5442 - mse: 0.3847
64/82 [======================>.......] - ETA: 0s - loss: 0.3230 - mae: 0.4873 - mse: 0.3230
82/82 [==============================] - 1s 12ms/step - loss: 0.2901 - mae: 0.4590 - mse: 0.2901 - val_loss: 0.1451 - val_mae: 0.3184 - val_mse: 0.1451
Epoch 2/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0897 - mae: 0.2449 - mse: 0.0897
64/82 [======================>.......] - ETA: 0s - loss: 0.1155 - mae: 0.2879 - mse: 0.1155
82/82 [==============================] - 1s 7ms/step - loss: 0.1560 - mae: 0.3243 - mse: 0.1560 - val_loss: 0.0430 - val_mae: 0.1678 - val_mse: 0.0430
Epoch 3/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0733 - mae: 0.2202 - mse: 0.0733
64/82 [======================>.......] - ETA: 0s - loss: 0.1072 - mae: 0.2608 - mse: 0.1072
82/82 [==============================] - 1s 7ms/step - loss: 0.1119 - mae: 0.2698 - mse: 0.1119 - val_loss: 0.0934 - val_mae: 0.2461 - val_mse: 0.0934
Epoch 4/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0619 - mae: 0.1843 - mse: 0.0619
64/82 [======================>.......] - ETA: 0s - loss: 0.0625 - mae: 0.1998 - mse: 0.0625
82/82 [==============================] - 1s 7ms/step - loss: 0.0724 - mae: 0.2149 - mse: 0.0724 - val_loss: 0.1496 - val_mae: 0.3382 - val_mse: 0.1496
Epoch 5/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0955 - mae: 0.2633 - mse: 0.0955
64/82 [======================>.......] - ETA: 0s - loss: 0.0779 - mae: 0.2391 - mse: 0.0779
82/82 [==============================] - 1s 7ms/step - loss: 0.0927 - mae: 0.2621 - mse: 0.0927 - val_loss: 0.1462 - val_mae: 0.3313 - val_mse: 0.1462
Epoch 6/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1027 - mae: 0.2465 - mse: 0.1027
64/82 [======================>.......] - ETA: 0s - loss: 0.0956 - mae: 0.2514 - mse: 0.0956
82/82 [==============================] - 1s 6ms/step - loss: 0.0855 - mae: 0.2329 - mse: 0.0855 - val_loss: 0.1026 - val_mae: 0.2513 - val_mse: 0.1026
Epoch 7/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0660 - mae: 0.1965 - mse: 0.0660
64/82 [======================>.......] - ETA: 0s - loss: 0.0688 - mae: 0.2008 - mse: 0.0688
82/82 [==============================] - 1s 7ms/step - loss: 0.0614 - mae: 0.1918 - mse: 0.0614 - val_loss: 0.0738 - val_mae: 0.1622 - val_mse: 0.0738
Epoch 8/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0307 - mae: 0.1350 - mse: 0.0307
64/82 [======================>.......] - ETA: 0s - loss: 0.0447 - mae: 0.1621 - mse: 0.0447
82/82 [==============================] - 1s 7ms/step - loss: 0.0555 - mae: 0.1767 - mse: 0.0555 - val_loss: 0.0703 - val_mae: 0.1313 - val_mse: 0.0703
Epoch 9/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0691 - mae: 0.1896 - mse: 0.0691
64/82 [======================>.......] - ETA: 0s - loss: 0.0560 - mae: 0.1805 - mse: 0.0560
82/82 [==============================] - 1s 7ms/step - loss: 0.0575 - mae: 0.1819 - mse: 0.0575 - val_loss: 0.0830 - val_mae: 0.1666 - val_mse: 0.0830
Epoch 10/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0647 - mae: 0.1908 - mse: 0.0647
64/82 [======================>.......] - ETA: 0s - loss: 0.0685 - mae: 0.1941 - mse: 0.0685
82/82 [==============================] - 1s 7ms/step - loss: 0.0570 - mae: 0.1741 - mse: 0.0570 - val_loss: 0.1147 - val_mae: 0.2575 - val_mse: 0.1147
Epoch 11/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0343 - mae: 0.1641 - mse: 0.0343
64/82 [======================>.......] - ETA: 0s - loss: 0.0440 - mae: 0.1775 - mse: 0.0440
82/82 [==============================] - 1s 7ms/step - loss: 0.0587 - mae: 0.1911 - mse: 0.0587 - val_loss: 0.1232 - val_mae: 0.2759 - val_mse: 0.1232
Epoch 12/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0424 - mae: 0.1460 - mse: 0.0424
64/82 [======================>.......] - ETA: 0s - loss: 0.0603 - mae: 0.1742 - mse: 0.0603
82/82 [==============================] - 1s 7ms/step - loss: 0.0543 - mae: 0.1697 - mse: 0.0543 - val_loss: 0.1018 - val_mae: 0.2245 - val_mse: 0.1018
Epoch 13/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0830 - mae: 0.2056 - mse: 0.0830
64/82 [======================>.......] - ETA: 0s - loss: 0.0511 - mae: 0.1571 - mse: 0.0511
82/82 [==============================] - 1s 7ms/step - loss: 0.0471 - mae: 0.1551 - mse: 0.0471 - val_loss: 0.0805 - val_mae: 0.1466 - val_mse: 0.0805
Epoch 14/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0474 - mae: 0.1545 - mse: 0.0474
64/82 [======================>.......] - ETA: 0s - loss: 0.0478 - mae: 0.1598 - mse: 0.0478
82/82 [==============================] - 1s 7ms/step - loss: 0.0531 - mae: 0.1661 - mse: 0.0531 - val_loss: 0.0756 - val_mae: 0.1276 - val_mse: 0.0756
Epoch 15/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0509 - mae: 0.1669 - mse: 0.0509
64/82 [======================>.......] - ETA: 0s - loss: 0.0465 - mae: 0.1653 - mse: 0.0465
82/82 [==============================] - 1s 7ms/step - loss: 0.0568 - mae: 0.1702 - mse: 0.0568 - val_loss: 0.0857 - val_mae: 0.1683 - val_mse: 0.0857
Epoch 16/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0185 - mae: 0.1163 - mse: 0.0185
64/82 [======================>.......] - ETA: 0s - loss: 0.0409 - mae: 0.1394 - mse: 0.0409
82/82 [==============================] - 1s 6ms/step - loss: 0.0447 - mae: 0.1428 - mse: 0.0447 - val_loss: 0.1025 - val_mae: 0.2237 - val_mse: 0.1025
Epoch 17/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0441 - mae: 0.1457 - mse: 0.0441
64/82 [======================>.......] - ETA: 0s - loss: 0.0453 - mae: 0.1482 - mse: 0.0453
82/82 [==============================] - 1s 7ms/step - loss: 0.0476 - mae: 0.1502 - mse: 0.0476 - val_loss: 0.0973 - val_mae: 0.2079 - val_mse: 0.0973
Epoch 18/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0284 - mae: 0.1363 - mse: 0.0284
64/82 [======================>.......] - ETA: 0s - loss: 0.0592 - mae: 0.1743 - mse: 0.0592
82/82 [==============================] - 1s 6ms/step - loss: 0.0532 - mae: 0.1672 - mse: 0.0532 - val_loss: 0.0835 - val_mae: 0.1636 - val_mse: 0.0835
Epoch 19/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0644 - mae: 0.1684 - mse: 0.0644
64/82 [======================>.......] - ETA: 0s - loss: 0.0505 - mae: 0.1518 - mse: 0.0505
82/82 [==============================] - 0s 5ms/step - loss: 0.0448 - mae: 0.1437 - mse: 0.0448 - val_loss: 0.0789 - val_mae: 0.1436 - val_mse: 0.0789
Epoch 20/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0570 - mae: 0.1603 - mse: 0.0570
64/82 [======================>.......] - ETA: 0s - loss: 0.0416 - mae: 0.1430 - mse: 0.0416
82/82 [==============================] - 0s 5ms/step - loss: 0.0446 - mae: 0.1430 - mse: 0.0446 - val_loss: 0.0789 - val_mae: 0.1432 - val_mse: 0.0789
Epoch 21/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0426 - mae: 0.1612 - mse: 0.0426
64/82 [======================>.......] - ETA: 0s - loss: 0.0417 - mae: 0.1484 - mse: 0.0417
82/82 [==============================] - 0s 5ms/step - loss: 0.0455 - mae: 0.1492 - mse: 0.0455 - val_loss: 0.0953 - val_mae: 0.2092 - val_mse: 0.0953
Epoch 22/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0384 - mae: 0.1293 - mse: 0.0384
64/82 [======================>.......] - ETA: 0s - loss: 0.0292 - mae: 0.1219 - mse: 0.0292
82/82 [==============================] - 0s 5ms/step - loss: 0.0417 - mae: 0.1306 - mse: 0.0417 - val_loss: 0.1019 - val_mae: 0.2286 - val_mse: 0.1019
Epoch 23/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0429 - mae: 0.1500 - mse: 0.0429
64/82 [======================>.......] - ETA: 0s - loss: 0.0307 - mae: 0.1340 - mse: 0.0307
82/82 [==============================] - 0s 5ms/step - loss: 0.0445 - mae: 0.1523 - mse: 0.0445 - val_loss: 0.0842 - val_mae: 0.1752 - val_mse: 0.0842
Epoch 24/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0546 - mae: 0.1443 - mse: 0.0546
64/82 [======================>.......] - ETA: 0s - loss: 0.0389 - mae: 0.1361 - mse: 0.0389
82/82 [==============================] - 0s 5ms/step - loss: 0.0422 - mae: 0.1430 - mse: 0.0422 - val_loss: 0.0705 - val_mae: 0.1203 - val_mse: 0.0705
Epoch 25/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0528 - mae: 0.1685 - mse: 0.0528
64/82 [======================>.......] - ETA: 0s - loss: 0.0479 - mae: 0.1594 - mse: 0.0479
82/82 [==============================] - 0s 5ms/step - loss: 0.0473 - mae: 0.1540 - mse: 0.0473 - val_loss: 0.0770 - val_mae: 0.1627 - val_mse: 0.0770
Epoch 26/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0390 - mae: 0.1617 - mse: 0.0390
64/82 [======================>.......] - ETA: 0s - loss: 0.0350 - mae: 0.1403 - mse: 0.0350
82/82 [==============================] - 0s 5ms/step - loss: 0.0462 - mae: 0.1562 - mse: 0.0462 - val_loss: 0.0955 - val_mae: 0.2238 - val_mse: 0.0955
Epoch 27/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0273 - mae: 0.1343 - mse: 0.0273
64/82 [======================>.......] - ETA: 0s - loss: 0.0510 - mae: 0.1526 - mse: 0.0510
82/82 [==============================] - 0s 5ms/step - loss: 0.0448 - mae: 0.1454 - mse: 0.0448 - val_loss: 0.0889 - val_mae: 0.2082 - val_mse: 0.0889
Epoch 28/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0739 - mae: 0.1767 - mse: 0.0739
64/82 [======================>.......] - ETA: 0s - loss: 0.0471 - mae: 0.1468 - mse: 0.0471
82/82 [==============================] - 0s 5ms/step - loss: 0.0418 - mae: 0.1399 - mse: 0.0418 - val_loss: 0.0749 - val_mae: 0.1639 - val_mse: 0.0749
Epoch 29/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0237 - mae: 0.1233 - mse: 0.0237
64/82 [======================>.......] - ETA: 0s - loss: 0.0428 - mae: 0.1454 - mse: 0.0428
82/82 [==============================] - 0s 5ms/step - loss: 0.0437 - mae: 0.1422 - mse: 0.0437 - val_loss: 0.0748 - val_mae: 0.1545 - val_mse: 0.0748
Epoch 30/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0374 - mae: 0.1354 - mse: 0.0374
64/82 [======================>.......] - ETA: 0s - loss: 0.0413 - mae: 0.1332 - mse: 0.0413
82/82 [==============================] - 0s 5ms/step - loss: 0.0369 - mae: 0.1294 - mse: 0.0369 - val_loss: 0.0787 - val_mae: 0.1701 - val_mse: 0.0787
Saving trained model...
105
Testing...
heightdiff= [ 0.         0.         0.         0.         0.        12.8223877]
average prediction= [3.6074967]
baseline= 7.413043478260869
eachuser= [0. 0. 0. 0. 0. 4.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 3.205596923828125
['train-height-2.py', '0']
2_155_65_2_csi_a2_20.dat
155 2
2_155_65_2_csi_a2_23.dat
155 4
2_155_65_2_csi_a2_16.dat
155 6
155 7
155 8
155 9
2_155_65_2_csi_a2_1.dat
2_155_65_2_csi_a2_21.dat
155 12
155 13
155 14
2_155_65_2_csi_a2_25.dat
2_155_65_2_csi_a2_18.dat
2_155_65_2_csi_a2_17.dat
2_155_65_2_csi_a2_14.dat
155 19
155 20
155 21
155 22
155 23
155 24
2_155_65_2_csi_a2_27.dat
2_155_65_2_csi_a2_24.dat
155 27
155 28
2_155_65_2_csi_a2_7.dat
155 30
170 31
170 32
170 33
2_170_60_2_csi_a2_2.dat
170 35
170 36
170 37
170 38
2_170_60_2_csi_a2_10.dat
170 40
1_165_65_2_csi_a2_30.dat
1_165_65_2_csi_a2_15.dat
1_165_65_2_csi_a2_10.dat
1_165_65_2_csi_a2_21.dat
1_165_65_2_csi_a2_9.dat
1_165_65_2_csi_a2_13.dat
1_165_65_2_csi_a2_19.dat
1_165_65_2_csi_a2_18.dat
1_165_65_2_csi_a2_7.dat
1_165_65_2_csi_a2_20.dat
1_165_65_2_csi_a2_11.dat
1_165_65_2_csi_a2_26.dat
1_165_65_2_csi_a2_16.dat
1_165_65_2_csi_a2_28.dat
1_165_65_2_csi_a2_5.dat
1_165_65_2_csi_a2_6.dat
1_165_65_2_csi_a2_25.dat
1_165_65_2_csi_a2_29.dat
1_165_65_2_csi_a2_23.dat
1_165_65_2_csi_a2_3.dat
1_165_65_2_csi_a2_27.dat
1_165_65_2_csi_a2_2.dat
1_165_65_2_csi_a2_17.dat
1_165_65_2_csi_a2_8.dat
1_165_65_2_csi_a2_14.dat
1_165_65_2_csi_a2_1.dat
1_165_65_2_csi_a2_24.dat
1_165_65_2_csi_a2_4.dat
1_165_65_2_csi_a2_22.dat
1_165_65_2_csi_a2_12.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
2_165_50_2_csi_a2_13.dat
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
2_165_50_2_csi_a2_24.dat
165 100
1_175_70_2_csi_a2_25.dat
175 102
1_175_70_2_csi_a2_20.dat
1_175_70_2_csi_a2_28.dat
175 105
1_175_70_2_csi_a2_22.dat
1_175_70_2_csi_a2_5.dat
1_175_70_2_csi_a2_11.dat
1_175_70_2_csi_a2_6.dat
1_175_70_2_csi_a2_8.dat
1_175_70_2_csi_a2_3.dat
1_175_70_2_csi_a2_16.dat
1_175_70_2_csi_a2_19.dat
1_175_70_2_csi_a2_1.dat
1_175_70_2_csi_a2_27.dat
175 116
1_175_70_2_csi_a2_18.dat
1_175_70_2_csi_a2_23.dat
1_175_70_2_csi_a2_21.dat
1_175_70_2_csi_a2_12.dat
1_175_70_2_csi_a2_30.dat
1_175_70_2_csi_a2_15.dat
1_175_70_2_csi_a2_17.dat
1_175_70_2_csi_a2_14.dat
1_175_70_2_csi_a2_9.dat
1_175_70_2_csi_a2_24.dat
1_175_70_2_csi_a2_10.dat
1_175_70_2_csi_a2_4.dat
1_175_70_2_csi_a2_29.dat
1_175_70_2_csi_a2_13.dat
1_180_85_2_csi_a2_22.dat
1_180_85_2_csi_a2_17.dat
1_180_85_2_csi_a2_15.dat
1_180_85_2_csi_a2_21.dat
180 135
1_180_85_2_csi_a2_7.dat
1_180_85_2_csi_a2_25.dat
1_180_85_2_csi_a2_19.dat
180 139
1_180_85_2_csi_a2_6.dat
1_180_85_2_csi_a2_1.dat
1_180_85_2_csi_a2_16.dat
1_180_85_2_csi_a2_14.dat
1_180_85_2_csi_a2_23.dat
1_180_85_2_csi_a2_24.dat
1_180_85_2_csi_a2_8.dat
180 147
1_180_85_2_csi_a2_13.dat
1_180_85_2_csi_a2_29.dat
1_180_85_2_csi_a2_9.dat
1_180_85_2_csi_a2_18.dat
1_180_85_2_csi_a2_28.dat
1_180_85_2_csi_a2_12.dat
1_180_85_2_csi_a2_30.dat
1_180_85_2_csi_a2_20.dat
180 156
1_180_85_2_csi_a2_26.dat
1_180_85_2_csi_a2_11.dat
180 159
1_180_85_2_csi_a2_10.dat
180 161
180 162
180 163
180 164
180 165
180 166
1_180_75_2_csi_a2_9.dat
180 168
180 169
180 170
180 171
180 172
180 173
180 174
180 175
180 176
180 177
180 178
180 179
180 180
180 181
1_180_75_2_csi_a2_1.dat
180 183
180 184
180 185
1_180_75_2_csi_a2_4.dat
180 187
180 188
180 189
180 190
173 191
173 192
173 193
173 194
173 195
173 196
173 197
173 198
173 199
1_173_85_2_csi_a2_23.dat
173 201
173 202
173 203
173 204
173 205
173 206
173 207
1_173_85_2_csi_a2_13.dat
173 209
173 210
173 211
173 212
173 213
173 214
173 215
173 216
1_173_85_2_csi_a2_28.dat
173 218
1_173_85_2_csi_a2_4.dat
173 220
(115, 30, 3)
(115, 421, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155
 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 173
 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173
 173 173 173 173 173 173 173]
(115, 421, 30, 3, 1)

Loaded dataset of 115 samples, each sized (421, 30, 3, 1)


Train on 92 samples
Test on 23 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 421, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 421, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 421, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 421, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 421, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 421, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 421, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 82 samples, validate on 10 samples
Epoch 1/30

32/82 [==========>...................] - ETA: 0s - loss: 0.4144 - mae: 0.5706 - mse: 0.4144
64/82 [======================>.......] - ETA: 0s - loss: 0.3804 - mae: 0.5415 - mse: 0.3804
82/82 [==============================] - 1s 10ms/step - loss: 0.3562 - mae: 0.5193 - mse: 0.3562 - val_loss: 0.2394 - val_mae: 0.4284 - val_mse: 0.2394
Epoch 2/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1853 - mae: 0.3835 - mse: 0.1853
64/82 [======================>.......] - ETA: 0s - loss: 0.1704 - mae: 0.3642 - mse: 0.1704
82/82 [==============================] - 0s 5ms/step - loss: 0.1736 - mae: 0.3709 - mse: 0.1736 - val_loss: 0.0711 - val_mae: 0.2407 - val_mse: 0.0711
Epoch 3/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1912 - mae: 0.3868 - mse: 0.1912
64/82 [======================>.......] - ETA: 0s - loss: 0.1823 - mae: 0.3633 - mse: 0.1823
82/82 [==============================] - 0s 5ms/step - loss: 0.1600 - mae: 0.3342 - mse: 0.1600 - val_loss: 0.0513 - val_mae: 0.1873 - val_mse: 0.0513
Epoch 4/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1845 - mae: 0.3342 - mse: 0.1845
64/82 [======================>.......] - ETA: 0s - loss: 0.1414 - mae: 0.2967 - mse: 0.1414
82/82 [==============================] - 0s 5ms/step - loss: 0.1333 - mae: 0.2848 - mse: 0.1333 - val_loss: 0.0691 - val_mae: 0.1923 - val_mse: 0.0691
Epoch 5/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1006 - mae: 0.2672 - mse: 0.1006
64/82 [======================>.......] - ETA: 0s - loss: 0.0848 - mae: 0.2378 - mse: 0.0848
82/82 [==============================] - 0s 5ms/step - loss: 0.0819 - mae: 0.2292 - mse: 0.0819 - val_loss: 0.0960 - val_mae: 0.2533 - val_mse: 0.0960
Epoch 6/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0573 - mae: 0.1945 - mse: 0.0573
64/82 [======================>.......] - ETA: 0s - loss: 0.0712 - mae: 0.2058 - mse: 0.0712
82/82 [==============================] - 0s 5ms/step - loss: 0.0688 - mae: 0.2029 - mse: 0.0688 - val_loss: 0.0851 - val_mae: 0.2281 - val_mse: 0.0851
Epoch 7/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0783 - mae: 0.2118 - mse: 0.0783
64/82 [======================>.......] - ETA: 0s - loss: 0.0729 - mae: 0.2079 - mse: 0.0729
82/82 [==============================] - 0s 5ms/step - loss: 0.0684 - mae: 0.2021 - mse: 0.0684 - val_loss: 0.0636 - val_mae: 0.1554 - val_mse: 0.0636
Epoch 8/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0470 - mae: 0.1704 - mse: 0.0470
64/82 [======================>.......] - ETA: 0s - loss: 0.0640 - mae: 0.1894 - mse: 0.0640
82/82 [==============================] - 0s 5ms/step - loss: 0.0604 - mae: 0.1859 - mse: 0.0604 - val_loss: 0.0625 - val_mae: 0.1462 - val_mse: 0.0625
Epoch 9/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0760 - mae: 0.2051 - mse: 0.0760
64/82 [======================>.......] - ETA: 0s - loss: 0.0666 - mae: 0.1899 - mse: 0.0666
82/82 [==============================] - 0s 5ms/step - loss: 0.0648 - mae: 0.1890 - mse: 0.0648 - val_loss: 0.0742 - val_mae: 0.1663 - val_mse: 0.0742
Epoch 10/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0319 - mae: 0.1272 - mse: 0.0319
64/82 [======================>.......] - ETA: 0s - loss: 0.0526 - mae: 0.1568 - mse: 0.0526
82/82 [==============================] - 0s 5ms/step - loss: 0.0543 - mae: 0.1587 - mse: 0.0543 - val_loss: 0.0859 - val_mae: 0.2026 - val_mse: 0.0859
Epoch 11/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0473 - mae: 0.1542 - mse: 0.0473
64/82 [======================>.......] - ETA: 0s - loss: 0.0350 - mae: 0.1410 - mse: 0.0350
82/82 [==============================] - 0s 5ms/step - loss: 0.0568 - mae: 0.1716 - mse: 0.0568 - val_loss: 0.0804 - val_mae: 0.1832 - val_mse: 0.0804
Epoch 12/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0819 - mae: 0.2090 - mse: 0.0819
64/82 [======================>.......] - ETA: 0s - loss: 0.0660 - mae: 0.1921 - mse: 0.0660
82/82 [==============================] - 0s 4ms/step - loss: 0.0568 - mae: 0.1785 - mse: 0.0568 - val_loss: 0.0709 - val_mae: 0.1503 - val_mse: 0.0709
Epoch 13/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0501 - mae: 0.1609 - mse: 0.0501
64/82 [======================>.......] - ETA: 0s - loss: 0.0592 - mae: 0.1733 - mse: 0.0592
82/82 [==============================] - 0s 5ms/step - loss: 0.0568 - mae: 0.1702 - mse: 0.0568 - val_loss: 0.0728 - val_mae: 0.1647 - val_mse: 0.0728
Epoch 14/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0343 - mae: 0.1492 - mse: 0.0343
64/82 [======================>.......] - ETA: 0s - loss: 0.0583 - mae: 0.1757 - mse: 0.0583
82/82 [==============================] - 0s 5ms/step - loss: 0.0592 - mae: 0.1740 - mse: 0.0592 - val_loss: 0.0725 - val_mae: 0.1695 - val_mse: 0.0725
Epoch 15/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0474 - mae: 0.1721 - mse: 0.0474
64/82 [======================>.......] - ETA: 0s - loss: 0.0622 - mae: 0.1807 - mse: 0.0622
82/82 [==============================] - 0s 5ms/step - loss: 0.0550 - mae: 0.1692 - mse: 0.0550 - val_loss: 0.0692 - val_mae: 0.1626 - val_mse: 0.0692
Epoch 16/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0501 - mae: 0.1487 - mse: 0.0501
64/82 [======================>.......] - ETA: 0s - loss: 0.0501 - mae: 0.1603 - mse: 0.0501
82/82 [==============================] - 0s 4ms/step - loss: 0.0500 - mae: 0.1612 - mse: 0.0500 - val_loss: 0.0748 - val_mae: 0.1856 - val_mse: 0.0748
Epoch 17/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0429 - mae: 0.1545 - mse: 0.0429
64/82 [======================>.......] - ETA: 0s - loss: 0.0622 - mae: 0.1776 - mse: 0.0622
82/82 [==============================] - 0s 5ms/step - loss: 0.0540 - mae: 0.1666 - mse: 0.0540 - val_loss: 0.0799 - val_mae: 0.2030 - val_mse: 0.0799
Epoch 18/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0527 - mae: 0.1755 - mse: 0.0527
64/82 [======================>.......] - ETA: 0s - loss: 0.0586 - mae: 0.1681 - mse: 0.0586
82/82 [==============================] - 0s 4ms/step - loss: 0.0499 - mae: 0.1552 - mse: 0.0499 - val_loss: 0.0749 - val_mae: 0.1871 - val_mse: 0.0749
Epoch 19/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0248 - mae: 0.1308 - mse: 0.0248
64/82 [======================>.......] - ETA: 0s - loss: 0.0386 - mae: 0.1447 - mse: 0.0386
82/82 [==============================] - 0s 5ms/step - loss: 0.0511 - mae: 0.1644 - mse: 0.0511 - val_loss: 0.0676 - val_mae: 0.1596 - val_mse: 0.0676
Epoch 20/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0647 - mae: 0.1809 - mse: 0.0647
64/82 [======================>.......] - ETA: 0s - loss: 0.0584 - mae: 0.1660 - mse: 0.0584
82/82 [==============================] - 0s 5ms/step - loss: 0.0517 - mae: 0.1595 - mse: 0.0517 - val_loss: 0.0737 - val_mae: 0.1845 - val_mse: 0.0737
Epoch 21/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0702 - mae: 0.1744 - mse: 0.0702
64/82 [======================>.......] - ETA: 0s - loss: 0.0578 - mae: 0.1646 - mse: 0.0578
82/82 [==============================] - 0s 5ms/step - loss: 0.0515 - mae: 0.1577 - mse: 0.0515 - val_loss: 0.0826 - val_mae: 0.2139 - val_mse: 0.0826
Epoch 22/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0483 - mae: 0.1501 - mse: 0.0483
64/82 [======================>.......] - ETA: 0s - loss: 0.0519 - mae: 0.1589 - mse: 0.0519
82/82 [==============================] - 0s 5ms/step - loss: 0.0548 - mae: 0.1661 - mse: 0.0548 - val_loss: 0.0760 - val_mae: 0.1935 - val_mse: 0.0760
Epoch 23/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0436 - mae: 0.1411 - mse: 0.0436
64/82 [======================>.......] - ETA: 0s - loss: 0.0546 - mae: 0.1611 - mse: 0.0546
82/82 [==============================] - 0s 5ms/step - loss: 0.0547 - mae: 0.1608 - mse: 0.0547 - val_loss: 0.0656 - val_mae: 0.1509 - val_mse: 0.0656
Epoch 24/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0694 - mae: 0.1745 - mse: 0.0694
64/82 [======================>.......] - ETA: 0s - loss: 0.0468 - mae: 0.1519 - mse: 0.0468
82/82 [==============================] - 0s 4ms/step - loss: 0.0477 - mae: 0.1535 - mse: 0.0477 - val_loss: 0.0731 - val_mae: 0.1830 - val_mse: 0.0731
Epoch 25/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0536 - mae: 0.1524 - mse: 0.0536
64/82 [======================>.......] - ETA: 0s - loss: 0.0354 - mae: 0.1259 - mse: 0.0354
82/82 [==============================] - 0s 4ms/step - loss: 0.0468 - mae: 0.1458 - mse: 0.0468 - val_loss: 0.0797 - val_mae: 0.2043 - val_mse: 0.0797
Epoch 26/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0701 - mae: 0.1701 - mse: 0.0701
64/82 [======================>.......] - ETA: 0s - loss: 0.0560 - mae: 0.1559 - mse: 0.0560
82/82 [==============================] - 0s 4ms/step - loss: 0.0473 - mae: 0.1432 - mse: 0.0473 - val_loss: 0.0761 - val_mae: 0.1951 - val_mse: 0.0761
Epoch 27/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0690 - mae: 0.1642 - mse: 0.0690
64/82 [======================>.......] - ETA: 0s - loss: 0.0551 - mae: 0.1555 - mse: 0.0551
82/82 [==============================] - 0s 5ms/step - loss: 0.0474 - mae: 0.1440 - mse: 0.0474 - val_loss: 0.0691 - val_mae: 0.1711 - val_mse: 0.0691
Epoch 28/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0370 - mae: 0.1371 - mse: 0.0370
64/82 [======================>.......] - ETA: 0s - loss: 0.0453 - mae: 0.1474 - mse: 0.0453
82/82 [==============================] - 0s 5ms/step - loss: 0.0469 - mae: 0.1512 - mse: 0.0469 - val_loss: 0.0806 - val_mae: 0.2069 - val_mse: 0.0806
Epoch 29/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0720 - mae: 0.1815 - mse: 0.0720
64/82 [======================>.......] - ETA: 0s - loss: 0.0577 - mae: 0.1622 - mse: 0.0577
82/82 [==============================] - 0s 5ms/step - loss: 0.0501 - mae: 0.1534 - mse: 0.0501 - val_loss: 0.0804 - val_mae: 0.2056 - val_mse: 0.0804
Epoch 30/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0631 - mae: 0.1769 - mse: 0.0631
64/82 [======================>.......] - ETA: 0s - loss: 0.0578 - mae: 0.1665 - mse: 0.0578
82/82 [==============================] - 0s 4ms/step - loss: 0.0503 - mae: 0.1559 - mse: 0.0503 - val_loss: 0.0669 - val_mae: 0.1584 - val_mse: 0.0669
Saving trained model...
105
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         21.31762695]
average prediction= [2.3179712]
baseline= 7.108695652173913
eachuser= [0. 0. 0. 0. 0. 5.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 4.263525390625
['train-height-2.py', '0']
2_155_65_2_csi_a2_20.dat
155 2
2_155_65_2_csi_a2_23.dat
155 4
2_155_65_2_csi_a2_16.dat
155 6
155 7
155 8
155 9
2_155_65_2_csi_a2_1.dat
2_155_65_2_csi_a2_21.dat
155 12
155 13
155 14
2_155_65_2_csi_a2_25.dat
2_155_65_2_csi_a2_18.dat
2_155_65_2_csi_a2_17.dat
2_155_65_2_csi_a2_14.dat
155 19
155 20
155 21
155 22
155 23
155 24
2_155_65_2_csi_a2_27.dat
2_155_65_2_csi_a2_24.dat
155 27
155 28
2_155_65_2_csi_a2_7.dat
155 30
170 31
170 32
170 33
2_170_60_2_csi_a2_2.dat
170 35
170 36
170 37
170 38
2_170_60_2_csi_a2_10.dat
170 40
1_165_65_2_csi_a2_30.dat
1_165_65_2_csi_a2_15.dat
1_165_65_2_csi_a2_10.dat
1_165_65_2_csi_a2_21.dat
1_165_65_2_csi_a2_9.dat
1_165_65_2_csi_a2_13.dat
1_165_65_2_csi_a2_19.dat
1_165_65_2_csi_a2_18.dat
1_165_65_2_csi_a2_7.dat
1_165_65_2_csi_a2_20.dat
1_165_65_2_csi_a2_11.dat
1_165_65_2_csi_a2_26.dat
1_165_65_2_csi_a2_16.dat
1_165_65_2_csi_a2_28.dat
1_165_65_2_csi_a2_5.dat
1_165_65_2_csi_a2_6.dat
1_165_65_2_csi_a2_25.dat
1_165_65_2_csi_a2_29.dat
1_165_65_2_csi_a2_23.dat
1_165_65_2_csi_a2_3.dat
1_165_65_2_csi_a2_27.dat
1_165_65_2_csi_a2_2.dat
1_165_65_2_csi_a2_17.dat
1_165_65_2_csi_a2_8.dat
1_165_65_2_csi_a2_14.dat
1_165_65_2_csi_a2_1.dat
1_165_65_2_csi_a2_24.dat
1_165_65_2_csi_a2_4.dat
1_165_65_2_csi_a2_22.dat
1_165_65_2_csi_a2_12.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
2_165_50_2_csi_a2_13.dat
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
2_165_50_2_csi_a2_24.dat
165 100
1_175_70_2_csi_a2_25.dat
175 102
1_175_70_2_csi_a2_20.dat
1_175_70_2_csi_a2_28.dat
175 105
1_175_70_2_csi_a2_22.dat
1_175_70_2_csi_a2_5.dat
1_175_70_2_csi_a2_11.dat
1_175_70_2_csi_a2_6.dat
1_175_70_2_csi_a2_8.dat
1_175_70_2_csi_a2_3.dat
1_175_70_2_csi_a2_16.dat
1_175_70_2_csi_a2_19.dat
1_175_70_2_csi_a2_1.dat
1_175_70_2_csi_a2_27.dat
175 116
1_175_70_2_csi_a2_18.dat
1_175_70_2_csi_a2_23.dat
1_175_70_2_csi_a2_21.dat
1_175_70_2_csi_a2_12.dat
1_175_70_2_csi_a2_30.dat
1_175_70_2_csi_a2_15.dat
1_175_70_2_csi_a2_17.dat
1_175_70_2_csi_a2_14.dat
1_175_70_2_csi_a2_9.dat
1_175_70_2_csi_a2_24.dat
1_175_70_2_csi_a2_10.dat
1_175_70_2_csi_a2_4.dat
1_175_70_2_csi_a2_29.dat
1_175_70_2_csi_a2_13.dat
1_180_85_2_csi_a2_22.dat
1_180_85_2_csi_a2_17.dat
1_180_85_2_csi_a2_15.dat
1_180_85_2_csi_a2_21.dat
180 135
1_180_85_2_csi_a2_7.dat
1_180_85_2_csi_a2_25.dat
1_180_85_2_csi_a2_19.dat
180 139
1_180_85_2_csi_a2_6.dat
1_180_85_2_csi_a2_1.dat
1_180_85_2_csi_a2_16.dat
1_180_85_2_csi_a2_14.dat
1_180_85_2_csi_a2_23.dat
1_180_85_2_csi_a2_24.dat
1_180_85_2_csi_a2_8.dat
180 147
1_180_85_2_csi_a2_13.dat
1_180_85_2_csi_a2_29.dat
1_180_85_2_csi_a2_9.dat
1_180_85_2_csi_a2_18.dat
1_180_85_2_csi_a2_28.dat
1_180_85_2_csi_a2_12.dat
1_180_85_2_csi_a2_30.dat
1_180_85_2_csi_a2_20.dat
180 156
1_180_85_2_csi_a2_26.dat
1_180_85_2_csi_a2_11.dat
180 159
1_180_85_2_csi_a2_10.dat
180 161
180 162
180 163
180 164
180 165
180 166
1_180_75_2_csi_a2_9.dat
180 168
180 169
180 170
180 171
180 172
180 173
180 174
180 175
180 176
180 177
180 178
180 179
180 180
180 181
1_180_75_2_csi_a2_1.dat
180 183
180 184
180 185
1_180_75_2_csi_a2_4.dat
180 187
180 188
180 189
180 190
173 191
173 192
173 193
173 194
173 195
173 196
173 197
173 198
173 199
1_173_85_2_csi_a2_23.dat
173 201
173 202
173 203
173 204
173 205
173 206
173 207
1_173_85_2_csi_a2_13.dat
173 209
173 210
173 211
173 212
173 213
173 214
173 215
173 216
1_173_85_2_csi_a2_28.dat
173 218
1_173_85_2_csi_a2_4.dat
173 220
(115, 30, 3)
(115, 421, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155
 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 173
 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173
 173 173 173 173 173 173 173]
(115, 421, 30, 3, 1)

Loaded dataset of 115 samples, each sized (421, 30, 3, 1)


Train on 92 samples
Test on 23 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 421, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 421, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 421, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 421, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 421, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 421, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 421, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 82 samples, validate on 10 samples
Epoch 1/30

32/82 [==========>...................] - ETA: 0s - loss: 0.4468 - mae: 0.6083 - mse: 0.4468
64/82 [======================>.......] - ETA: 0s - loss: 0.4087 - mae: 0.5706 - mse: 0.4087
82/82 [==============================] - 1s 10ms/step - loss: 0.4193 - mae: 0.5818 - mse: 0.4193 - val_loss: 0.2638 - val_mae: 0.4801 - val_mse: 0.2638
Epoch 2/30

32/82 [==========>...................] - ETA: 0s - loss: 0.2293 - mae: 0.4262 - mse: 0.2293
64/82 [======================>.......] - ETA: 0s - loss: 0.2043 - mae: 0.4093 - mse: 0.2043
82/82 [==============================] - 0s 5ms/step - loss: 0.1953 - mae: 0.3973 - mse: 0.1953 - val_loss: 0.1796 - val_mae: 0.3803 - val_mse: 0.1796
Epoch 3/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1001 - mae: 0.2282 - mse: 0.1001
64/82 [======================>.......] - ETA: 0s - loss: 0.1193 - mae: 0.2471 - mse: 0.1193
82/82 [==============================] - 0s 5ms/step - loss: 0.1604 - mae: 0.2783 - mse: 0.1604 - val_loss: 0.1940 - val_mae: 0.3366 - val_mse: 0.1940
Epoch 4/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0990 - mae: 0.2314 - mse: 0.0990
64/82 [======================>.......] - ETA: 0s - loss: 0.1164 - mae: 0.2509 - mse: 0.1164
82/82 [==============================] - 0s 6ms/step - loss: 0.1332 - mae: 0.2607 - mse: 0.1332 - val_loss: 0.1480 - val_mae: 0.3266 - val_mse: 0.1480
Epoch 5/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1000 - mae: 0.2033 - mse: 0.1000
64/82 [======================>.......] - ETA: 0s - loss: 0.0983 - mae: 0.2161 - mse: 0.0983
82/82 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.2144 - mse: 0.0926 - val_loss: 0.1364 - val_mae: 0.3364 - val_mse: 0.1364
Epoch 6/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0795 - mae: 0.2313 - mse: 0.0795
64/82 [======================>.......] - ETA: 0s - loss: 0.0784 - mae: 0.2273 - mse: 0.0784
82/82 [==============================] - 0s 5ms/step - loss: 0.0735 - mae: 0.2191 - mse: 0.0735 - val_loss: 0.1271 - val_mae: 0.3226 - val_mse: 0.1271
Epoch 7/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0762 - mae: 0.2308 - mse: 0.0762
64/82 [======================>.......] - ETA: 0s - loss: 0.0832 - mae: 0.2360 - mse: 0.0832
82/82 [==============================] - 0s 5ms/step - loss: 0.0821 - mae: 0.2398 - mse: 0.0821 - val_loss: 0.1095 - val_mae: 0.2772 - val_mse: 0.1095
Epoch 8/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0322 - mae: 0.1471 - mse: 0.0322
64/82 [======================>.......] - ETA: 0s - loss: 0.0501 - mae: 0.1766 - mse: 0.0501
82/82 [==============================] - 0s 5ms/step - loss: 0.0549 - mae: 0.1844 - mse: 0.0549 - val_loss: 0.0984 - val_mae: 0.2216 - val_mse: 0.0984
Epoch 9/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0595 - mae: 0.1774 - mse: 0.0595
64/82 [======================>.......] - ETA: 0s - loss: 0.0537 - mae: 0.1656 - mse: 0.0537
82/82 [==============================] - 0s 5ms/step - loss: 0.0541 - mae: 0.1650 - mse: 0.0541 - val_loss: 0.0955 - val_mae: 0.2051 - val_mse: 0.0955
Epoch 10/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0538 - mae: 0.1688 - mse: 0.0538
64/82 [======================>.......] - ETA: 0s - loss: 0.0439 - mae: 0.1612 - mse: 0.0439
82/82 [==============================] - 0s 5ms/step - loss: 0.0462 - mae: 0.1630 - mse: 0.0462 - val_loss: 0.0895 - val_mae: 0.1878 - val_mse: 0.0895
Epoch 11/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0438 - mae: 0.1443 - mse: 0.0438
64/82 [======================>.......] - ETA: 0s - loss: 0.0380 - mae: 0.1443 - mse: 0.0380
82/82 [==============================] - 0s 5ms/step - loss: 0.0367 - mae: 0.1449 - mse: 0.0367 - val_loss: 0.0907 - val_mae: 0.1908 - val_mse: 0.0907
Epoch 12/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0390 - mae: 0.1383 - mse: 0.0390
64/82 [======================>.......] - ETA: 0s - loss: 0.0341 - mae: 0.1427 - mse: 0.0341
82/82 [==============================] - 0s 5ms/step - loss: 0.0342 - mae: 0.1470 - mse: 0.0342 - val_loss: 0.0890 - val_mae: 0.1679 - val_mse: 0.0890
Epoch 13/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0212 - mae: 0.1146 - mse: 0.0212
64/82 [======================>.......] - ETA: 0s - loss: 0.0189 - mae: 0.1091 - mse: 0.0189
82/82 [==============================] - 0s 5ms/step - loss: 0.0260 - mae: 0.1178 - mse: 0.0260 - val_loss: 0.0881 - val_mae: 0.1437 - val_mse: 0.0881
Epoch 14/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0216 - mae: 0.1166 - mse: 0.0216
64/82 [======================>.......] - ETA: 0s - loss: 0.0347 - mae: 0.1355 - mse: 0.0347
82/82 [==============================] - 0s 5ms/step - loss: 0.0327 - mae: 0.1346 - mse: 0.0327 - val_loss: 0.0888 - val_mae: 0.1344 - val_mse: 0.0888
Epoch 15/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0219 - mae: 0.1133 - mse: 0.0219
64/82 [======================>.......] - ETA: 0s - loss: 0.0299 - mae: 0.1170 - mse: 0.0299
82/82 [==============================] - 0s 5ms/step - loss: 0.0287 - mae: 0.1191 - mse: 0.0287 - val_loss: 0.0925 - val_mae: 0.1504 - val_mse: 0.0925
Epoch 16/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0403 - mae: 0.1248 - mse: 0.0403
64/82 [======================>.......] - ETA: 0s - loss: 0.0270 - mae: 0.1138 - mse: 0.0270
82/82 [==============================] - 0s 5ms/step - loss: 0.0232 - mae: 0.1075 - mse: 0.0232 - val_loss: 0.0966 - val_mae: 0.1599 - val_mse: 0.0966
Epoch 17/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0174 - mae: 0.1036 - mse: 0.0174
64/82 [======================>.......] - ETA: 0s - loss: 0.0262 - mae: 0.1050 - mse: 0.0262
82/82 [==============================] - 0s 5ms/step - loss: 0.0246 - mae: 0.1045 - mse: 0.0246 - val_loss: 0.0951 - val_mae: 0.1463 - val_mse: 0.0951
Epoch 18/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0163 - mae: 0.1091 - mse: 0.0163
64/82 [======================>.......] - ETA: 0s - loss: 0.0311 - mae: 0.1174 - mse: 0.0311
82/82 [==============================] - 0s 5ms/step - loss: 0.0304 - mae: 0.1221 - mse: 0.0304 - val_loss: 0.0931 - val_mae: 0.1287 - val_mse: 0.0931
Epoch 19/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0551 - mae: 0.1395 - mse: 0.0551
64/82 [======================>.......] - ETA: 0s - loss: 0.0340 - mae: 0.1180 - mse: 0.0340
82/82 [==============================] - 0s 5ms/step - loss: 0.0313 - mae: 0.1172 - mse: 0.0313 - val_loss: 0.0919 - val_mae: 0.1401 - val_mse: 0.0919
Epoch 20/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0447 - mae: 0.1295 - mse: 0.0447
64/82 [======================>.......] - ETA: 0s - loss: 0.0292 - mae: 0.1105 - mse: 0.0292
82/82 [==============================] - 0s 5ms/step - loss: 0.0262 - mae: 0.1094 - mse: 0.0262 - val_loss: 0.0938 - val_mae: 0.1548 - val_mse: 0.0938
Epoch 21/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0207 - mae: 0.0989 - mse: 0.0207
64/82 [======================>.......] - ETA: 0s - loss: 0.0296 - mae: 0.1095 - mse: 0.0296
82/82 [==============================] - 0s 5ms/step - loss: 0.0280 - mae: 0.1123 - mse: 0.0280 - val_loss: 0.0949 - val_mae: 0.1598 - val_mse: 0.0949
Epoch 22/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0373 - mae: 0.1162 - mse: 0.0373
64/82 [======================>.......] - ETA: 0s - loss: 0.0253 - mae: 0.1055 - mse: 0.0253
82/82 [==============================] - 0s 5ms/step - loss: 0.0236 - mae: 0.1056 - mse: 0.0236 - val_loss: 0.0898 - val_mae: 0.1344 - val_mse: 0.0898
Epoch 23/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0247 - mae: 0.1189 - mse: 0.0247
64/82 [======================>.......] - ETA: 0s - loss: 0.0295 - mae: 0.1136 - mse: 0.0295
82/82 [==============================] - 0s 5ms/step - loss: 0.0299 - mae: 0.1179 - mse: 0.0299 - val_loss: 0.0904 - val_mae: 0.1359 - val_mse: 0.0904
Epoch 24/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0089 - mae: 0.0809 - mse: 0.0089
64/82 [======================>.......] - ETA: 0s - loss: 0.0231 - mae: 0.1034 - mse: 0.0231
82/82 [==============================] - 0s 4ms/step - loss: 0.0212 - mae: 0.1037 - mse: 0.0212 - val_loss: 0.0991 - val_mae: 0.1750 - val_mse: 0.0991
Epoch 25/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0156 - mae: 0.0974 - mse: 0.0156
64/82 [======================>.......] - ETA: 0s - loss: 0.0314 - mae: 0.1172 - mse: 0.0314
82/82 [==============================] - 0s 5ms/step - loss: 0.0274 - mae: 0.1134 - mse: 0.0274 - val_loss: 0.0985 - val_mae: 0.1737 - val_mse: 0.0985
Epoch 26/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0143 - mae: 0.0896 - mse: 0.0143
64/82 [======================>.......] - ETA: 0s - loss: 0.0239 - mae: 0.1006 - mse: 0.0239
82/82 [==============================] - 0s 5ms/step - loss: 0.0198 - mae: 0.0914 - mse: 0.0198 - val_loss: 0.0902 - val_mae: 0.1403 - val_mse: 0.0902
Epoch 27/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0148 - mae: 0.0943 - mse: 0.0148
64/82 [======================>.......] - ETA: 0s - loss: 0.0125 - mae: 0.0875 - mse: 0.0125
82/82 [==============================] - 0s 5ms/step - loss: 0.0216 - mae: 0.0984 - mse: 0.0216 - val_loss: 0.0880 - val_mae: 0.1287 - val_mse: 0.0880
Epoch 28/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0176 - mae: 0.1031 - mse: 0.0176
64/82 [======================>.......] - ETA: 0s - loss: 0.0163 - mae: 0.0988 - mse: 0.0163
82/82 [==============================] - 0s 5ms/step - loss: 0.0264 - mae: 0.1155 - mse: 0.0264 - val_loss: 0.0863 - val_mae: 0.1290 - val_mse: 0.0863
Epoch 29/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0147 - mae: 0.0935 - mse: 0.0147
64/82 [======================>.......] - ETA: 0s - loss: 0.0259 - mae: 0.1028 - mse: 0.0259
82/82 [==============================] - 0s 5ms/step - loss: 0.0229 - mae: 0.1022 - mse: 0.0229 - val_loss: 0.0887 - val_mae: 0.1563 - val_mse: 0.0887
Epoch 30/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0409 - mae: 0.1363 - mse: 0.0409
64/82 [======================>.......] - ETA: 0s - loss: 0.0261 - mae: 0.1119 - mse: 0.0261
82/82 [==============================] - 0s 5ms/step - loss: 0.0236 - mae: 0.1093 - mse: 0.0236 - val_loss: 0.0888 - val_mae: 0.1641 - val_mse: 0.0888
Saving trained model...
105
Testing...
heightdiff= [0.         0.         0.         0.         0.         4.96347046]
average prediction= [5.7078824]
baseline= 7.543478260869565
eachuser= [0. 0. 0. 0. 0. 3.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 1.6544901529947917
['train-height-2.py', '0']
2_155_65_2_csi_a2_20.dat
155 2
2_155_65_2_csi_a2_23.dat
155 4
2_155_65_2_csi_a2_16.dat
155 6
155 7
155 8
155 9
2_155_65_2_csi_a2_1.dat
2_155_65_2_csi_a2_21.dat
155 12
155 13
155 14
2_155_65_2_csi_a2_25.dat
2_155_65_2_csi_a2_18.dat
2_155_65_2_csi_a2_17.dat
2_155_65_2_csi_a2_14.dat
155 19
155 20
155 21
155 22
155 23
155 24
2_155_65_2_csi_a2_27.dat
2_155_65_2_csi_a2_24.dat
155 27
155 28
2_155_65_2_csi_a2_7.dat
155 30
170 31
170 32
170 33
2_170_60_2_csi_a2_2.dat
170 35
170 36
170 37
170 38
2_170_60_2_csi_a2_10.dat
170 40
1_165_65_2_csi_a2_30.dat
1_165_65_2_csi_a2_15.dat
1_165_65_2_csi_a2_10.dat
1_165_65_2_csi_a2_21.dat
1_165_65_2_csi_a2_9.dat
1_165_65_2_csi_a2_13.dat
1_165_65_2_csi_a2_19.dat
1_165_65_2_csi_a2_18.dat
1_165_65_2_csi_a2_7.dat
1_165_65_2_csi_a2_20.dat
1_165_65_2_csi_a2_11.dat
1_165_65_2_csi_a2_26.dat
1_165_65_2_csi_a2_16.dat
1_165_65_2_csi_a2_28.dat
1_165_65_2_csi_a2_5.dat
1_165_65_2_csi_a2_6.dat
1_165_65_2_csi_a2_25.dat
1_165_65_2_csi_a2_29.dat
1_165_65_2_csi_a2_23.dat
1_165_65_2_csi_a2_3.dat
1_165_65_2_csi_a2_27.dat
1_165_65_2_csi_a2_2.dat
1_165_65_2_csi_a2_17.dat
1_165_65_2_csi_a2_8.dat
1_165_65_2_csi_a2_14.dat
1_165_65_2_csi_a2_1.dat
1_165_65_2_csi_a2_24.dat
1_165_65_2_csi_a2_4.dat
1_165_65_2_csi_a2_22.dat
1_165_65_2_csi_a2_12.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
2_165_50_2_csi_a2_13.dat
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
2_165_50_2_csi_a2_24.dat
165 100
1_175_70_2_csi_a2_25.dat
175 102
1_175_70_2_csi_a2_20.dat
1_175_70_2_csi_a2_28.dat
175 105
1_175_70_2_csi_a2_22.dat
1_175_70_2_csi_a2_5.dat
1_175_70_2_csi_a2_11.dat
1_175_70_2_csi_a2_6.dat
1_175_70_2_csi_a2_8.dat
1_175_70_2_csi_a2_3.dat
1_175_70_2_csi_a2_16.dat
1_175_70_2_csi_a2_19.dat
1_175_70_2_csi_a2_1.dat
1_175_70_2_csi_a2_27.dat
175 116
1_175_70_2_csi_a2_18.dat
1_175_70_2_csi_a2_23.dat
1_175_70_2_csi_a2_21.dat
1_175_70_2_csi_a2_12.dat
1_175_70_2_csi_a2_30.dat
1_175_70_2_csi_a2_15.dat
1_175_70_2_csi_a2_17.dat
1_175_70_2_csi_a2_14.dat
1_175_70_2_csi_a2_9.dat
1_175_70_2_csi_a2_24.dat
1_175_70_2_csi_a2_10.dat
1_175_70_2_csi_a2_4.dat
1_175_70_2_csi_a2_29.dat
1_175_70_2_csi_a2_13.dat
1_180_85_2_csi_a2_22.dat
1_180_85_2_csi_a2_17.dat
1_180_85_2_csi_a2_15.dat
1_180_85_2_csi_a2_21.dat
180 135
1_180_85_2_csi_a2_7.dat
1_180_85_2_csi_a2_25.dat
1_180_85_2_csi_a2_19.dat
180 139
1_180_85_2_csi_a2_6.dat
1_180_85_2_csi_a2_1.dat
1_180_85_2_csi_a2_16.dat
1_180_85_2_csi_a2_14.dat
1_180_85_2_csi_a2_23.dat
1_180_85_2_csi_a2_24.dat
1_180_85_2_csi_a2_8.dat
180 147
1_180_85_2_csi_a2_13.dat
1_180_85_2_csi_a2_29.dat
1_180_85_2_csi_a2_9.dat
1_180_85_2_csi_a2_18.dat
1_180_85_2_csi_a2_28.dat
1_180_85_2_csi_a2_12.dat
1_180_85_2_csi_a2_30.dat
1_180_85_2_csi_a2_20.dat
180 156
1_180_85_2_csi_a2_26.dat
1_180_85_2_csi_a2_11.dat
180 159
1_180_85_2_csi_a2_10.dat
180 161
180 162
180 163
180 164
180 165
180 166
1_180_75_2_csi_a2_9.dat
180 168
180 169
180 170
180 171
180 172
180 173
180 174
180 175
180 176
180 177
180 178
180 179
180 180
180 181
1_180_75_2_csi_a2_1.dat
180 183
180 184
180 185
1_180_75_2_csi_a2_4.dat
180 187
180 188
180 189
180 190
173 191
173 192
173 193
173 194
173 195
173 196
173 197
173 198
173 199
1_173_85_2_csi_a2_23.dat
173 201
173 202
173 203
173 204
173 205
173 206
173 207
1_173_85_2_csi_a2_13.dat
173 209
173 210
173 211
173 212
173 213
173 214
173 215
173 216
1_173_85_2_csi_a2_28.dat
173 218
1_173_85_2_csi_a2_4.dat
173 220
(115, 30, 3)
(115, 421, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155
 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 173
 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173
 173 173 173 173 173 173 173]
(115, 421, 30, 3, 1)

Loaded dataset of 115 samples, each sized (421, 30, 3, 1)


Train on 92 samples
Test on 23 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 421, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 421, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 421, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 421, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 421, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 421, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 421, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 82 samples, validate on 10 samples
Epoch 1/30

32/82 [==========>...................] - ETA: 0s - loss: 0.5942 - mae: 0.7279 - mse: 0.5942
64/82 [======================>.......] - ETA: 0s - loss: 0.4818 - mae: 0.6174 - mse: 0.4818
82/82 [==============================] - 1s 11ms/step - loss: 0.4540 - mae: 0.5974 - mse: 0.4540 - val_loss: 0.2862 - val_mae: 0.4694 - val_mse: 0.2862
Epoch 2/30

32/82 [==========>...................] - ETA: 0s - loss: 0.3427 - mae: 0.4861 - mse: 0.3427
64/82 [======================>.......] - ETA: 0s - loss: 0.2723 - mae: 0.4422 - mse: 0.2723
82/82 [==============================] - 0s 5ms/step - loss: 0.2705 - mae: 0.4506 - mse: 0.2705 - val_loss: 0.1836 - val_mae: 0.3977 - val_mse: 0.1836
Epoch 3/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1919 - mae: 0.4018 - mse: 0.1919
64/82 [======================>.......] - ETA: 0s - loss: 0.2132 - mae: 0.4241 - mse: 0.2132
82/82 [==============================] - 0s 5ms/step - loss: 0.2137 - mae: 0.4192 - mse: 0.2137 - val_loss: 0.1408 - val_mae: 0.3370 - val_mse: 0.1408
Epoch 4/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1734 - mae: 0.3775 - mse: 0.1734
64/82 [======================>.......] - ETA: 0s - loss: 0.1535 - mae: 0.3508 - mse: 0.1535
82/82 [==============================] - 0s 5ms/step - loss: 0.1492 - mae: 0.3412 - mse: 0.1492 - val_loss: 0.1058 - val_mae: 0.2803 - val_mse: 0.1058
Epoch 5/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0995 - mae: 0.2801 - mse: 0.0995
64/82 [======================>.......] - ETA: 0s - loss: 0.0994 - mae: 0.2749 - mse: 0.0994
82/82 [==============================] - 0s 5ms/step - loss: 0.1050 - mae: 0.2844 - mse: 0.1050 - val_loss: 0.0878 - val_mae: 0.2506 - val_mse: 0.0878
Epoch 6/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0776 - mae: 0.2329 - mse: 0.0776
64/82 [======================>.......] - ETA: 0s - loss: 0.0690 - mae: 0.2110 - mse: 0.0690
82/82 [==============================] - 0s 5ms/step - loss: 0.0687 - mae: 0.2087 - mse: 0.0687 - val_loss: 0.0694 - val_mae: 0.2222 - val_mse: 0.0694
Epoch 7/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0643 - mae: 0.1921 - mse: 0.0643
64/82 [======================>.......] - ETA: 0s - loss: 0.0686 - mae: 0.1888 - mse: 0.0686
82/82 [==============================] - 0s 5ms/step - loss: 0.0663 - mae: 0.1890 - mse: 0.0663 - val_loss: 0.0528 - val_mae: 0.1638 - val_mse: 0.0528
Epoch 8/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0706 - mae: 0.2076 - mse: 0.0706
64/82 [======================>.......] - ETA: 0s - loss: 0.0588 - mae: 0.1827 - mse: 0.0588
82/82 [==============================] - 0s 5ms/step - loss: 0.0617 - mae: 0.1894 - mse: 0.0617 - val_loss: 0.0455 - val_mae: 0.1548 - val_mse: 0.0455
Epoch 9/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0381 - mae: 0.1467 - mse: 0.0381
64/82 [======================>.......] - ETA: 0s - loss: 0.0636 - mae: 0.1762 - mse: 0.0636
82/82 [==============================] - 0s 4ms/step - loss: 0.0573 - mae: 0.1703 - mse: 0.0573 - val_loss: 0.0354 - val_mae: 0.1446 - val_mse: 0.0354
Epoch 10/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0298 - mae: 0.1390 - mse: 0.0298
64/82 [======================>.......] - ETA: 0s - loss: 0.0552 - mae: 0.1739 - mse: 0.0552
82/82 [==============================] - 0s 5ms/step - loss: 0.0512 - mae: 0.1718 - mse: 0.0512 - val_loss: 0.0336 - val_mae: 0.1452 - val_mse: 0.0336
Epoch 11/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0661 - mae: 0.2111 - mse: 0.0661
64/82 [======================>.......] - ETA: 0s - loss: 0.0682 - mae: 0.2120 - mse: 0.0682
82/82 [==============================] - 0s 5ms/step - loss: 0.0588 - mae: 0.1956 - mse: 0.0588 - val_loss: 0.0285 - val_mae: 0.1372 - val_mse: 0.0285
Epoch 12/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0581 - mae: 0.1965 - mse: 0.0581
64/82 [======================>.......] - ETA: 0s - loss: 0.0605 - mae: 0.1872 - mse: 0.0605
82/82 [==============================] - 0s 5ms/step - loss: 0.0575 - mae: 0.1857 - mse: 0.0575 - val_loss: 0.0267 - val_mae: 0.1361 - val_mse: 0.0267
Epoch 13/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0463 - mae: 0.1585 - mse: 0.0463
64/82 [======================>.......] - ETA: 0s - loss: 0.0520 - mae: 0.1695 - mse: 0.0520
82/82 [==============================] - 0s 5ms/step - loss: 0.0506 - mae: 0.1689 - mse: 0.0506 - val_loss: 0.0234 - val_mae: 0.1271 - val_mse: 0.0234
Epoch 14/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0608 - mae: 0.1793 - mse: 0.0608
64/82 [======================>.......] - ETA: 0s - loss: 0.0540 - mae: 0.1707 - mse: 0.0540
82/82 [==============================] - 0s 5ms/step - loss: 0.0461 - mae: 0.1535 - mse: 0.0461 - val_loss: 0.0231 - val_mae: 0.1183 - val_mse: 0.0231
Epoch 15/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0231 - mae: 0.1158 - mse: 0.0231
64/82 [======================>.......] - ETA: 0s - loss: 0.0381 - mae: 0.1381 - mse: 0.0381
82/82 [==============================] - 0s 5ms/step - loss: 0.0411 - mae: 0.1455 - mse: 0.0411 - val_loss: 0.0267 - val_mae: 0.1402 - val_mse: 0.0267
Epoch 16/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0377 - mae: 0.1372 - mse: 0.0377
64/82 [======================>.......] - ETA: 0s - loss: 0.0424 - mae: 0.1476 - mse: 0.0424
82/82 [==============================] - 0s 5ms/step - loss: 0.0467 - mae: 0.1549 - mse: 0.0467 - val_loss: 0.0208 - val_mae: 0.1117 - val_mse: 0.0208
Epoch 17/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0350 - mae: 0.1338 - mse: 0.0350
64/82 [======================>.......] - ETA: 0s - loss: 0.0373 - mae: 0.1371 - mse: 0.0373
82/82 [==============================] - 0s 5ms/step - loss: 0.0448 - mae: 0.1512 - mse: 0.0448 - val_loss: 0.0199 - val_mae: 0.1161 - val_mse: 0.0199
Epoch 18/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0339 - mae: 0.1260 - mse: 0.0339
64/82 [======================>.......] - ETA: 0s - loss: 0.0401 - mae: 0.1433 - mse: 0.0401
82/82 [==============================] - 0s 5ms/step - loss: 0.0429 - mae: 0.1481 - mse: 0.0429 - val_loss: 0.0191 - val_mae: 0.1103 - val_mse: 0.0191
Epoch 19/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0406 - mae: 0.1195 - mse: 0.0406
64/82 [======================>.......] - ETA: 0s - loss: 0.0338 - mae: 0.1221 - mse: 0.0338
82/82 [==============================] - 0s 4ms/step - loss: 0.0357 - mae: 0.1244 - mse: 0.0357 - val_loss: 0.0246 - val_mae: 0.1340 - val_mse: 0.0246
Epoch 20/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0508 - mae: 0.1585 - mse: 0.0508
64/82 [======================>.......] - ETA: 0s - loss: 0.0449 - mae: 0.1503 - mse: 0.0449
82/82 [==============================] - 0s 5ms/step - loss: 0.0406 - mae: 0.1416 - mse: 0.0406 - val_loss: 0.0212 - val_mae: 0.1160 - val_mse: 0.0212
Epoch 21/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0504 - mae: 0.1491 - mse: 0.0504
64/82 [======================>.......] - ETA: 0s - loss: 0.0342 - mae: 0.1252 - mse: 0.0342
82/82 [==============================] - 0s 5ms/step - loss: 0.0389 - mae: 0.1331 - mse: 0.0389 - val_loss: 0.0194 - val_mae: 0.1099 - val_mse: 0.0194
Epoch 22/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0227 - mae: 0.1189 - mse: 0.0227
64/82 [======================>.......] - ETA: 0s - loss: 0.0395 - mae: 0.1407 - mse: 0.0395
82/82 [==============================] - 0s 4ms/step - loss: 0.0408 - mae: 0.1408 - mse: 0.0408 - val_loss: 0.0194 - val_mae: 0.1099 - val_mse: 0.0194
Epoch 23/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0542 - mae: 0.1480 - mse: 0.0542
64/82 [======================>.......] - ETA: 0s - loss: 0.0395 - mae: 0.1376 - mse: 0.0395
82/82 [==============================] - 0s 5ms/step - loss: 0.0381 - mae: 0.1346 - mse: 0.0381 - val_loss: 0.0185 - val_mae: 0.1065 - val_mse: 0.0185
Epoch 24/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0517 - mae: 0.1456 - mse: 0.0517
64/82 [======================>.......] - ETA: 0s - loss: 0.0335 - mae: 0.1244 - mse: 0.0335
82/82 [==============================] - 0s 5ms/step - loss: 0.0336 - mae: 0.1225 - mse: 0.0336 - val_loss: 0.0166 - val_mae: 0.1008 - val_mse: 0.0166
Epoch 25/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0315 - mae: 0.1276 - mse: 0.0315
64/82 [======================>.......] - ETA: 0s - loss: 0.0342 - mae: 0.1298 - mse: 0.0342
82/82 [==============================] - 0s 5ms/step - loss: 0.0423 - mae: 0.1368 - mse: 0.0423 - val_loss: 0.0186 - val_mae: 0.1067 - val_mse: 0.0186
Epoch 26/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0278 - mae: 0.1130 - mse: 0.0278
64/82 [======================>.......] - ETA: 0s - loss: 0.0338 - mae: 0.1220 - mse: 0.0338
82/82 [==============================] - 0s 4ms/step - loss: 0.0378 - mae: 0.1307 - mse: 0.0378 - val_loss: 0.0257 - val_mae: 0.1466 - val_mse: 0.0257
Epoch 27/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0571 - mae: 0.1531 - mse: 0.0571
64/82 [======================>.......] - ETA: 0s - loss: 0.0513 - mae: 0.1546 - mse: 0.0513
82/82 [==============================] - 0s 4ms/step - loss: 0.0446 - mae: 0.1446 - mse: 0.0446 - val_loss: 0.0220 - val_mae: 0.1301 - val_mse: 0.0220
Epoch 28/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0332 - mae: 0.1233 - mse: 0.0332
64/82 [======================>.......] - ETA: 0s - loss: 0.0407 - mae: 0.1355 - mse: 0.0407
82/82 [==============================] - 0s 5ms/step - loss: 0.0370 - mae: 0.1323 - mse: 0.0370 - val_loss: 0.0161 - val_mae: 0.0935 - val_mse: 0.0161
Epoch 29/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0226 - mae: 0.0986 - mse: 0.0226
64/82 [======================>.......] - ETA: 0s - loss: 0.0402 - mae: 0.1407 - mse: 0.0402
82/82 [==============================] - 0s 4ms/step - loss: 0.0380 - mae: 0.1375 - mse: 0.0380 - val_loss: 0.0205 - val_mae: 0.1253 - val_mse: 0.0205
Epoch 30/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0436 - mae: 0.1402 - mse: 0.0436
64/82 [======================>.......] - ETA: 0s - loss: 0.0370 - mae: 0.1257 - mse: 0.0370
82/82 [==============================] - 0s 5ms/step - loss: 0.0394 - mae: 0.1294 - mse: 0.0394 - val_loss: 0.0216 - val_mae: 0.1347 - val_mse: 0.0216
Saving trained model...
105
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         40.88401794]
average prediction= [5.031668]
baseline= 9.282608695652174
eachuser= [0. 0. 0. 0. 0. 8.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 5.110502243041992
['train-height-2.py', '0']
2_155_65_2_csi_a2_20.dat
155 2
2_155_65_2_csi_a2_23.dat
155 4
2_155_65_2_csi_a2_16.dat
155 6
155 7
155 8
155 9
2_155_65_2_csi_a2_1.dat
2_155_65_2_csi_a2_21.dat
155 12
155 13
155 14
2_155_65_2_csi_a2_25.dat
2_155_65_2_csi_a2_18.dat
2_155_65_2_csi_a2_17.dat
2_155_65_2_csi_a2_14.dat
155 19
155 20
155 21
155 22
155 23
155 24
2_155_65_2_csi_a2_27.dat
2_155_65_2_csi_a2_24.dat
155 27
155 28
2_155_65_2_csi_a2_7.dat
155 30
170 31
170 32
170 33
2_170_60_2_csi_a2_2.dat
170 35
170 36
170 37
170 38
2_170_60_2_csi_a2_10.dat
170 40
1_165_65_2_csi_a2_30.dat
1_165_65_2_csi_a2_15.dat
1_165_65_2_csi_a2_10.dat
1_165_65_2_csi_a2_21.dat
1_165_65_2_csi_a2_9.dat
1_165_65_2_csi_a2_13.dat
1_165_65_2_csi_a2_19.dat
1_165_65_2_csi_a2_18.dat
1_165_65_2_csi_a2_7.dat
1_165_65_2_csi_a2_20.dat
1_165_65_2_csi_a2_11.dat
1_165_65_2_csi_a2_26.dat
1_165_65_2_csi_a2_16.dat
1_165_65_2_csi_a2_28.dat
1_165_65_2_csi_a2_5.dat
1_165_65_2_csi_a2_6.dat
1_165_65_2_csi_a2_25.dat
1_165_65_2_csi_a2_29.dat
1_165_65_2_csi_a2_23.dat
1_165_65_2_csi_a2_3.dat
1_165_65_2_csi_a2_27.dat
1_165_65_2_csi_a2_2.dat
1_165_65_2_csi_a2_17.dat
1_165_65_2_csi_a2_8.dat
1_165_65_2_csi_a2_14.dat
1_165_65_2_csi_a2_1.dat
1_165_65_2_csi_a2_24.dat
1_165_65_2_csi_a2_4.dat
1_165_65_2_csi_a2_22.dat
1_165_65_2_csi_a2_12.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
2_165_50_2_csi_a2_13.dat
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
165 89
165 90
165 91
165 92
165 93
165 94
165 95
165 96
165 97
165 98
2_165_50_2_csi_a2_24.dat
165 100
1_175_70_2_csi_a2_25.dat
175 102
1_175_70_2_csi_a2_20.dat
1_175_70_2_csi_a2_28.dat
175 105
1_175_70_2_csi_a2_22.dat
1_175_70_2_csi_a2_5.dat
1_175_70_2_csi_a2_11.dat
1_175_70_2_csi_a2_6.dat
1_175_70_2_csi_a2_8.dat
1_175_70_2_csi_a2_3.dat
1_175_70_2_csi_a2_16.dat
1_175_70_2_csi_a2_19.dat
1_175_70_2_csi_a2_1.dat
1_175_70_2_csi_a2_27.dat
175 116
1_175_70_2_csi_a2_18.dat
1_175_70_2_csi_a2_23.dat
1_175_70_2_csi_a2_21.dat
1_175_70_2_csi_a2_12.dat
1_175_70_2_csi_a2_30.dat
1_175_70_2_csi_a2_15.dat
1_175_70_2_csi_a2_17.dat
1_175_70_2_csi_a2_14.dat
1_175_70_2_csi_a2_9.dat
1_175_70_2_csi_a2_24.dat
1_175_70_2_csi_a2_10.dat
1_175_70_2_csi_a2_4.dat
1_175_70_2_csi_a2_29.dat
1_175_70_2_csi_a2_13.dat
1_180_85_2_csi_a2_22.dat
1_180_85_2_csi_a2_17.dat
1_180_85_2_csi_a2_15.dat
1_180_85_2_csi_a2_21.dat
180 135
1_180_85_2_csi_a2_7.dat
1_180_85_2_csi_a2_25.dat
1_180_85_2_csi_a2_19.dat
180 139
1_180_85_2_csi_a2_6.dat
1_180_85_2_csi_a2_1.dat
1_180_85_2_csi_a2_16.dat
1_180_85_2_csi_a2_14.dat
1_180_85_2_csi_a2_23.dat
1_180_85_2_csi_a2_24.dat
1_180_85_2_csi_a2_8.dat
180 147
1_180_85_2_csi_a2_13.dat
1_180_85_2_csi_a2_29.dat
1_180_85_2_csi_a2_9.dat
1_180_85_2_csi_a2_18.dat
1_180_85_2_csi_a2_28.dat
1_180_85_2_csi_a2_12.dat
1_180_85_2_csi_a2_30.dat
1_180_85_2_csi_a2_20.dat
180 156
1_180_85_2_csi_a2_26.dat
1_180_85_2_csi_a2_11.dat
180 159
1_180_85_2_csi_a2_10.dat
180 161
180 162
180 163
180 164
180 165
180 166
1_180_75_2_csi_a2_9.dat
180 168
180 169
180 170
180 171
180 172
180 173
180 174
180 175
180 176
180 177
180 178
180 179
180 180
180 181
1_180_75_2_csi_a2_1.dat
180 183
180 184
180 185
1_180_75_2_csi_a2_4.dat
180 187
180 188
180 189
180 190
173 191
173 192
173 193
173 194
173 195
173 196
173 197
173 198
173 199
1_173_85_2_csi_a2_23.dat
173 201
173 202
173 203
173 204
173 205
173 206
173 207
1_173_85_2_csi_a2_13.dat
173 209
173 210
173 211
173 212
173 213
173 214
173 215
173 216
1_173_85_2_csi_a2_28.dat
173 218
1_173_85_2_csi_a2_4.dat
173 220
(115, 30, 3)
(115, 421, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155 155
 170 170 170 170 170 170 170 170 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 175 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 173
 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173 173
 173 173 173 173 173 173 173]
(115, 421, 30, 3, 1)

Loaded dataset of 115 samples, each sized (421, 30, 3, 1)


Train on 92 samples
Test on 23 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 421, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 421, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 421, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 421, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 421, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 421, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 421, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 82 samples, validate on 10 samples
Epoch 1/30

32/82 [==========>...................] - ETA: 0s - loss: 1.0315 - mae: 0.9694 - mse: 1.0315
64/82 [======================>.......] - ETA: 0s - loss: 0.7795 - mae: 0.8084 - mse: 0.7795
82/82 [==============================] - 1s 11ms/step - loss: 0.6878 - mae: 0.7353 - mse: 0.6878 - val_loss: 0.2551 - val_mae: 0.4552 - val_mse: 0.2551
Epoch 2/30

32/82 [==========>...................] - ETA: 0s - loss: 0.3490 - mae: 0.5167 - mse: 0.3490
64/82 [======================>.......] - ETA: 0s - loss: 0.2685 - mae: 0.4370 - mse: 0.2685
82/82 [==============================] - 1s 8ms/step - loss: 0.2676 - mae: 0.4390 - mse: 0.2676 - val_loss: 0.1662 - val_mae: 0.3618 - val_mse: 0.1662
Epoch 3/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1680 - mae: 0.3309 - mse: 0.1680
64/82 [======================>.......] - ETA: 0s - loss: 0.1609 - mae: 0.3273 - mse: 0.1609
82/82 [==============================] - 0s 5ms/step - loss: 0.1700 - mae: 0.3434 - mse: 0.1700 - val_loss: 0.1350 - val_mae: 0.3211 - val_mse: 0.1350
Epoch 4/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1439 - mae: 0.3152 - mse: 0.1439
64/82 [======================>.......] - ETA: 0s - loss: 0.1599 - mae: 0.3220 - mse: 0.1599
82/82 [==============================] - 0s 5ms/step - loss: 0.1422 - mae: 0.3030 - mse: 0.1422 - val_loss: 0.1045 - val_mae: 0.2696 - val_mse: 0.1045
Epoch 5/30

32/82 [==========>...................] - ETA: 0s - loss: 0.1261 - mae: 0.2893 - mse: 0.1261
64/82 [======================>.......] - ETA: 0s - loss: 0.1180 - mae: 0.2887 - mse: 0.1180
82/82 [==============================] - 0s 5ms/step - loss: 0.1056 - mae: 0.2723 - mse: 0.1056 - val_loss: 0.0993 - val_mae: 0.2429 - val_mse: 0.0993
Epoch 6/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0684 - mae: 0.1946 - mse: 0.0684
64/82 [======================>.......] - ETA: 0s - loss: 0.0654 - mae: 0.1944 - mse: 0.0654
82/82 [==============================] - 0s 5ms/step - loss: 0.0721 - mae: 0.2109 - mse: 0.0721 - val_loss: 0.0888 - val_mae: 0.2118 - val_mse: 0.0888
Epoch 7/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0525 - mae: 0.1813 - mse: 0.0525
64/82 [======================>.......] - ETA: 0s - loss: 0.0615 - mae: 0.1915 - mse: 0.0615
82/82 [==============================] - 0s 5ms/step - loss: 0.0612 - mae: 0.1887 - mse: 0.0612 - val_loss: 0.0807 - val_mae: 0.1817 - val_mse: 0.0807
Epoch 8/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0810 - mae: 0.2115 - mse: 0.0810
64/82 [======================>.......] - ETA: 0s - loss: 0.0772 - mae: 0.2090 - mse: 0.0772
82/82 [==============================] - 0s 5ms/step - loss: 0.0702 - mae: 0.2021 - mse: 0.0702 - val_loss: 0.0815 - val_mae: 0.1761 - val_mse: 0.0815
Epoch 9/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0301 - mae: 0.1412 - mse: 0.0301
64/82 [======================>.......] - ETA: 0s - loss: 0.0488 - mae: 0.1726 - mse: 0.0488
82/82 [==============================] - 0s 5ms/step - loss: 0.0596 - mae: 0.1890 - mse: 0.0596 - val_loss: 0.0845 - val_mae: 0.1800 - val_mse: 0.0845
Epoch 10/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0506 - mae: 0.1716 - mse: 0.0506
64/82 [======================>.......] - ETA: 0s - loss: 0.0495 - mae: 0.1702 - mse: 0.0495
82/82 [==============================] - 0s 5ms/step - loss: 0.0595 - mae: 0.1859 - mse: 0.0595 - val_loss: 0.0888 - val_mae: 0.1940 - val_mse: 0.0888
Epoch 11/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0916 - mae: 0.1994 - mse: 0.0916
64/82 [======================>.......] - ETA: 0s - loss: 0.0626 - mae: 0.1785 - mse: 0.0626
82/82 [==============================] - 0s 5ms/step - loss: 0.0580 - mae: 0.1770 - mse: 0.0580 - val_loss: 0.0924 - val_mae: 0.2117 - val_mse: 0.0924
Epoch 12/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0655 - mae: 0.1798 - mse: 0.0655
64/82 [======================>.......] - ETA: 0s - loss: 0.0509 - mae: 0.1667 - mse: 0.0509
82/82 [==============================] - 0s 5ms/step - loss: 0.0582 - mae: 0.1772 - mse: 0.0582 - val_loss: 0.0829 - val_mae: 0.1850 - val_mse: 0.0829
Epoch 13/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0415 - mae: 0.1627 - mse: 0.0415
64/82 [======================>.......] - ETA: 0s - loss: 0.0385 - mae: 0.1568 - mse: 0.0385
82/82 [==============================] - 0s 5ms/step - loss: 0.0516 - mae: 0.1746 - mse: 0.0516 - val_loss: 0.0742 - val_mae: 0.1627 - val_mse: 0.0742
Epoch 14/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0461 - mae: 0.1474 - mse: 0.0461
64/82 [======================>.......] - ETA: 0s - loss: 0.0328 - mae: 0.1290 - mse: 0.0328
82/82 [==============================] - 0s 4ms/step - loss: 0.0490 - mae: 0.1534 - mse: 0.0490 - val_loss: 0.0736 - val_mae: 0.1621 - val_mse: 0.0736
Epoch 15/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0665 - mae: 0.1842 - mse: 0.0665
64/82 [======================>.......] - ETA: 0s - loss: 0.0549 - mae: 0.1678 - mse: 0.0549
82/82 [==============================] - 0s 4ms/step - loss: 0.0495 - mae: 0.1632 - mse: 0.0495 - val_loss: 0.0792 - val_mae: 0.1808 - val_mse: 0.0792
Epoch 16/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0565 - mae: 0.1912 - mse: 0.0565
64/82 [======================>.......] - ETA: 0s - loss: 0.0649 - mae: 0.2002 - mse: 0.0649
82/82 [==============================] - 0s 4ms/step - loss: 0.0576 - mae: 0.1873 - mse: 0.0576 - val_loss: 0.0826 - val_mae: 0.1983 - val_mse: 0.0826
Epoch 17/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0553 - mae: 0.1599 - mse: 0.0553
64/82 [======================>.......] - ETA: 0s - loss: 0.0423 - mae: 0.1454 - mse: 0.0423
82/82 [==============================] - 0s 5ms/step - loss: 0.0418 - mae: 0.1381 - mse: 0.0418 - val_loss: 0.0797 - val_mae: 0.1883 - val_mse: 0.0797
Epoch 18/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0534 - mae: 0.1765 - mse: 0.0534
64/82 [======================>.......] - ETA: 0s - loss: 0.0484 - mae: 0.1640 - mse: 0.0484
82/82 [==============================] - 0s 5ms/step - loss: 0.0481 - mae: 0.1596 - mse: 0.0481 - val_loss: 0.0737 - val_mae: 0.1648 - val_mse: 0.0737
Epoch 19/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0749 - mae: 0.1952 - mse: 0.0749
64/82 [======================>.......] - ETA: 0s - loss: 0.0546 - mae: 0.1729 - mse: 0.0546
82/82 [==============================] - 0s 5ms/step - loss: 0.0521 - mae: 0.1722 - mse: 0.0521 - val_loss: 0.0730 - val_mae: 0.1629 - val_mse: 0.0730
Epoch 20/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0562 - mae: 0.1597 - mse: 0.0562
64/82 [======================>.......] - ETA: 0s - loss: 0.0421 - mae: 0.1407 - mse: 0.0421
82/82 [==============================] - 0s 5ms/step - loss: 0.0445 - mae: 0.1480 - mse: 0.0445 - val_loss: 0.0732 - val_mae: 0.1617 - val_mse: 0.0732
Epoch 21/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0291 - mae: 0.1342 - mse: 0.0291
64/82 [======================>.......] - ETA: 0s - loss: 0.0282 - mae: 0.1375 - mse: 0.0282
82/82 [==============================] - 0s 5ms/step - loss: 0.0487 - mae: 0.1618 - mse: 0.0487 - val_loss: 0.0757 - val_mae: 0.1679 - val_mse: 0.0757
Epoch 22/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0499 - mae: 0.1620 - mse: 0.0499
64/82 [======================>.......] - ETA: 0s - loss: 0.0490 - mae: 0.1617 - mse: 0.0490
82/82 [==============================] - 0s 5ms/step - loss: 0.0459 - mae: 0.1599 - mse: 0.0459 - val_loss: 0.0755 - val_mae: 0.1679 - val_mse: 0.0755
Epoch 23/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0376 - mae: 0.1359 - mse: 0.0376
64/82 [======================>.......] - ETA: 0s - loss: 0.0360 - mae: 0.1321 - mse: 0.0360
82/82 [==============================] - 0s 5ms/step - loss: 0.0431 - mae: 0.1491 - mse: 0.0431 - val_loss: 0.0786 - val_mae: 0.1755 - val_mse: 0.0786
Epoch 24/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0715 - mae: 0.1735 - mse: 0.0715
64/82 [======================>.......] - ETA: 0s - loss: 0.0450 - mae: 0.1431 - mse: 0.0450
82/82 [==============================] - 0s 5ms/step - loss: 0.0412 - mae: 0.1413 - mse: 0.0412 - val_loss: 0.0790 - val_mae: 0.1805 - val_mse: 0.0790
Epoch 25/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0519 - mae: 0.1563 - mse: 0.0519
64/82 [======================>.......] - ETA: 0s - loss: 0.0437 - mae: 0.1435 - mse: 0.0437
82/82 [==============================] - 0s 5ms/step - loss: 0.0417 - mae: 0.1485 - mse: 0.0417 - val_loss: 0.0773 - val_mae: 0.1736 - val_mse: 0.0773
Epoch 26/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0535 - mae: 0.1527 - mse: 0.0535
64/82 [======================>.......] - ETA: 0s - loss: 0.0372 - mae: 0.1328 - mse: 0.0372
82/82 [==============================] - 0s 5ms/step - loss: 0.0406 - mae: 0.1366 - mse: 0.0406 - val_loss: 0.0740 - val_mae: 0.1526 - val_mse: 0.0740
Epoch 27/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0346 - mae: 0.1313 - mse: 0.0346
64/82 [======================>.......] - ETA: 0s - loss: 0.0285 - mae: 0.1231 - mse: 0.0285
82/82 [==============================] - 0s 5ms/step - loss: 0.0407 - mae: 0.1392 - mse: 0.0407 - val_loss: 0.0746 - val_mae: 0.1527 - val_mse: 0.0746
Epoch 28/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0354 - mae: 0.1331 - mse: 0.0354
64/82 [======================>.......] - ETA: 0s - loss: 0.0350 - mae: 0.1317 - mse: 0.0350
82/82 [==============================] - 0s 5ms/step - loss: 0.0365 - mae: 0.1312 - mse: 0.0365 - val_loss: 0.0751 - val_mae: 0.1582 - val_mse: 0.0751
Epoch 29/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0443 - mae: 0.1522 - mse: 0.0443
64/82 [======================>.......] - ETA: 0s - loss: 0.0397 - mae: 0.1363 - mse: 0.0397
82/82 [==============================] - 0s 5ms/step - loss: 0.0402 - mae: 0.1365 - mse: 0.0402 - val_loss: 0.0746 - val_mae: 0.1606 - val_mse: 0.0746
Epoch 30/30

32/82 [==========>...................] - ETA: 0s - loss: 0.0450 - mae: 0.1471 - mse: 0.0450
64/82 [======================>.......] - ETA: 0s - loss: 0.0310 - mae: 0.1274 - mse: 0.0310
82/82 [==============================] - 0s 5ms/step - loss: 0.0332 - mae: 0.1285 - mse: 0.0332 - val_loss: 0.0749 - val_mae: 0.1741 - val_mse: 0.0749
Saving trained model...
105
Testing...
heightdiff= [ 0.         0.         0.         0.         0.        14.2665863]
average prediction= [3.23304]
baseline= 5.8478260869565215
eachuser= [0. 0. 0. 0. 0. 3.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 4.7555287679036455
