['train-height-9.py', '0']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
155 7
2_155_65_9_csi_a9_19.dat
155 9
155 10
155 11
155 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
155 15
2_155_65_9_csi_a9_7.dat
155 17
155 18
155 19
2_155_65_9_csi_a9_21.dat
155 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
155 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_9_csi_a9_29.dat
165 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
165 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
165 52
1_165_65_9_csi_a9_16.dat
165 54
165 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
165 58
165 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
165 62
165 63
1_165_65_9_csi_a9_5.dat
165 65
165 66
1_165_65_9_csi_a9_18.dat
165 68
165 69
1_165_65_9_csi_a9_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_9_csi_a9_22.dat
165 90
165 91
165 92
165 93
165 94
2_165_50_9_csi_a9_26.dat
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
1_180_85_9_csi_a9_7.dat
180 132
180 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
180 140
180 141
180 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
180 146
180 147
180 148
180 149
180 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
180 153
180 154
180 155
1_180_85_9_csi_a9_18.dat
180 157
180 158
180 159
180 160
180 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
180 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
180 171
1_180_75_9_csi_a9_25.dat
180 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
180 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
180 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
180 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
173 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
173 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
173 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
173 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
173 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 170 170 170 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 173 173 173 173 173]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.5117 - mae: 0.6492 - mse: 0.5117
64/86 [=====================>........] - ETA: 0s - loss: 0.4418 - mae: 0.6034 - mse: 0.4418
86/86 [==============================] - 1s 11ms/step - loss: 0.4138 - mae: 0.5744 - mse: 0.4138 - val_loss: 0.2954 - val_mae: 0.5041 - val_mse: 0.2954
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.3215 - mae: 0.5095 - mse: 0.3215
64/86 [=====================>........] - ETA: 0s - loss: 0.2668 - mae: 0.4604 - mse: 0.2668
86/86 [==============================] - 1s 7ms/step - loss: 0.2217 - mae: 0.4003 - mse: 0.2217 - val_loss: 0.0927 - val_mae: 0.2613 - val_mse: 0.0927
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1188 - mae: 0.2835 - mse: 0.1188
64/86 [=====================>........] - ETA: 0s - loss: 0.1171 - mae: 0.2853 - mse: 0.1171
86/86 [==============================] - 1s 7ms/step - loss: 0.1237 - mae: 0.2864 - mse: 0.1237 - val_loss: 0.0481 - val_mae: 0.1978 - val_mse: 0.0481
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1506 - mae: 0.3013 - mse: 0.1506
64/86 [=====================>........] - ETA: 0s - loss: 0.1673 - mae: 0.3240 - mse: 0.1673
86/86 [==============================] - 1s 7ms/step - loss: 0.1619 - mae: 0.3167 - mse: 0.1619 - val_loss: 0.0484 - val_mae: 0.2010 - val_mse: 0.0484
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0931 - mae: 0.2527 - mse: 0.0931
64/86 [=====================>........] - ETA: 0s - loss: 0.1056 - mae: 0.2652 - mse: 0.1056
86/86 [==============================] - 1s 6ms/step - loss: 0.1138 - mae: 0.2782 - mse: 0.1138 - val_loss: 0.0860 - val_mae: 0.2403 - val_mse: 0.0860
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1093 - mae: 0.2533 - mse: 0.1093
64/86 [=====================>........] - ETA: 0s - loss: 0.1198 - mae: 0.2761 - mse: 0.1198
86/86 [==============================] - 1s 7ms/step - loss: 0.1198 - mae: 0.2793 - mse: 0.1198 - val_loss: 0.1024 - val_mae: 0.2586 - val_mse: 0.1024
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1015 - mae: 0.2582 - mse: 0.1015
64/86 [=====================>........] - ETA: 0s - loss: 0.1196 - mae: 0.2874 - mse: 0.1196
86/86 [==============================] - 1s 7ms/step - loss: 0.1134 - mae: 0.2750 - mse: 0.1134 - val_loss: 0.0810 - val_mae: 0.2232 - val_mse: 0.0810
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0785 - mae: 0.2215 - mse: 0.0785
64/86 [=====================>........] - ETA: 0s - loss: 0.0907 - mae: 0.2303 - mse: 0.0907
86/86 [==============================] - 1s 7ms/step - loss: 0.0932 - mae: 0.2381 - mse: 0.0932 - val_loss: 0.0587 - val_mae: 0.1809 - val_mse: 0.0587
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1321 - mae: 0.3005 - mse: 0.1321
64/86 [=====================>........] - ETA: 0s - loss: 0.1144 - mae: 0.2761 - mse: 0.1144
86/86 [==============================] - 1s 7ms/step - loss: 0.1026 - mae: 0.2593 - mse: 0.1026 - val_loss: 0.0496 - val_mae: 0.1596 - val_mse: 0.0496
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1139 - mae: 0.2679 - mse: 0.1139
64/86 [=====================>........] - ETA: 0s - loss: 0.0979 - mae: 0.2448 - mse: 0.0979
86/86 [==============================] - 1s 7ms/step - loss: 0.0996 - mae: 0.2513 - mse: 0.0996 - val_loss: 0.0483 - val_mae: 0.1538 - val_mse: 0.0483
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1038 - mae: 0.2585 - mse: 0.1038
64/86 [=====================>........] - ETA: 0s - loss: 0.1049 - mae: 0.2584 - mse: 0.1049
86/86 [==============================] - 1s 7ms/step - loss: 0.1016 - mae: 0.2527 - mse: 0.1016 - val_loss: 0.0543 - val_mae: 0.1676 - val_mse: 0.0543
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0895 - mae: 0.2317 - mse: 0.0895
64/86 [=====================>........] - ETA: 0s - loss: 0.0744 - mae: 0.2009 - mse: 0.0744
86/86 [==============================] - 1s 9ms/step - loss: 0.0756 - mae: 0.2077 - mse: 0.0756 - val_loss: 0.0540 - val_mae: 0.1745 - val_mse: 0.0540
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1057 - mae: 0.2495 - mse: 0.1057
64/86 [=====================>........] - ETA: 0s - loss: 0.0979 - mae: 0.2342 - mse: 0.0979
86/86 [==============================] - 1s 7ms/step - loss: 0.0863 - mae: 0.2148 - mse: 0.0863 - val_loss: 0.0532 - val_mae: 0.1748 - val_mse: 0.0532
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0969 - mae: 0.2453 - mse: 0.0969
64/86 [=====================>........] - ETA: 0s - loss: 0.0913 - mae: 0.2334 - mse: 0.0913
86/86 [==============================] - 1s 8ms/step - loss: 0.0837 - mae: 0.2154 - mse: 0.0837 - val_loss: 0.0412 - val_mae: 0.1437 - val_mse: 0.0412
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0657 - mae: 0.1731 - mse: 0.0657
64/86 [=====================>........] - ETA: 0s - loss: 0.0827 - mae: 0.2117 - mse: 0.0827
86/86 [==============================] - 1s 7ms/step - loss: 0.0862 - mae: 0.2202 - mse: 0.0862 - val_loss: 0.0364 - val_mae: 0.1253 - val_mse: 0.0364
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0525 - mae: 0.1730 - mse: 0.0525
64/86 [=====================>........] - ETA: 0s - loss: 0.0727 - mae: 0.2034 - mse: 0.0727
86/86 [==============================] - 1s 7ms/step - loss: 0.0757 - mae: 0.1988 - mse: 0.0757 - val_loss: 0.0332 - val_mae: 0.1109 - val_mse: 0.0332
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0714 - mae: 0.2033 - mse: 0.0714
64/86 [=====================>........] - ETA: 0s - loss: 0.0684 - mae: 0.1889 - mse: 0.0684
86/86 [==============================] - 1s 7ms/step - loss: 0.0730 - mae: 0.2016 - mse: 0.0730 - val_loss: 0.0386 - val_mae: 0.1236 - val_mse: 0.0386
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0801 - mae: 0.2169 - mse: 0.0801
64/86 [=====================>........] - ETA: 0s - loss: 0.0727 - mae: 0.1980 - mse: 0.0727
86/86 [==============================] - 1s 7ms/step - loss: 0.0703 - mae: 0.1917 - mse: 0.0703 - val_loss: 0.0416 - val_mae: 0.1259 - val_mse: 0.0416
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0595 - mae: 0.1869 - mse: 0.0595
64/86 [=====================>........] - ETA: 0s - loss: 0.0687 - mae: 0.2004 - mse: 0.0687
86/86 [==============================] - 1s 7ms/step - loss: 0.0643 - mae: 0.1927 - mse: 0.0643 - val_loss: 0.0432 - val_mae: 0.1257 - val_mse: 0.0432
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0698 - mae: 0.2004 - mse: 0.0698
64/86 [=====================>........] - ETA: 0s - loss: 0.0677 - mae: 0.2005 - mse: 0.0677
86/86 [==============================] - 1s 7ms/step - loss: 0.0719 - mae: 0.2084 - mse: 0.0719 - val_loss: 0.0419 - val_mae: 0.1291 - val_mse: 0.0419
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0524 - mae: 0.1579 - mse: 0.0524
64/86 [=====================>........] - ETA: 0s - loss: 0.0562 - mae: 0.1780 - mse: 0.0562
86/86 [==============================] - 1s 7ms/step - loss: 0.0592 - mae: 0.1810 - mse: 0.0592 - val_loss: 0.0391 - val_mae: 0.1249 - val_mse: 0.0391
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0785 - mae: 0.2194 - mse: 0.0785
64/86 [=====================>........] - ETA: 0s - loss: 0.0673 - mae: 0.1901 - mse: 0.0673
86/86 [==============================] - 1s 8ms/step - loss: 0.0681 - mae: 0.1911 - mse: 0.0681 - val_loss: 0.0425 - val_mae: 0.1360 - val_mse: 0.0425
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0672 - mae: 0.1917 - mse: 0.0672
64/86 [=====================>........] - ETA: 0s - loss: 0.0580 - mae: 0.1759 - mse: 0.0580
86/86 [==============================] - 1s 7ms/step - loss: 0.0524 - mae: 0.1659 - mse: 0.0524 - val_loss: 0.0452 - val_mae: 0.1316 - val_mse: 0.0452
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0404 - mae: 0.1423 - mse: 0.0404
64/86 [=====================>........] - ETA: 0s - loss: 0.0519 - mae: 0.1623 - mse: 0.0519
86/86 [==============================] - 1s 7ms/step - loss: 0.0530 - mae: 0.1681 - mse: 0.0530 - val_loss: 0.0522 - val_mae: 0.1435 - val_mse: 0.0522
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0380 - mae: 0.1432 - mse: 0.0380
64/86 [=====================>........] - ETA: 0s - loss: 0.0432 - mae: 0.1494 - mse: 0.0432
86/86 [==============================] - 1s 7ms/step - loss: 0.0525 - mae: 0.1672 - mse: 0.0525 - val_loss: 0.0620 - val_mae: 0.1814 - val_mse: 0.0620
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0379 - mae: 0.1294 - mse: 0.0379
64/86 [=====================>........] - ETA: 0s - loss: 0.0490 - mae: 0.1562 - mse: 0.0490
86/86 [==============================] - 1s 7ms/step - loss: 0.0456 - mae: 0.1552 - mse: 0.0456 - val_loss: 0.0657 - val_mae: 0.1975 - val_mse: 0.0657
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0437 - mae: 0.1532 - mse: 0.0437
64/86 [=====================>........] - ETA: 0s - loss: 0.0458 - mae: 0.1509 - mse: 0.0458
86/86 [==============================] - 1s 7ms/step - loss: 0.0423 - mae: 0.1441 - mse: 0.0423 - val_loss: 0.0756 - val_mae: 0.2123 - val_mse: 0.0756
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0330 - mae: 0.1317 - mse: 0.0330
64/86 [=====================>........] - ETA: 0s - loss: 0.0404 - mae: 0.1463 - mse: 0.0404
86/86 [==============================] - 1s 8ms/step - loss: 0.0396 - mae: 0.1438 - mse: 0.0396 - val_loss: 0.0677 - val_mae: 0.1901 - val_mse: 0.0677
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0243 - mae: 0.1227 - mse: 0.0243
64/86 [=====================>........] - ETA: 0s - loss: 0.0304 - mae: 0.1303 - mse: 0.0304
86/86 [==============================] - 1s 7ms/step - loss: 0.0348 - mae: 0.1388 - mse: 0.0348 - val_loss: 0.0674 - val_mae: 0.1977 - val_mse: 0.0674
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0254 - mae: 0.1328 - mse: 0.0254
64/86 [=====================>........] - ETA: 0s - loss: 0.0442 - mae: 0.1604 - mse: 0.0442
86/86 [==============================] - 0s 5ms/step - loss: 0.0405 - mae: 0.1531 - mse: 0.0405 - val_loss: 0.0608 - val_mae: 0.1870 - val_mse: 0.0608
Saving trained model...
99
Testing...
heightdiff= [0.         0.         0.         0.         0.         8.48803711]
average prediction= [3.1357458]
baseline= 6.02
eachuser= [0. 0. 0. 0. 0. 1.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 8.488037109375
['train-height-9.py', '0']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
155 7
2_155_65_9_csi_a9_19.dat
155 9
155 10
155 11
155 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
155 15
2_155_65_9_csi_a9_7.dat
155 17
155 18
155 19
2_155_65_9_csi_a9_21.dat
155 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
155 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_9_csi_a9_29.dat
165 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
165 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
165 52
1_165_65_9_csi_a9_16.dat
165 54
165 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
165 58
165 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
165 62
165 63
1_165_65_9_csi_a9_5.dat
165 65
165 66
1_165_65_9_csi_a9_18.dat
165 68
165 69
1_165_65_9_csi_a9_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_9_csi_a9_22.dat
165 90
165 91
165 92
165 93
165 94
2_165_50_9_csi_a9_26.dat
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
1_180_85_9_csi_a9_7.dat
180 132
180 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
180 140
180 141
180 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
180 146
180 147
180 148
180 149
180 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
180 153
180 154
180 155
1_180_85_9_csi_a9_18.dat
180 157
180 158
180 159
180 160
180 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
180 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
180 171
1_180_75_9_csi_a9_25.dat
180 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
180 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
180 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
180 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
173 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
173 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
173 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
173 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
173 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 170 170 170 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 173 173 173 173 173]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 1s - loss: 0.3624 - mae: 0.5480 - mse: 0.3624
64/86 [=====================>........] - ETA: 0s - loss: 0.3493 - mae: 0.5313 - mse: 0.3493
86/86 [==============================] - 1s 13ms/step - loss: 0.3138 - mae: 0.4991 - mse: 0.3138 - val_loss: 0.1580 - val_mae: 0.3269 - val_mse: 0.1580
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1429 - mae: 0.3077 - mse: 0.1429
64/86 [=====================>........] - ETA: 0s - loss: 0.1423 - mae: 0.2971 - mse: 0.1423
86/86 [==============================] - 1s 7ms/step - loss: 0.1498 - mae: 0.3040 - mse: 0.1498 - val_loss: 0.1164 - val_mae: 0.2858 - val_mse: 0.1164
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1181 - mae: 0.2590 - mse: 0.1181
64/86 [=====================>........] - ETA: 0s - loss: 0.1401 - mae: 0.2815 - mse: 0.1401
86/86 [==============================] - 1s 6ms/step - loss: 0.1253 - mae: 0.2644 - mse: 0.1253 - val_loss: 0.1025 - val_mae: 0.2669 - val_mse: 0.1025
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1183 - mae: 0.2572 - mse: 0.1183
64/86 [=====================>........] - ETA: 0s - loss: 0.1008 - mae: 0.2416 - mse: 0.1008
86/86 [==============================] - 1s 7ms/step - loss: 0.1090 - mae: 0.2619 - mse: 0.1090 - val_loss: 0.1163 - val_mae: 0.2831 - val_mse: 0.1163
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1231 - mae: 0.2969 - mse: 0.1231
64/86 [=====================>........] - ETA: 0s - loss: 0.1115 - mae: 0.2764 - mse: 0.1115
86/86 [==============================] - 1s 7ms/step - loss: 0.1033 - mae: 0.2650 - mse: 0.1033 - val_loss: 0.1075 - val_mae: 0.2657 - val_mse: 0.1075
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0824 - mae: 0.2184 - mse: 0.0824
64/86 [=====================>........] - ETA: 0s - loss: 0.1000 - mae: 0.2506 - mse: 0.1000
86/86 [==============================] - 1s 7ms/step - loss: 0.0980 - mae: 0.2529 - mse: 0.0980 - val_loss: 0.0863 - val_mae: 0.2248 - val_mse: 0.0863
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0670 - mae: 0.1994 - mse: 0.0670
64/86 [=====================>........] - ETA: 0s - loss: 0.0873 - mae: 0.2261 - mse: 0.0873
86/86 [==============================] - 1s 7ms/step - loss: 0.0959 - mae: 0.2426 - mse: 0.0959 - val_loss: 0.0779 - val_mae: 0.1993 - val_mse: 0.0779
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0909 - mae: 0.2454 - mse: 0.0909
64/86 [=====================>........] - ETA: 0s - loss: 0.0887 - mae: 0.2345 - mse: 0.0887
86/86 [==============================] - 1s 7ms/step - loss: 0.0953 - mae: 0.2418 - mse: 0.0953 - val_loss: 0.0811 - val_mae: 0.2069 - val_mse: 0.0811
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1103 - mae: 0.2834 - mse: 0.1103
64/86 [=====================>........] - ETA: 0s - loss: 0.0867 - mae: 0.2289 - mse: 0.0867
86/86 [==============================] - 1s 7ms/step - loss: 0.0756 - mae: 0.2101 - mse: 0.0756 - val_loss: 0.0891 - val_mae: 0.2279 - val_mse: 0.0891
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0471 - mae: 0.1651 - mse: 0.0471
64/86 [=====================>........] - ETA: 0s - loss: 0.0798 - mae: 0.2156 - mse: 0.0798
86/86 [==============================] - 1s 7ms/step - loss: 0.0761 - mae: 0.2085 - mse: 0.0761 - val_loss: 0.0940 - val_mae: 0.2357 - val_mse: 0.0940
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0614 - mae: 0.1952 - mse: 0.0614
64/86 [=====================>........] - ETA: 0s - loss: 0.0908 - mae: 0.2267 - mse: 0.0908
86/86 [==============================] - 1s 7ms/step - loss: 0.0854 - mae: 0.2219 - mse: 0.0854 - val_loss: 0.0942 - val_mae: 0.2327 - val_mse: 0.0942
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0686 - mae: 0.1872 - mse: 0.0686
64/86 [=====================>........] - ETA: 0s - loss: 0.0814 - mae: 0.2093 - mse: 0.0814
86/86 [==============================] - 1s 7ms/step - loss: 0.0776 - mae: 0.2024 - mse: 0.0776 - val_loss: 0.0896 - val_mae: 0.2185 - val_mse: 0.0896
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1169 - mae: 0.2509 - mse: 0.1169
64/86 [=====================>........] - ETA: 0s - loss: 0.0828 - mae: 0.2100 - mse: 0.0828
86/86 [==============================] - 1s 7ms/step - loss: 0.0812 - mae: 0.2084 - mse: 0.0812 - val_loss: 0.0873 - val_mae: 0.2106 - val_mse: 0.0873
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0598 - mae: 0.1761 - mse: 0.0598
64/86 [=====================>........] - ETA: 0s - loss: 0.0656 - mae: 0.1814 - mse: 0.0656
86/86 [==============================] - 1s 7ms/step - loss: 0.0666 - mae: 0.1851 - mse: 0.0666 - val_loss: 0.0843 - val_mae: 0.2028 - val_mse: 0.0843
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0748 - mae: 0.1901 - mse: 0.0748
64/86 [=====================>........] - ETA: 0s - loss: 0.0836 - mae: 0.2080 - mse: 0.0836
86/86 [==============================] - 1s 7ms/step - loss: 0.0768 - mae: 0.1941 - mse: 0.0768 - val_loss: 0.0849 - val_mae: 0.2066 - val_mse: 0.0849
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0934 - mae: 0.2257 - mse: 0.0934
64/86 [=====================>........] - ETA: 0s - loss: 0.0863 - mae: 0.2203 - mse: 0.0863
86/86 [==============================] - 1s 7ms/step - loss: 0.0747 - mae: 0.2023 - mse: 0.0747 - val_loss: 0.0835 - val_mae: 0.2032 - val_mse: 0.0835
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0713 - mae: 0.2001 - mse: 0.0713
64/86 [=====================>........] - ETA: 0s - loss: 0.0675 - mae: 0.1951 - mse: 0.0675
86/86 [==============================] - 1s 7ms/step - loss: 0.0688 - mae: 0.1940 - mse: 0.0688 - val_loss: 0.0792 - val_mae: 0.1854 - val_mse: 0.0792
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0848 - mae: 0.2082 - mse: 0.0848
64/86 [=====================>........] - ETA: 0s - loss: 0.0735 - mae: 0.1981 - mse: 0.0735
86/86 [==============================] - 1s 7ms/step - loss: 0.0716 - mae: 0.1977 - mse: 0.0716 - val_loss: 0.0801 - val_mae: 0.1878 - val_mse: 0.0801
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0465 - mae: 0.1650 - mse: 0.0465
64/86 [=====================>........] - ETA: 0s - loss: 0.0769 - mae: 0.2043 - mse: 0.0769
86/86 [==============================] - 1s 7ms/step - loss: 0.0779 - mae: 0.2009 - mse: 0.0779 - val_loss: 0.0819 - val_mae: 0.1958 - val_mse: 0.0819
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0652 - mae: 0.1698 - mse: 0.0652
64/86 [=====================>........] - ETA: 0s - loss: 0.0583 - mae: 0.1674 - mse: 0.0583
86/86 [==============================] - 1s 7ms/step - loss: 0.0597 - mae: 0.1720 - mse: 0.0597 - val_loss: 0.0873 - val_mae: 0.2163 - val_mse: 0.0873
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0814 - mae: 0.2122 - mse: 0.0814
64/86 [=====================>........] - ETA: 0s - loss: 0.0708 - mae: 0.1888 - mse: 0.0708
86/86 [==============================] - 1s 6ms/step - loss: 0.0715 - mae: 0.1932 - mse: 0.0715 - val_loss: 0.0873 - val_mae: 0.2169 - val_mse: 0.0873
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0891 - mae: 0.2257 - mse: 0.0891
64/86 [=====================>........] - ETA: 0s - loss: 0.0662 - mae: 0.1865 - mse: 0.0662
86/86 [==============================] - 0s 6ms/step - loss: 0.0680 - mae: 0.1848 - mse: 0.0680 - val_loss: 0.0801 - val_mae: 0.1922 - val_mse: 0.0801
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0562 - mae: 0.1613 - mse: 0.0562
64/86 [=====================>........] - ETA: 0s - loss: 0.0676 - mae: 0.1880 - mse: 0.0676
86/86 [==============================] - 0s 6ms/step - loss: 0.0639 - mae: 0.1787 - mse: 0.0639 - val_loss: 0.0808 - val_mae: 0.1957 - val_mse: 0.0808
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0439 - mae: 0.1364 - mse: 0.0439
64/86 [=====================>........] - ETA: 0s - loss: 0.0660 - mae: 0.1831 - mse: 0.0660
86/86 [==============================] - 0s 5ms/step - loss: 0.0656 - mae: 0.1834 - mse: 0.0656 - val_loss: 0.0829 - val_mae: 0.2081 - val_mse: 0.0829
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0720 - mae: 0.2018 - mse: 0.0720
64/86 [=====================>........] - ETA: 0s - loss: 0.0635 - mae: 0.1812 - mse: 0.0635
86/86 [==============================] - 1s 6ms/step - loss: 0.0592 - mae: 0.1714 - mse: 0.0592 - val_loss: 0.0804 - val_mae: 0.2053 - val_mse: 0.0804
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0518 - mae: 0.1702 - mse: 0.0518
64/86 [=====================>........] - ETA: 0s - loss: 0.0596 - mae: 0.1772 - mse: 0.0596
86/86 [==============================] - 0s 5ms/step - loss: 0.0608 - mae: 0.1771 - mse: 0.0608 - val_loss: 0.0757 - val_mae: 0.1905 - val_mse: 0.0757
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0500 - mae: 0.1595 - mse: 0.0500
64/86 [=====================>........] - ETA: 0s - loss: 0.0601 - mae: 0.1715 - mse: 0.0601
86/86 [==============================] - 0s 5ms/step - loss: 0.0665 - mae: 0.1761 - mse: 0.0665 - val_loss: 0.0775 - val_mae: 0.2042 - val_mse: 0.0775
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0665 - mae: 0.1935 - mse: 0.0665
64/86 [=====================>........] - ETA: 0s - loss: 0.0488 - mae: 0.1585 - mse: 0.0488
86/86 [==============================] - 0s 5ms/step - loss: 0.0549 - mae: 0.1708 - mse: 0.0549 - val_loss: 0.0860 - val_mae: 0.2269 - val_mse: 0.0860
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0436 - mae: 0.1422 - mse: 0.0436
64/86 [=====================>........] - ETA: 0s - loss: 0.0556 - mae: 0.1638 - mse: 0.0556
86/86 [==============================] - 0s 6ms/step - loss: 0.0549 - mae: 0.1619 - mse: 0.0549 - val_loss: 0.0758 - val_mae: 0.1906 - val_mse: 0.0758
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0340 - mae: 0.1444 - mse: 0.0340
64/86 [=====================>........] - ETA: 0s - loss: 0.0480 - mae: 0.1642 - mse: 0.0480
86/86 [==============================] - 0s 6ms/step - loss: 0.0584 - mae: 0.1810 - mse: 0.0584 - val_loss: 0.0685 - val_mae: 0.1769 - val_mse: 0.0685
Saving trained model...
99
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         10.83825684]
average prediction= [2.3417242]
baseline= 4.98
eachuser= [0. 0. 0. 0. 0. 1.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 10.8382568359375
['train-height-9.py', '0']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
155 7
2_155_65_9_csi_a9_19.dat
155 9
155 10
155 11
155 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
155 15
2_155_65_9_csi_a9_7.dat
155 17
155 18
155 19
2_155_65_9_csi_a9_21.dat
155 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
155 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_9_csi_a9_29.dat
165 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
165 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
165 52
1_165_65_9_csi_a9_16.dat
165 54
165 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
165 58
165 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
165 62
165 63
1_165_65_9_csi_a9_5.dat
165 65
165 66
1_165_65_9_csi_a9_18.dat
165 68
165 69
1_165_65_9_csi_a9_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_9_csi_a9_22.dat
165 90
165 91
165 92
165 93
165 94
2_165_50_9_csi_a9_26.dat
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
1_180_85_9_csi_a9_7.dat
180 132
180 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
180 140
180 141
180 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
180 146
180 147
180 148
180 149
180 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
180 153
180 154
180 155
1_180_85_9_csi_a9_18.dat
180 157
180 158
180 159
180 160
180 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
180 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
180 171
1_180_75_9_csi_a9_25.dat
180 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
180 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
180 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
180 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
173 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
173 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
173 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
173 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
173 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 170 170 170 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 173 173 173 173 173]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.4204 - mae: 0.5807 - mse: 0.4204
64/86 [=====================>........] - ETA: 0s - loss: 0.4309 - mae: 0.5945 - mse: 0.4309
86/86 [==============================] - 1s 9ms/step - loss: 0.4070 - mae: 0.5695 - mse: 0.4070 - val_loss: 0.2018 - val_mae: 0.3830 - val_mse: 0.2018
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.2418 - mae: 0.4319 - mse: 0.2418
64/86 [=====================>........] - ETA: 0s - loss: 0.2082 - mae: 0.3840 - mse: 0.2082
86/86 [==============================] - 0s 5ms/step - loss: 0.1844 - mae: 0.3573 - mse: 0.1844 - val_loss: 0.0966 - val_mae: 0.2774 - val_mse: 0.0966
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1727 - mae: 0.3467 - mse: 0.1727
64/86 [=====================>........] - ETA: 0s - loss: 0.1572 - mae: 0.3247 - mse: 0.1572
86/86 [==============================] - 0s 5ms/step - loss: 0.1610 - mae: 0.3305 - mse: 0.1610 - val_loss: 0.0964 - val_mae: 0.2674 - val_mse: 0.0964
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1141 - mae: 0.2894 - mse: 0.1141
64/86 [=====================>........] - ETA: 0s - loss: 0.1235 - mae: 0.3039 - mse: 0.1235
86/86 [==============================] - 0s 5ms/step - loss: 0.1323 - mae: 0.3113 - mse: 0.1323 - val_loss: 0.0788 - val_mae: 0.2151 - val_mse: 0.0788
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0934 - mae: 0.2355 - mse: 0.0934
64/86 [=====================>........] - ETA: 0s - loss: 0.1066 - mae: 0.2569 - mse: 0.1066
86/86 [==============================] - 0s 5ms/step - loss: 0.1109 - mae: 0.2630 - mse: 0.1109 - val_loss: 0.1020 - val_mae: 0.2577 - val_mse: 0.1020
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1231 - mae: 0.2763 - mse: 0.1231
64/86 [=====================>........] - ETA: 0s - loss: 0.1292 - mae: 0.2897 - mse: 0.1292
86/86 [==============================] - 0s 5ms/step - loss: 0.1234 - mae: 0.2870 - mse: 0.1234 - val_loss: 0.0982 - val_mae: 0.2594 - val_mse: 0.0982
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1176 - mae: 0.2796 - mse: 0.1176
64/86 [=====================>........] - ETA: 0s - loss: 0.1160 - mae: 0.2756 - mse: 0.1160
86/86 [==============================] - 0s 5ms/step - loss: 0.1090 - mae: 0.2679 - mse: 0.1090 - val_loss: 0.0805 - val_mae: 0.2320 - val_mse: 0.0805
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0938 - mae: 0.2324 - mse: 0.0938
64/86 [=====================>........] - ETA: 0s - loss: 0.1041 - mae: 0.2461 - mse: 0.1041
86/86 [==============================] - 0s 5ms/step - loss: 0.0957 - mae: 0.2337 - mse: 0.0957 - val_loss: 0.0680 - val_mae: 0.1979 - val_mse: 0.0680
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0998 - mae: 0.2501 - mse: 0.0998
64/86 [=====================>........] - ETA: 0s - loss: 0.0970 - mae: 0.2465 - mse: 0.0970
86/86 [==============================] - 0s 5ms/step - loss: 0.0945 - mae: 0.2379 - mse: 0.0945 - val_loss: 0.0642 - val_mae: 0.1823 - val_mse: 0.0642
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1278 - mae: 0.2544 - mse: 0.1278
64/86 [=====================>........] - ETA: 0s - loss: 0.0991 - mae: 0.2300 - mse: 0.0991
86/86 [==============================] - 0s 5ms/step - loss: 0.0864 - mae: 0.2200 - mse: 0.0864 - val_loss: 0.0638 - val_mae: 0.1901 - val_mse: 0.0638
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1052 - mae: 0.2393 - mse: 0.1052
64/86 [=====================>........] - ETA: 0s - loss: 0.1026 - mae: 0.2468 - mse: 0.1026
86/86 [==============================] - 0s 5ms/step - loss: 0.0932 - mae: 0.2353 - mse: 0.0932 - val_loss: 0.0677 - val_mae: 0.2130 - val_mse: 0.0677
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1095 - mae: 0.2670 - mse: 0.1095
64/86 [=====================>........] - ETA: 0s - loss: 0.0952 - mae: 0.2435 - mse: 0.0952
86/86 [==============================] - 0s 5ms/step - loss: 0.0769 - mae: 0.2137 - mse: 0.0769 - val_loss: 0.0680 - val_mae: 0.2181 - val_mse: 0.0680
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0912 - mae: 0.2278 - mse: 0.0912
64/86 [=====================>........] - ETA: 0s - loss: 0.0856 - mae: 0.2221 - mse: 0.0856
86/86 [==============================] - 0s 5ms/step - loss: 0.0858 - mae: 0.2266 - mse: 0.0858 - val_loss: 0.0655 - val_mae: 0.2137 - val_mse: 0.0655
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0591 - mae: 0.1773 - mse: 0.0591
64/86 [=====================>........] - ETA: 0s - loss: 0.0640 - mae: 0.1883 - mse: 0.0640
86/86 [==============================] - 0s 5ms/step - loss: 0.0705 - mae: 0.1931 - mse: 0.0705 - val_loss: 0.0603 - val_mae: 0.1983 - val_mse: 0.0603
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0627 - mae: 0.1999 - mse: 0.0627
64/86 [=====================>........] - ETA: 0s - loss: 0.0851 - mae: 0.2299 - mse: 0.0851
86/86 [==============================] - 0s 5ms/step - loss: 0.0804 - mae: 0.2214 - mse: 0.0804 - val_loss: 0.0571 - val_mae: 0.1886 - val_mse: 0.0571
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1036 - mae: 0.2445 - mse: 0.1036
64/86 [=====================>........] - ETA: 0s - loss: 0.0922 - mae: 0.2271 - mse: 0.0922
86/86 [==============================] - 0s 5ms/step - loss: 0.0880 - mae: 0.2207 - mse: 0.0880 - val_loss: 0.0549 - val_mae: 0.1854 - val_mse: 0.0549
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0876 - mae: 0.2317 - mse: 0.0876
64/86 [=====================>........] - ETA: 0s - loss: 0.0746 - mae: 0.2044 - mse: 0.0746
86/86 [==============================] - 0s 5ms/step - loss: 0.0776 - mae: 0.2115 - mse: 0.0776 - val_loss: 0.0564 - val_mae: 0.1960 - val_mse: 0.0564
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0908 - mae: 0.2327 - mse: 0.0908
64/86 [=====================>........] - ETA: 0s - loss: 0.0795 - mae: 0.2093 - mse: 0.0795
86/86 [==============================] - 0s 5ms/step - loss: 0.0779 - mae: 0.2058 - mse: 0.0779 - val_loss: 0.0571 - val_mae: 0.2005 - val_mse: 0.0571
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1119 - mae: 0.2553 - mse: 0.1119
64/86 [=====================>........] - ETA: 0s - loss: 0.0873 - mae: 0.2251 - mse: 0.0873
86/86 [==============================] - 1s 7ms/step - loss: 0.0768 - mae: 0.2063 - mse: 0.0768 - val_loss: 0.0526 - val_mae: 0.1857 - val_mse: 0.0526
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0624 - mae: 0.1890 - mse: 0.0624
64/86 [=====================>........] - ETA: 0s - loss: 0.0590 - mae: 0.1840 - mse: 0.0590
86/86 [==============================] - 1s 8ms/step - loss: 0.0715 - mae: 0.2028 - mse: 0.0715 - val_loss: 0.0538 - val_mae: 0.1892 - val_mse: 0.0538
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1227 - mae: 0.2659 - mse: 0.1227
64/86 [=====================>........] - ETA: 0s - loss: 0.0955 - mae: 0.2320 - mse: 0.0955
86/86 [==============================] - 1s 9ms/step - loss: 0.0828 - mae: 0.2144 - mse: 0.0828 - val_loss: 0.0542 - val_mae: 0.1926 - val_mse: 0.0542
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0416 - mae: 0.1506 - mse: 0.0416
64/86 [=====================>........] - ETA: 0s - loss: 0.0552 - mae: 0.1651 - mse: 0.0552
86/86 [==============================] - 1s 7ms/step - loss: 0.0612 - mae: 0.1738 - mse: 0.0612 - val_loss: 0.0514 - val_mae: 0.1849 - val_mse: 0.0514
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0745 - mae: 0.2002 - mse: 0.0745
64/86 [=====================>........] - ETA: 0s - loss: 0.0737 - mae: 0.1982 - mse: 0.0737
86/86 [==============================] - 1s 7ms/step - loss: 0.0720 - mae: 0.1971 - mse: 0.0720 - val_loss: 0.0465 - val_mae: 0.1707 - val_mse: 0.0465
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0574 - mae: 0.1829 - mse: 0.0574
64/86 [=====================>........] - ETA: 0s - loss: 0.0536 - mae: 0.1664 - mse: 0.0536
86/86 [==============================] - 1s 7ms/step - loss: 0.0601 - mae: 0.1763 - mse: 0.0601 - val_loss: 0.0408 - val_mae: 0.1498 - val_mse: 0.0408
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0545 - mae: 0.1686 - mse: 0.0545
64/86 [=====================>........] - ETA: 0s - loss: 0.0448 - mae: 0.1518 - mse: 0.0448
86/86 [==============================] - 1s 7ms/step - loss: 0.0498 - mae: 0.1571 - mse: 0.0498 - val_loss: 0.0465 - val_mae: 0.1776 - val_mse: 0.0465
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0452 - mae: 0.1649 - mse: 0.0452
64/86 [=====================>........] - ETA: 0s - loss: 0.0675 - mae: 0.1945 - mse: 0.0675
86/86 [==============================] - 1s 7ms/step - loss: 0.0667 - mae: 0.1900 - mse: 0.0667 - val_loss: 0.0503 - val_mae: 0.1902 - val_mse: 0.0503
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0825 - mae: 0.2049 - mse: 0.0825
64/86 [=====================>........] - ETA: 0s - loss: 0.0698 - mae: 0.1862 - mse: 0.0698
86/86 [==============================] - 1s 7ms/step - loss: 0.0639 - mae: 0.1803 - mse: 0.0639 - val_loss: 0.0381 - val_mae: 0.1582 - val_mse: 0.0381
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0650 - mae: 0.2135 - mse: 0.0650
64/86 [=====================>........] - ETA: 0s - loss: 0.0638 - mae: 0.1933 - mse: 0.0638
86/86 [==============================] - 1s 7ms/step - loss: 0.0659 - mae: 0.1970 - mse: 0.0659 - val_loss: 0.0306 - val_mae: 0.1276 - val_mse: 0.0306
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0469 - mae: 0.1581 - mse: 0.0469
64/86 [=====================>........] - ETA: 0s - loss: 0.0470 - mae: 0.1550 - mse: 0.0470
86/86 [==============================] - 1s 7ms/step - loss: 0.0517 - mae: 0.1618 - mse: 0.0517 - val_loss: 0.0419 - val_mae: 0.1738 - val_mse: 0.0419
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0685 - mae: 0.1972 - mse: 0.0685
64/86 [=====================>........] - ETA: 0s - loss: 0.0647 - mae: 0.1935 - mse: 0.0647
86/86 [==============================] - 1s 7ms/step - loss: 0.0631 - mae: 0.1878 - mse: 0.0631 - val_loss: 0.0505 - val_mae: 0.2029 - val_mse: 0.0505
Saving trained model...
99
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         17.44984436]
average prediction= [4.678551]
baseline= 6.18
eachuser= [0. 0. 0. 0. 0. 2.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 8.724922180175781
['train-height-9.py', '0']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
155 7
2_155_65_9_csi_a9_19.dat
155 9
155 10
155 11
155 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
155 15
2_155_65_9_csi_a9_7.dat
155 17
155 18
155 19
2_155_65_9_csi_a9_21.dat
155 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
155 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_9_csi_a9_29.dat
165 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
165 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
165 52
1_165_65_9_csi_a9_16.dat
165 54
165 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
165 58
165 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
165 62
165 63
1_165_65_9_csi_a9_5.dat
165 65
165 66
1_165_65_9_csi_a9_18.dat
165 68
165 69
1_165_65_9_csi_a9_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_9_csi_a9_22.dat
165 90
165 91
165 92
165 93
165 94
2_165_50_9_csi_a9_26.dat
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
1_180_85_9_csi_a9_7.dat
180 132
180 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
180 140
180 141
180 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
180 146
180 147
180 148
180 149
180 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
180 153
180 154
180 155
1_180_85_9_csi_a9_18.dat
180 157
180 158
180 159
180 160
180 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
180 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
180 171
1_180_75_9_csi_a9_25.dat
180 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
180 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
180 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
180 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
173 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
173 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
173 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
173 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
173 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 170 170 170 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 173 173 173 173 173]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 1s - loss: 0.4007 - mae: 0.5693 - mse: 0.4007
64/86 [=====================>........] - ETA: 0s - loss: 0.3268 - mae: 0.4946 - mse: 0.3268
86/86 [==============================] - 1s 16ms/step - loss: 0.2855 - mae: 0.4522 - mse: 0.2855 - val_loss: 0.1592 - val_mae: 0.3512 - val_mse: 0.1592
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1297 - mae: 0.3169 - mse: 0.1297
64/86 [=====================>........] - ETA: 0s - loss: 0.1459 - mae: 0.3323 - mse: 0.1459
86/86 [==============================] - 1s 9ms/step - loss: 0.1675 - mae: 0.3518 - mse: 0.1675 - val_loss: 0.1586 - val_mae: 0.3623 - val_mse: 0.1586
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.2315 - mae: 0.4031 - mse: 0.2315
64/86 [=====================>........] - ETA: 0s - loss: 0.1952 - mae: 0.3630 - mse: 0.1952
86/86 [==============================] - 1s 9ms/step - loss: 0.1707 - mae: 0.3374 - mse: 0.1707 - val_loss: 0.1660 - val_mae: 0.3364 - val_mse: 0.1660
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0921 - mae: 0.2555 - mse: 0.0921
64/86 [=====================>........] - ETA: 0s - loss: 0.1122 - mae: 0.2723 - mse: 0.1122
86/86 [==============================] - 1s 9ms/step - loss: 0.1256 - mae: 0.2888 - mse: 0.1256 - val_loss: 0.2096 - val_mae: 0.3700 - val_mse: 0.2096
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1216 - mae: 0.2714 - mse: 0.1216
64/86 [=====================>........] - ETA: 0s - loss: 0.1347 - mae: 0.2938 - mse: 0.1347
86/86 [==============================] - 1s 9ms/step - loss: 0.1452 - mae: 0.3077 - mse: 0.1452 - val_loss: 0.2006 - val_mae: 0.3603 - val_mse: 0.2006
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1662 - mae: 0.3193 - mse: 0.1662
64/86 [=====================>........] - ETA: 0s - loss: 0.1411 - mae: 0.2922 - mse: 0.1411
86/86 [==============================] - 1s 9ms/step - loss: 0.1285 - mae: 0.2754 - mse: 0.1285 - val_loss: 0.1613 - val_mae: 0.3272 - val_mse: 0.1613
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0715 - mae: 0.2199 - mse: 0.0715
64/86 [=====================>........] - ETA: 0s - loss: 0.0959 - mae: 0.2515 - mse: 0.0959
86/86 [==============================] - 1s 8ms/step - loss: 0.0887 - mae: 0.2436 - mse: 0.0887 - val_loss: 0.1331 - val_mae: 0.3221 - val_mse: 0.1331
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0652 - mae: 0.2049 - mse: 0.0652
64/86 [=====================>........] - ETA: 0s - loss: 0.0890 - mae: 0.2418 - mse: 0.0890
86/86 [==============================] - 1s 7ms/step - loss: 0.0950 - mae: 0.2513 - mse: 0.0950 - val_loss: 0.1246 - val_mae: 0.3171 - val_mse: 0.1246
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1066 - mae: 0.2757 - mse: 0.1066
64/86 [=====================>........] - ETA: 0s - loss: 0.1175 - mae: 0.2996 - mse: 0.1175
86/86 [==============================] - 1s 7ms/step - loss: 0.1109 - mae: 0.2875 - mse: 0.1109 - val_loss: 0.1292 - val_mae: 0.3062 - val_mse: 0.1292
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1047 - mae: 0.2819 - mse: 0.1047
64/86 [=====================>........] - ETA: 0s - loss: 0.0836 - mae: 0.2437 - mse: 0.0836
86/86 [==============================] - 1s 7ms/step - loss: 0.0870 - mae: 0.2434 - mse: 0.0870 - val_loss: 0.1430 - val_mae: 0.3075 - val_mse: 0.1430
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0513 - mae: 0.1808 - mse: 0.0513
64/86 [=====================>........] - ETA: 0s - loss: 0.0762 - mae: 0.2235 - mse: 0.0762
86/86 [==============================] - 1s 9ms/step - loss: 0.0789 - mae: 0.2273 - mse: 0.0789 - val_loss: 0.1393 - val_mae: 0.3020 - val_mse: 0.1393
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0806 - mae: 0.2328 - mse: 0.0806
64/86 [=====================>........] - ETA: 0s - loss: 0.0780 - mae: 0.2184 - mse: 0.0780
86/86 [==============================] - 1s 8ms/step - loss: 0.0736 - mae: 0.2110 - mse: 0.0736 - val_loss: 0.1256 - val_mae: 0.2896 - val_mse: 0.1256
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0795 - mae: 0.2313 - mse: 0.0795
64/86 [=====================>........] - ETA: 0s - loss: 0.0968 - mae: 0.2547 - mse: 0.0968
86/86 [==============================] - 1s 7ms/step - loss: 0.0862 - mae: 0.2392 - mse: 0.0862 - val_loss: 0.1145 - val_mae: 0.2770 - val_mse: 0.1145
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0674 - mae: 0.2136 - mse: 0.0674
64/86 [=====================>........] - ETA: 0s - loss: 0.0732 - mae: 0.2184 - mse: 0.0732
86/86 [==============================] - 1s 8ms/step - loss: 0.0843 - mae: 0.2370 - mse: 0.0843 - val_loss: 0.1095 - val_mae: 0.2676 - val_mse: 0.1095
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0444 - mae: 0.1656 - mse: 0.0444
64/86 [=====================>........] - ETA: 0s - loss: 0.0566 - mae: 0.1906 - mse: 0.0566
86/86 [==============================] - 1s 7ms/step - loss: 0.0642 - mae: 0.1981 - mse: 0.0642 - val_loss: 0.1167 - val_mae: 0.2848 - val_mse: 0.1167
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0741 - mae: 0.2011 - mse: 0.0741
64/86 [=====================>........] - ETA: 0s - loss: 0.0723 - mae: 0.2034 - mse: 0.0723
86/86 [==============================] - 1s 8ms/step - loss: 0.0709 - mae: 0.2005 - mse: 0.0709 - val_loss: 0.1286 - val_mae: 0.3086 - val_mse: 0.1286
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0663 - mae: 0.1997 - mse: 0.0663
64/86 [=====================>........] - ETA: 0s - loss: 0.0609 - mae: 0.1930 - mse: 0.0609
86/86 [==============================] - 1s 8ms/step - loss: 0.0690 - mae: 0.2064 - mse: 0.0690 - val_loss: 0.1194 - val_mae: 0.2984 - val_mse: 0.1194
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0795 - mae: 0.2147 - mse: 0.0795
64/86 [=====================>........] - ETA: 0s - loss: 0.0721 - mae: 0.2032 - mse: 0.0721
86/86 [==============================] - 1s 8ms/step - loss: 0.0664 - mae: 0.1911 - mse: 0.0664 - val_loss: 0.1054 - val_mae: 0.2772 - val_mse: 0.1054
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0448 - mae: 0.1567 - mse: 0.0448
64/86 [=====================>........] - ETA: 0s - loss: 0.0513 - mae: 0.1758 - mse: 0.0513
86/86 [==============================] - 1s 7ms/step - loss: 0.0706 - mae: 0.2015 - mse: 0.0706 - val_loss: 0.1082 - val_mae: 0.2862 - val_mse: 0.1082
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0395 - mae: 0.1584 - mse: 0.0395
64/86 [=====================>........] - ETA: 0s - loss: 0.0438 - mae: 0.1625 - mse: 0.0438
86/86 [==============================] - 1s 8ms/step - loss: 0.0463 - mae: 0.1654 - mse: 0.0463 - val_loss: 0.1213 - val_mae: 0.3092 - val_mse: 0.1213
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0670 - mae: 0.2036 - mse: 0.0670
64/86 [=====================>........] - ETA: 0s - loss: 0.0697 - mae: 0.2019 - mse: 0.0697
86/86 [==============================] - 1s 7ms/step - loss: 0.0620 - mae: 0.1900 - mse: 0.0620 - val_loss: 0.1138 - val_mae: 0.2993 - val_mse: 0.1138
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0499 - mae: 0.1567 - mse: 0.0499
64/86 [=====================>........] - ETA: 0s - loss: 0.0549 - mae: 0.1686 - mse: 0.0549
86/86 [==============================] - 1s 8ms/step - loss: 0.0541 - mae: 0.1681 - mse: 0.0541 - val_loss: 0.1050 - val_mae: 0.2848 - val_mse: 0.1050
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0483 - mae: 0.1710 - mse: 0.0483
64/86 [=====================>........] - ETA: 0s - loss: 0.0506 - mae: 0.1716 - mse: 0.0506
86/86 [==============================] - 1s 7ms/step - loss: 0.0574 - mae: 0.1831 - mse: 0.0574 - val_loss: 0.0993 - val_mae: 0.2741 - val_mse: 0.0993
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0537 - mae: 0.1908 - mse: 0.0537
64/86 [=====================>........] - ETA: 0s - loss: 0.0629 - mae: 0.1960 - mse: 0.0629
86/86 [==============================] - 1s 8ms/step - loss: 0.0593 - mae: 0.1920 - mse: 0.0593 - val_loss: 0.1144 - val_mae: 0.2993 - val_mse: 0.1144
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0483 - mae: 0.1664 - mse: 0.0483
64/86 [=====================>........] - ETA: 0s - loss: 0.0473 - mae: 0.1665 - mse: 0.0473
86/86 [==============================] - 0s 5ms/step - loss: 0.0548 - mae: 0.1799 - mse: 0.0548 - val_loss: 0.1258 - val_mae: 0.3137 - val_mse: 0.1258
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0495 - mae: 0.1701 - mse: 0.0495
64/86 [=====================>........] - ETA: 0s - loss: 0.0523 - mae: 0.1784 - mse: 0.0523
86/86 [==============================] - 0s 5ms/step - loss: 0.0485 - mae: 0.1697 - mse: 0.0485 - val_loss: 0.0972 - val_mae: 0.2678 - val_mse: 0.0972
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0332 - mae: 0.1307 - mse: 0.0332
64/86 [=====================>........] - ETA: 0s - loss: 0.0446 - mae: 0.1595 - mse: 0.0446
86/86 [==============================] - 0s 6ms/step - loss: 0.0415 - mae: 0.1545 - mse: 0.0415 - val_loss: 0.0886 - val_mae: 0.2489 - val_mse: 0.0886
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0551 - mae: 0.1850 - mse: 0.0551
64/86 [=====================>........] - ETA: 0s - loss: 0.0476 - mae: 0.1650 - mse: 0.0476
86/86 [==============================] - 0s 5ms/step - loss: 0.0491 - mae: 0.1670 - mse: 0.0491 - val_loss: 0.0973 - val_mae: 0.2665 - val_mse: 0.0973
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0358 - mae: 0.1399 - mse: 0.0358
64/86 [=====================>........] - ETA: 0s - loss: 0.0414 - mae: 0.1435 - mse: 0.0414
86/86 [==============================] - 0s 5ms/step - loss: 0.0438 - mae: 0.1545 - mse: 0.0438 - val_loss: 0.1073 - val_mae: 0.2831 - val_mse: 0.1073
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0455 - mae: 0.1591 - mse: 0.0455
64/86 [=====================>........] - ETA: 0s - loss: 0.0404 - mae: 0.1538 - mse: 0.0404
86/86 [==============================] - 0s 5ms/step - loss: 0.0472 - mae: 0.1629 - mse: 0.0472 - val_loss: 0.0981 - val_mae: 0.2685 - val_mse: 0.0981
Saving trained model...
99
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         35.02005005]
average prediction= [4.6491237]
baseline= 6.34
eachuser= [0. 0. 0. 0. 0. 3.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 11.673350016276041
['train-height-9.py', '0']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
155 7
2_155_65_9_csi_a9_19.dat
155 9
155 10
155 11
155 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
155 15
2_155_65_9_csi_a9_7.dat
155 17
155 18
155 19
2_155_65_9_csi_a9_21.dat
155 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
155 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_9_csi_a9_29.dat
165 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
165 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
165 52
1_165_65_9_csi_a9_16.dat
165 54
165 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
165 58
165 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
165 62
165 63
1_165_65_9_csi_a9_5.dat
165 65
165 66
1_165_65_9_csi_a9_18.dat
165 68
165 69
1_165_65_9_csi_a9_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_9_csi_a9_22.dat
165 90
165 91
165 92
165 93
165 94
2_165_50_9_csi_a9_26.dat
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
1_180_85_9_csi_a9_7.dat
180 132
180 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
180 140
180 141
180 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
180 146
180 147
180 148
180 149
180 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
180 153
180 154
180 155
1_180_85_9_csi_a9_18.dat
180 157
180 158
180 159
180 160
180 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
180 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
180 171
1_180_75_9_csi_a9_25.dat
180 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
180 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
180 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
180 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
173 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
173 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
173 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
173 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
173 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 170 170 170 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 173 173 173 173 173]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.3072 - mae: 0.4707 - mse: 0.3072
64/86 [=====================>........] - ETA: 0s - loss: 0.2263 - mae: 0.3995 - mse: 0.2263
86/86 [==============================] - 1s 11ms/step - loss: 0.2038 - mae: 0.3784 - mse: 0.2038 - val_loss: 0.1090 - val_mae: 0.2954 - val_mse: 0.1090
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1993 - mae: 0.3762 - mse: 0.1993
64/86 [=====================>........] - ETA: 0s - loss: 0.1712 - mae: 0.3413 - mse: 0.1712
86/86 [==============================] - 1s 7ms/step - loss: 0.1743 - mae: 0.3486 - mse: 0.1743 - val_loss: 0.1364 - val_mae: 0.3301 - val_mse: 0.1364
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1112 - mae: 0.2929 - mse: 0.1112
64/86 [=====================>........] - ETA: 0s - loss: 0.0966 - mae: 0.2658 - mse: 0.0966
86/86 [==============================] - 1s 7ms/step - loss: 0.1077 - mae: 0.2848 - mse: 0.1077 - val_loss: 0.1926 - val_mae: 0.3993 - val_mse: 0.1926
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1104 - mae: 0.2699 - mse: 0.1104
64/86 [=====================>........] - ETA: 0s - loss: 0.0994 - mae: 0.2553 - mse: 0.0994
86/86 [==============================] - 1s 7ms/step - loss: 0.1157 - mae: 0.2782 - mse: 0.1157 - val_loss: 0.1919 - val_mae: 0.4001 - val_mse: 0.1919
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1365 - mae: 0.3051 - mse: 0.1365
64/86 [=====================>........] - ETA: 0s - loss: 0.1272 - mae: 0.3039 - mse: 0.1272
86/86 [==============================] - 1s 7ms/step - loss: 0.1089 - mae: 0.2713 - mse: 0.1089 - val_loss: 0.1365 - val_mae: 0.3381 - val_mse: 0.1365
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0991 - mae: 0.2557 - mse: 0.0991
64/86 [=====================>........] - ETA: 0s - loss: 0.0855 - mae: 0.2404 - mse: 0.0855
86/86 [==============================] - 1s 7ms/step - loss: 0.0985 - mae: 0.2600 - mse: 0.0985 - val_loss: 0.1068 - val_mae: 0.2903 - val_mse: 0.1068
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0884 - mae: 0.2518 - mse: 0.0884
64/86 [=====================>........] - ETA: 0s - loss: 0.0925 - mae: 0.2473 - mse: 0.0925
86/86 [==============================] - 1s 7ms/step - loss: 0.0956 - mae: 0.2466 - mse: 0.0956 - val_loss: 0.1136 - val_mae: 0.3072 - val_mse: 0.1136
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0999 - mae: 0.2607 - mse: 0.0999
64/86 [=====================>........] - ETA: 0s - loss: 0.0989 - mae: 0.2531 - mse: 0.0989
86/86 [==============================] - 1s 7ms/step - loss: 0.0941 - mae: 0.2477 - mse: 0.0941 - val_loss: 0.1307 - val_mae: 0.3353 - val_mse: 0.1307
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0964 - mae: 0.2508 - mse: 0.0964
64/86 [=====================>........] - ETA: 0s - loss: 0.0918 - mae: 0.2454 - mse: 0.0918
86/86 [==============================] - 1s 7ms/step - loss: 0.0926 - mae: 0.2460 - mse: 0.0926 - val_loss: 0.1378 - val_mae: 0.3440 - val_mse: 0.1378
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0906 - mae: 0.2480 - mse: 0.0906
64/86 [=====================>........] - ETA: 0s - loss: 0.0841 - mae: 0.2449 - mse: 0.0841
86/86 [==============================] - 1s 7ms/step - loss: 0.0773 - mae: 0.2277 - mse: 0.0773 - val_loss: 0.1184 - val_mae: 0.3162 - val_mse: 0.1184
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0933 - mae: 0.2396 - mse: 0.0933
64/86 [=====================>........] - ETA: 0s - loss: 0.0790 - mae: 0.2221 - mse: 0.0790
86/86 [==============================] - 1s 7ms/step - loss: 0.0820 - mae: 0.2254 - mse: 0.0820 - val_loss: 0.1009 - val_mae: 0.2869 - val_mse: 0.1009
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0684 - mae: 0.2026 - mse: 0.0684
64/86 [=====================>........] - ETA: 0s - loss: 0.0774 - mae: 0.2104 - mse: 0.0774
86/86 [==============================] - 1s 7ms/step - loss: 0.0714 - mae: 0.2017 - mse: 0.0714 - val_loss: 0.0937 - val_mae: 0.2723 - val_mse: 0.0937
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1129 - mae: 0.2770 - mse: 0.1129
64/86 [=====================>........] - ETA: 0s - loss: 0.0879 - mae: 0.2362 - mse: 0.0879
86/86 [==============================] - 1s 7ms/step - loss: 0.0772 - mae: 0.2144 - mse: 0.0772 - val_loss: 0.1064 - val_mae: 0.2953 - val_mse: 0.1064
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0657 - mae: 0.1893 - mse: 0.0657
64/86 [=====================>........] - ETA: 0s - loss: 0.0782 - mae: 0.2163 - mse: 0.0782
86/86 [==============================] - 1s 6ms/step - loss: 0.0667 - mae: 0.1944 - mse: 0.0667 - val_loss: 0.1167 - val_mae: 0.3092 - val_mse: 0.1167
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0615 - mae: 0.2003 - mse: 0.0615
64/86 [=====================>........] - ETA: 0s - loss: 0.0620 - mae: 0.1948 - mse: 0.0620
86/86 [==============================] - 0s 5ms/step - loss: 0.0670 - mae: 0.2026 - mse: 0.0670 - val_loss: 0.1089 - val_mae: 0.2957 - val_mse: 0.1089
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0672 - mae: 0.2136 - mse: 0.0672
64/86 [=====================>........] - ETA: 0s - loss: 0.0606 - mae: 0.2032 - mse: 0.0606
86/86 [==============================] - 0s 5ms/step - loss: 0.0562 - mae: 0.1919 - mse: 0.0562 - val_loss: 0.0870 - val_mae: 0.2582 - val_mse: 0.0870
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0620 - mae: 0.1868 - mse: 0.0620
64/86 [=====================>........] - ETA: 0s - loss: 0.0646 - mae: 0.1889 - mse: 0.0646
86/86 [==============================] - 0s 5ms/step - loss: 0.0647 - mae: 0.1916 - mse: 0.0647 - val_loss: 0.0845 - val_mae: 0.2537 - val_mse: 0.0845
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0435 - mae: 0.1562 - mse: 0.0435
64/86 [=====================>........] - ETA: 0s - loss: 0.0550 - mae: 0.1757 - mse: 0.0550
86/86 [==============================] - 0s 5ms/step - loss: 0.0516 - mae: 0.1691 - mse: 0.0516 - val_loss: 0.0769 - val_mae: 0.2374 - val_mse: 0.0769
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0570 - mae: 0.1745 - mse: 0.0570
64/86 [=====================>........] - ETA: 0s - loss: 0.0431 - mae: 0.1513 - mse: 0.0431
86/86 [==============================] - 0s 5ms/step - loss: 0.0523 - mae: 0.1668 - mse: 0.0523 - val_loss: 0.0732 - val_mae: 0.2253 - val_mse: 0.0732
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0459 - mae: 0.1514 - mse: 0.0459
64/86 [=====================>........] - ETA: 0s - loss: 0.0454 - mae: 0.1564 - mse: 0.0454
86/86 [==============================] - 0s 5ms/step - loss: 0.0433 - mae: 0.1576 - mse: 0.0433 - val_loss: 0.0844 - val_mae: 0.2403 - val_mse: 0.0844
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0494 - mae: 0.1692 - mse: 0.0494
64/86 [=====================>........] - ETA: 0s - loss: 0.0486 - mae: 0.1627 - mse: 0.0486
86/86 [==============================] - 0s 5ms/step - loss: 0.0528 - mae: 0.1693 - mse: 0.0528 - val_loss: 0.0759 - val_mae: 0.2222 - val_mse: 0.0759
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0640 - mae: 0.1696 - mse: 0.0640
64/86 [=====================>........] - ETA: 0s - loss: 0.0478 - mae: 0.1538 - mse: 0.0478
86/86 [==============================] - 0s 5ms/step - loss: 0.0448 - mae: 0.1488 - mse: 0.0448 - val_loss: 0.0593 - val_mae: 0.1846 - val_mse: 0.0593
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0402 - mae: 0.1535 - mse: 0.0402
64/86 [=====================>........] - ETA: 0s - loss: 0.0490 - mae: 0.1676 - mse: 0.0490
86/86 [==============================] - 0s 5ms/step - loss: 0.0495 - mae: 0.1655 - mse: 0.0495 - val_loss: 0.0658 - val_mae: 0.2127 - val_mse: 0.0658
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0373 - mae: 0.1205 - mse: 0.0373
64/86 [=====================>........] - ETA: 0s - loss: 0.0295 - mae: 0.1114 - mse: 0.0295
86/86 [==============================] - 0s 5ms/step - loss: 0.0381 - mae: 0.1332 - mse: 0.0381 - val_loss: 0.0588 - val_mae: 0.1906 - val_mse: 0.0588
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0411 - mae: 0.1558 - mse: 0.0411
64/86 [=====================>........] - ETA: 0s - loss: 0.0413 - mae: 0.1533 - mse: 0.0413
86/86 [==============================] - 0s 5ms/step - loss: 0.0453 - mae: 0.1548 - mse: 0.0453 - val_loss: 0.0549 - val_mae: 0.1703 - val_mse: 0.0549
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0278 - mae: 0.1253 - mse: 0.0278
64/86 [=====================>........] - ETA: 0s - loss: 0.0356 - mae: 0.1425 - mse: 0.0356
86/86 [==============================] - 0s 5ms/step - loss: 0.0399 - mae: 0.1444 - mse: 0.0399 - val_loss: 0.0615 - val_mae: 0.1957 - val_mse: 0.0615
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0372 - mae: 0.1422 - mse: 0.0372
64/86 [=====================>........] - ETA: 0s - loss: 0.0420 - mae: 0.1550 - mse: 0.0420
86/86 [==============================] - 0s 5ms/step - loss: 0.0388 - mae: 0.1455 - mse: 0.0388 - val_loss: 0.0611 - val_mae: 0.2044 - val_mse: 0.0611
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0411 - mae: 0.1476 - mse: 0.0411
64/86 [=====================>........] - ETA: 0s - loss: 0.0409 - mae: 0.1496 - mse: 0.0409
86/86 [==============================] - 0s 5ms/step - loss: 0.0378 - mae: 0.1452 - mse: 0.0378 - val_loss: 0.0575 - val_mae: 0.1915 - val_mse: 0.0575
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0360 - mae: 0.1434 - mse: 0.0360
64/86 [=====================>........] - ETA: 0s - loss: 0.0395 - mae: 0.1491 - mse: 0.0395
86/86 [==============================] - 0s 5ms/step - loss: 0.0377 - mae: 0.1467 - mse: 0.0377 - val_loss: 0.0617 - val_mae: 0.1985 - val_mse: 0.0617
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0440 - mae: 0.1389 - mse: 0.0440
64/86 [=====================>........] - ETA: 0s - loss: 0.0360 - mae: 0.1288 - mse: 0.0360
86/86 [==============================] - 0s 5ms/step - loss: 0.0338 - mae: 0.1288 - mse: 0.0338 - val_loss: 0.0497 - val_mae: 0.1603 - val_mse: 0.0497
Saving trained model...
99
Testing...
heightdiff= [0.         0.         0.         0.         0.         4.28846741]
average prediction= [3.2743483]
baseline= 6.06
eachuser= [0. 0. 0. 0. 0. 1.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 4.2884674072265625
['train-height-9.py', '0']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
155 7
2_155_65_9_csi_a9_19.dat
155 9
155 10
155 11
155 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
155 15
2_155_65_9_csi_a9_7.dat
155 17
155 18
155 19
2_155_65_9_csi_a9_21.dat
155 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
155 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_9_csi_a9_29.dat
165 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
165 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
165 52
1_165_65_9_csi_a9_16.dat
165 54
165 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
165 58
165 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
165 62
165 63
1_165_65_9_csi_a9_5.dat
165 65
165 66
1_165_65_9_csi_a9_18.dat
165 68
165 69
1_165_65_9_csi_a9_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_9_csi_a9_22.dat
165 90
165 91
165 92
165 93
165 94
2_165_50_9_csi_a9_26.dat
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
1_180_85_9_csi_a9_7.dat
180 132
180 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
180 140
180 141
180 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
180 146
180 147
180 148
180 149
180 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
180 153
180 154
180 155
1_180_85_9_csi_a9_18.dat
180 157
180 158
180 159
180 160
180 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
180 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
180 171
1_180_75_9_csi_a9_25.dat
180 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
180 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
180 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
180 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
173 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
173 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
173 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
173 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
173 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 170 170 170 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 173 173 173 173 173]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.4527 - mae: 0.5815 - mse: 0.4527
64/86 [=====================>........] - ETA: 0s - loss: 0.4249 - mae: 0.5745 - mse: 0.4249
86/86 [==============================] - 1s 11ms/step - loss: 0.4067 - mae: 0.5684 - mse: 0.4067 - val_loss: 0.1847 - val_mae: 0.3680 - val_mse: 0.1847
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.2540 - mae: 0.4388 - mse: 0.2540
64/86 [=====================>........] - ETA: 0s - loss: 0.1766 - mae: 0.3531 - mse: 0.1766
86/86 [==============================] - 0s 5ms/step - loss: 0.1657 - mae: 0.3477 - mse: 0.1657 - val_loss: 0.0569 - val_mae: 0.2232 - val_mse: 0.0569
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1138 - mae: 0.2913 - mse: 0.1138
64/86 [=====================>........] - ETA: 0s - loss: 0.1588 - mae: 0.3276 - mse: 0.1588
86/86 [==============================] - 0s 5ms/step - loss: 0.1628 - mae: 0.3292 - mse: 0.1628 - val_loss: 0.0659 - val_mae: 0.2353 - val_mse: 0.0659
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.2008 - mae: 0.3728 - mse: 0.2008
64/86 [=====================>........] - ETA: 0s - loss: 0.1540 - mae: 0.3194 - mse: 0.1540
86/86 [==============================] - 0s 5ms/step - loss: 0.1447 - mae: 0.3064 - mse: 0.1447 - val_loss: 0.0474 - val_mae: 0.1863 - val_mse: 0.0474
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1051 - mae: 0.2644 - mse: 0.1051
64/86 [=====================>........] - ETA: 0s - loss: 0.1041 - mae: 0.2646 - mse: 0.1041
86/86 [==============================] - 0s 5ms/step - loss: 0.1137 - mae: 0.2789 - mse: 0.1137 - val_loss: 0.0735 - val_mae: 0.1995 - val_mse: 0.0735
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1444 - mae: 0.3303 - mse: 0.1444
64/86 [=====================>........] - ETA: 0s - loss: 0.1089 - mae: 0.2766 - mse: 0.1089
86/86 [==============================] - 0s 5ms/step - loss: 0.1069 - mae: 0.2737 - mse: 0.1069 - val_loss: 0.0756 - val_mae: 0.2063 - val_mse: 0.0756
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0723 - mae: 0.2277 - mse: 0.0723
64/86 [=====================>........] - ETA: 0s - loss: 0.0950 - mae: 0.2593 - mse: 0.0950
86/86 [==============================] - 0s 5ms/step - loss: 0.1051 - mae: 0.2736 - mse: 0.1051 - val_loss: 0.0576 - val_mae: 0.1896 - val_mse: 0.0576
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1035 - mae: 0.2624 - mse: 0.1035
64/86 [=====================>........] - ETA: 0s - loss: 0.0967 - mae: 0.2477 - mse: 0.0967
86/86 [==============================] - 0s 5ms/step - loss: 0.0892 - mae: 0.2366 - mse: 0.0892 - val_loss: 0.0435 - val_mae: 0.1725 - val_mse: 0.0435
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0917 - mae: 0.2440 - mse: 0.0917
64/86 [=====================>........] - ETA: 0s - loss: 0.0881 - mae: 0.2340 - mse: 0.0881
86/86 [==============================] - 0s 5ms/step - loss: 0.0811 - mae: 0.2271 - mse: 0.0811 - val_loss: 0.0392 - val_mae: 0.1667 - val_mse: 0.0392
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0940 - mae: 0.2430 - mse: 0.0940
64/86 [=====================>........] - ETA: 0s - loss: 0.1028 - mae: 0.2609 - mse: 0.1028
86/86 [==============================] - 0s 6ms/step - loss: 0.0896 - mae: 0.2439 - mse: 0.0896 - val_loss: 0.0379 - val_mae: 0.1632 - val_mse: 0.0379
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0845 - mae: 0.2316 - mse: 0.0845
64/86 [=====================>........] - ETA: 0s - loss: 0.0793 - mae: 0.2282 - mse: 0.0793
86/86 [==============================] - 0s 6ms/step - loss: 0.0862 - mae: 0.2352 - mse: 0.0862 - val_loss: 0.0445 - val_mae: 0.1807 - val_mse: 0.0445
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0654 - mae: 0.1952 - mse: 0.0654
64/86 [=====================>........] - ETA: 0s - loss: 0.0699 - mae: 0.2090 - mse: 0.0699
86/86 [==============================] - 1s 6ms/step - loss: 0.0746 - mae: 0.2107 - mse: 0.0746 - val_loss: 0.0522 - val_mae: 0.1924 - val_mse: 0.0522
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0579 - mae: 0.1958 - mse: 0.0579
64/86 [=====================>........] - ETA: 0s - loss: 0.0752 - mae: 0.2190 - mse: 0.0752
86/86 [==============================] - 0s 5ms/step - loss: 0.0862 - mae: 0.2349 - mse: 0.0862 - val_loss: 0.0474 - val_mae: 0.1874 - val_mse: 0.0474
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0802 - mae: 0.2304 - mse: 0.0802
64/86 [=====================>........] - ETA: 0s - loss: 0.0733 - mae: 0.2186 - mse: 0.0733
86/86 [==============================] - 0s 5ms/step - loss: 0.0720 - mae: 0.2129 - mse: 0.0720 - val_loss: 0.0319 - val_mae: 0.1642 - val_mse: 0.0319
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0952 - mae: 0.2232 - mse: 0.0952
64/86 [=====================>........] - ETA: 0s - loss: 0.0653 - mae: 0.1799 - mse: 0.0653
86/86 [==============================] - 0s 5ms/step - loss: 0.0680 - mae: 0.1882 - mse: 0.0680 - val_loss: 0.0276 - val_mae: 0.1535 - val_mse: 0.0276
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0766 - mae: 0.2126 - mse: 0.0766
64/86 [=====================>........] - ETA: 0s - loss: 0.0739 - mae: 0.2056 - mse: 0.0739
86/86 [==============================] - 0s 5ms/step - loss: 0.0765 - mae: 0.2083 - mse: 0.0765 - val_loss: 0.0292 - val_mae: 0.1586 - val_mse: 0.0292
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0673 - mae: 0.1925 - mse: 0.0673
64/86 [=====================>........] - ETA: 0s - loss: 0.0667 - mae: 0.1973 - mse: 0.0667
86/86 [==============================] - 0s 5ms/step - loss: 0.0615 - mae: 0.1919 - mse: 0.0615 - val_loss: 0.0310 - val_mae: 0.1624 - val_mse: 0.0310
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0481 - mae: 0.1651 - mse: 0.0481
64/86 [=====================>........] - ETA: 0s - loss: 0.0540 - mae: 0.1762 - mse: 0.0540
86/86 [==============================] - 0s 5ms/step - loss: 0.0574 - mae: 0.1803 - mse: 0.0574 - val_loss: 0.0266 - val_mae: 0.1520 - val_mse: 0.0266
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0529 - mae: 0.1726 - mse: 0.0529
64/86 [=====================>........] - ETA: 0s - loss: 0.0529 - mae: 0.1758 - mse: 0.0529
86/86 [==============================] - 0s 5ms/step - loss: 0.0609 - mae: 0.1906 - mse: 0.0609 - val_loss: 0.0272 - val_mae: 0.1532 - val_mse: 0.0272
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0482 - mae: 0.1612 - mse: 0.0482
64/86 [=====================>........] - ETA: 0s - loss: 0.0676 - mae: 0.2024 - mse: 0.0676
86/86 [==============================] - 0s 5ms/step - loss: 0.0617 - mae: 0.1887 - mse: 0.0617 - val_loss: 0.0264 - val_mae: 0.1509 - val_mse: 0.0264
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0712 - mae: 0.2116 - mse: 0.0712
64/86 [=====================>........] - ETA: 0s - loss: 0.0634 - mae: 0.1930 - mse: 0.0634
86/86 [==============================] - 0s 5ms/step - loss: 0.0543 - mae: 0.1759 - mse: 0.0543 - val_loss: 0.0242 - val_mae: 0.1452 - val_mse: 0.0242
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0581 - mae: 0.1756 - mse: 0.0581
64/86 [=====================>........] - ETA: 0s - loss: 0.0478 - mae: 0.1660 - mse: 0.0478
86/86 [==============================] - 0s 5ms/step - loss: 0.0533 - mae: 0.1790 - mse: 0.0533 - val_loss: 0.0226 - val_mae: 0.1397 - val_mse: 0.0226
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0526 - mae: 0.1750 - mse: 0.0526
64/86 [=====================>........] - ETA: 0s - loss: 0.0456 - mae: 0.1667 - mse: 0.0456
86/86 [==============================] - 0s 5ms/step - loss: 0.0442 - mae: 0.1634 - mse: 0.0442 - val_loss: 0.0213 - val_mae: 0.1361 - val_mse: 0.0213
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0410 - mae: 0.1598 - mse: 0.0410
64/86 [=====================>........] - ETA: 0s - loss: 0.0507 - mae: 0.1776 - mse: 0.0507
86/86 [==============================] - 0s 5ms/step - loss: 0.0580 - mae: 0.1889 - mse: 0.0580 - val_loss: 0.0210 - val_mae: 0.1367 - val_mse: 0.0210
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0552 - mae: 0.1814 - mse: 0.0552
64/86 [=====================>........] - ETA: 0s - loss: 0.0480 - mae: 0.1646 - mse: 0.0480
86/86 [==============================] - 0s 5ms/step - loss: 0.0464 - mae: 0.1623 - mse: 0.0464 - val_loss: 0.0252 - val_mae: 0.1480 - val_mse: 0.0252
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0638 - mae: 0.2054 - mse: 0.0638
64/86 [=====================>........] - ETA: 0s - loss: 0.0478 - mae: 0.1649 - mse: 0.0478
86/86 [==============================] - 0s 5ms/step - loss: 0.0513 - mae: 0.1733 - mse: 0.0513 - val_loss: 0.0191 - val_mae: 0.1326 - val_mse: 0.0191
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0503 - mae: 0.1788 - mse: 0.0503
64/86 [=====================>........] - ETA: 0s - loss: 0.0466 - mae: 0.1690 - mse: 0.0466
86/86 [==============================] - 0s 5ms/step - loss: 0.0413 - mae: 0.1549 - mse: 0.0413 - val_loss: 0.0165 - val_mae: 0.1196 - val_mse: 0.0165
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0430 - mae: 0.1533 - mse: 0.0430
64/86 [=====================>........] - ETA: 0s - loss: 0.0466 - mae: 0.1604 - mse: 0.0466
86/86 [==============================] - 0s 5ms/step - loss: 0.0454 - mae: 0.1566 - mse: 0.0454 - val_loss: 0.0194 - val_mae: 0.1240 - val_mse: 0.0194
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0427 - mae: 0.1605 - mse: 0.0427
64/86 [=====================>........] - ETA: 0s - loss: 0.0390 - mae: 0.1513 - mse: 0.0390
86/86 [==============================] - 0s 5ms/step - loss: 0.0402 - mae: 0.1536 - mse: 0.0402 - val_loss: 0.0184 - val_mae: 0.1199 - val_mse: 0.0184
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0476 - mae: 0.1646 - mse: 0.0476
64/86 [=====================>........] - ETA: 0s - loss: 0.0417 - mae: 0.1490 - mse: 0.0417
86/86 [==============================] - 0s 5ms/step - loss: 0.0414 - mae: 0.1469 - mse: 0.0414 - val_loss: 0.0163 - val_mae: 0.1143 - val_mse: 0.0163
Saving trained model...
99
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         13.08209229]
average prediction= [3.5496979]
baseline= 6.26
eachuser= [0. 0. 0. 0. 0. 2.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 6.541046142578125
['train-height-9.py', '0']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
155 7
2_155_65_9_csi_a9_19.dat
155 9
155 10
155 11
155 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
155 15
2_155_65_9_csi_a9_7.dat
155 17
155 18
155 19
2_155_65_9_csi_a9_21.dat
155 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
155 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_9_csi_a9_29.dat
165 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
165 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
165 52
1_165_65_9_csi_a9_16.dat
165 54
165 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
165 58
165 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
165 62
165 63
1_165_65_9_csi_a9_5.dat
165 65
165 66
1_165_65_9_csi_a9_18.dat
165 68
165 69
1_165_65_9_csi_a9_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_9_csi_a9_22.dat
165 90
165 91
165 92
165 93
165 94
2_165_50_9_csi_a9_26.dat
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
1_180_85_9_csi_a9_7.dat
180 132
180 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
180 140
180 141
180 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
180 146
180 147
180 148
180 149
180 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
180 153
180 154
180 155
1_180_85_9_csi_a9_18.dat
180 157
180 158
180 159
180 160
180 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
180 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
180 171
1_180_75_9_csi_a9_25.dat
180 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
180 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
180 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
180 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
173 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
173 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
173 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
173 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
173 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 170 170 170 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 173 173 173 173 173]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.3407 - mae: 0.5198 - mse: 0.3407
64/86 [=====================>........] - ETA: 0s - loss: 0.3340 - mae: 0.5149 - mse: 0.3340
86/86 [==============================] - 1s 10ms/step - loss: 0.3001 - mae: 0.4804 - mse: 0.3001 - val_loss: 0.1180 - val_mae: 0.2746 - val_mse: 0.1180
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0947 - mae: 0.2676 - mse: 0.0947
64/86 [=====================>........] - ETA: 0s - loss: 0.1090 - mae: 0.2815 - mse: 0.1090
86/86 [==============================] - 1s 6ms/step - loss: 0.1300 - mae: 0.2990 - mse: 0.1300 - val_loss: 0.2743 - val_mae: 0.4001 - val_mse: 0.2743
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.2145 - mae: 0.3348 - mse: 0.2145
64/86 [=====================>........] - ETA: 0s - loss: 0.1582 - mae: 0.2989 - mse: 0.1582
86/86 [==============================] - 0s 5ms/step - loss: 0.1341 - mae: 0.2731 - mse: 0.1341 - val_loss: 0.1635 - val_mae: 0.3241 - val_mse: 0.1635
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1078 - mae: 0.2625 - mse: 0.1078
64/86 [=====================>........] - ETA: 0s - loss: 0.0947 - mae: 0.2415 - mse: 0.0947
86/86 [==============================] - 0s 5ms/step - loss: 0.1030 - mae: 0.2582 - mse: 0.1030 - val_loss: 0.1156 - val_mae: 0.2775 - val_mse: 0.1156
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0945 - mae: 0.2420 - mse: 0.0945
64/86 [=====================>........] - ETA: 0s - loss: 0.1052 - mae: 0.2691 - mse: 0.1052
86/86 [==============================] - 0s 5ms/step - loss: 0.0935 - mae: 0.2483 - mse: 0.0935 - val_loss: 0.1054 - val_mae: 0.2693 - val_mse: 0.1054
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0951 - mae: 0.2534 - mse: 0.0951
64/86 [=====================>........] - ETA: 0s - loss: 0.0897 - mae: 0.2462 - mse: 0.0897
86/86 [==============================] - 0s 5ms/step - loss: 0.0923 - mae: 0.2488 - mse: 0.0923 - val_loss: 0.1051 - val_mae: 0.2632 - val_mse: 0.1051
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0686 - mae: 0.1945 - mse: 0.0686
64/86 [=====================>........] - ETA: 0s - loss: 0.0860 - mae: 0.2273 - mse: 0.0860
86/86 [==============================] - 0s 5ms/step - loss: 0.0843 - mae: 0.2196 - mse: 0.0843 - val_loss: 0.1218 - val_mae: 0.2620 - val_mse: 0.1218
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0794 - mae: 0.2099 - mse: 0.0794
64/86 [=====================>........] - ETA: 0s - loss: 0.0744 - mae: 0.2064 - mse: 0.0744
86/86 [==============================] - 0s 5ms/step - loss: 0.0742 - mae: 0.2056 - mse: 0.0742 - val_loss: 0.1272 - val_mae: 0.2582 - val_mse: 0.1272
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0879 - mae: 0.2376 - mse: 0.0879
64/86 [=====================>........] - ETA: 0s - loss: 0.0930 - mae: 0.2416 - mse: 0.0930
86/86 [==============================] - 0s 6ms/step - loss: 0.0906 - mae: 0.2350 - mse: 0.0906 - val_loss: 0.1082 - val_mae: 0.2527 - val_mse: 0.1082
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0783 - mae: 0.1882 - mse: 0.0783
64/86 [=====================>........] - ETA: 0s - loss: 0.0526 - mae: 0.1633 - mse: 0.0526
86/86 [==============================] - 0s 5ms/step - loss: 0.0718 - mae: 0.1935 - mse: 0.0718 - val_loss: 0.0875 - val_mae: 0.2472 - val_mse: 0.0875
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0754 - mae: 0.2154 - mse: 0.0754
64/86 [=====================>........] - ETA: 0s - loss: 0.0657 - mae: 0.1970 - mse: 0.0657
86/86 [==============================] - 0s 5ms/step - loss: 0.0666 - mae: 0.2031 - mse: 0.0666 - val_loss: 0.0781 - val_mae: 0.2363 - val_mse: 0.0781
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0661 - mae: 0.1969 - mse: 0.0661
64/86 [=====================>........] - ETA: 0s - loss: 0.0793 - mae: 0.2167 - mse: 0.0793
86/86 [==============================] - 0s 5ms/step - loss: 0.0730 - mae: 0.2034 - mse: 0.0730 - val_loss: 0.0752 - val_mae: 0.2245 - val_mse: 0.0752
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0480 - mae: 0.1614 - mse: 0.0480
64/86 [=====================>........] - ETA: 0s - loss: 0.0712 - mae: 0.2017 - mse: 0.0712
86/86 [==============================] - 0s 4ms/step - loss: 0.0648 - mae: 0.1929 - mse: 0.0648 - val_loss: 0.0852 - val_mae: 0.2162 - val_mse: 0.0852
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0371 - mae: 0.1457 - mse: 0.0371
64/86 [=====================>........] - ETA: 0s - loss: 0.0481 - mae: 0.1676 - mse: 0.0481
86/86 [==============================] - 0s 5ms/step - loss: 0.0564 - mae: 0.1706 - mse: 0.0564 - val_loss: 0.0864 - val_mae: 0.2165 - val_mse: 0.0864
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0697 - mae: 0.1932 - mse: 0.0697
64/86 [=====================>........] - ETA: 0s - loss: 0.0693 - mae: 0.1916 - mse: 0.0693
86/86 [==============================] - 0s 5ms/step - loss: 0.0702 - mae: 0.1973 - mse: 0.0702 - val_loss: 0.0687 - val_mae: 0.2103 - val_mse: 0.0687
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0552 - mae: 0.1674 - mse: 0.0552
64/86 [=====================>........] - ETA: 0s - loss: 0.0703 - mae: 0.1979 - mse: 0.0703
86/86 [==============================] - 0s 5ms/step - loss: 0.0693 - mae: 0.1899 - mse: 0.0693 - val_loss: 0.0591 - val_mae: 0.2012 - val_mse: 0.0591
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0716 - mae: 0.1977 - mse: 0.0716
64/86 [=====================>........] - ETA: 0s - loss: 0.0649 - mae: 0.1902 - mse: 0.0649
86/86 [==============================] - 0s 5ms/step - loss: 0.0596 - mae: 0.1852 - mse: 0.0596 - val_loss: 0.0572 - val_mae: 0.1929 - val_mse: 0.0572
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0851 - mae: 0.2189 - mse: 0.0851
64/86 [=====================>........] - ETA: 0s - loss: 0.0607 - mae: 0.1730 - mse: 0.0607
86/86 [==============================] - 0s 5ms/step - loss: 0.0545 - mae: 0.1643 - mse: 0.0545 - val_loss: 0.0621 - val_mae: 0.1845 - val_mse: 0.0621
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0385 - mae: 0.1480 - mse: 0.0385
64/86 [=====================>........] - ETA: 0s - loss: 0.0564 - mae: 0.1827 - mse: 0.0564
86/86 [==============================] - 0s 5ms/step - loss: 0.0735 - mae: 0.1993 - mse: 0.0735 - val_loss: 0.0634 - val_mae: 0.1841 - val_mse: 0.0634
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0878 - mae: 0.2183 - mse: 0.0878
64/86 [=====================>........] - ETA: 0s - loss: 0.0585 - mae: 0.1696 - mse: 0.0585
86/86 [==============================] - 0s 5ms/step - loss: 0.0596 - mae: 0.1742 - mse: 0.0596 - val_loss: 0.0489 - val_mae: 0.1823 - val_mse: 0.0489
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0687 - mae: 0.1940 - mse: 0.0687
64/86 [=====================>........] - ETA: 0s - loss: 0.0633 - mae: 0.1886 - mse: 0.0633
86/86 [==============================] - 0s 5ms/step - loss: 0.0557 - mae: 0.1746 - mse: 0.0557 - val_loss: 0.0414 - val_mae: 0.1683 - val_mse: 0.0414
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0647 - mae: 0.1826 - mse: 0.0647
64/86 [=====================>........] - ETA: 0s - loss: 0.0624 - mae: 0.1751 - mse: 0.0624
86/86 [==============================] - 0s 5ms/step - loss: 0.0559 - mae: 0.1671 - mse: 0.0559 - val_loss: 0.0414 - val_mae: 0.1575 - val_mse: 0.0414
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0584 - mae: 0.1794 - mse: 0.0584
64/86 [=====================>........] - ETA: 0s - loss: 0.0515 - mae: 0.1575 - mse: 0.0515
86/86 [==============================] - 0s 5ms/step - loss: 0.0493 - mae: 0.1579 - mse: 0.0493 - val_loss: 0.0422 - val_mae: 0.1598 - val_mse: 0.0422
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0472 - mae: 0.1635 - mse: 0.0472
64/86 [=====================>........] - ETA: 0s - loss: 0.0481 - mae: 0.1645 - mse: 0.0481
86/86 [==============================] - 0s 5ms/step - loss: 0.0546 - mae: 0.1666 - mse: 0.0546 - val_loss: 0.0425 - val_mae: 0.1676 - val_mse: 0.0425
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0324 - mae: 0.1283 - mse: 0.0324
64/86 [=====================>........] - ETA: 0s - loss: 0.0361 - mae: 0.1362 - mse: 0.0361
86/86 [==============================] - 0s 5ms/step - loss: 0.0455 - mae: 0.1536 - mse: 0.0455 - val_loss: 0.0386 - val_mae: 0.1673 - val_mse: 0.0386
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0436 - mae: 0.1343 - mse: 0.0436
64/86 [=====================>........] - ETA: 0s - loss: 0.0415 - mae: 0.1449 - mse: 0.0415
86/86 [==============================] - 0s 5ms/step - loss: 0.0417 - mae: 0.1503 - mse: 0.0417 - val_loss: 0.0380 - val_mae: 0.1586 - val_mse: 0.0380
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0389 - mae: 0.1265 - mse: 0.0389
64/86 [=====================>........] - ETA: 0s - loss: 0.0477 - mae: 0.1515 - mse: 0.0477
86/86 [==============================] - 0s 5ms/step - loss: 0.0481 - mae: 0.1540 - mse: 0.0481 - val_loss: 0.0406 - val_mae: 0.1545 - val_mse: 0.0406
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0391 - mae: 0.1395 - mse: 0.0391
64/86 [=====================>........] - ETA: 0s - loss: 0.0459 - mae: 0.1586 - mse: 0.0459
86/86 [==============================] - 0s 6ms/step - loss: 0.0464 - mae: 0.1592 - mse: 0.0464 - val_loss: 0.0418 - val_mae: 0.1636 - val_mse: 0.0418
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0368 - mae: 0.1374 - mse: 0.0368
64/86 [=====================>........] - ETA: 0s - loss: 0.0444 - mae: 0.1554 - mse: 0.0444
86/86 [==============================] - 0s 5ms/step - loss: 0.0463 - mae: 0.1579 - mse: 0.0463 - val_loss: 0.0421 - val_mae: 0.1704 - val_mse: 0.0421
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0517 - mae: 0.1613 - mse: 0.0517
64/86 [=====================>........] - ETA: 0s - loss: 0.0458 - mae: 0.1521 - mse: 0.0458
86/86 [==============================] - 0s 5ms/step - loss: 0.0432 - mae: 0.1478 - mse: 0.0432 - val_loss: 0.0344 - val_mae: 0.1591 - val_mse: 0.0344
Saving trained model...
99
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         17.20600891]
average prediction= [5.4027033]
baseline= 7.06
eachuser= [0. 0. 0. 0. 0. 2.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 8.603004455566406
['train-height-9.py', '0']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
155 7
2_155_65_9_csi_a9_19.dat
155 9
155 10
155 11
155 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
155 15
2_155_65_9_csi_a9_7.dat
155 17
155 18
155 19
2_155_65_9_csi_a9_21.dat
155 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
155 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_9_csi_a9_29.dat
165 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
165 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
165 52
1_165_65_9_csi_a9_16.dat
165 54
165 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
165 58
165 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
165 62
165 63
1_165_65_9_csi_a9_5.dat
165 65
165 66
1_165_65_9_csi_a9_18.dat
165 68
165 69
1_165_65_9_csi_a9_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_9_csi_a9_22.dat
165 90
165 91
165 92
165 93
165 94
2_165_50_9_csi_a9_26.dat
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
1_180_85_9_csi_a9_7.dat
180 132
180 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
180 140
180 141
180 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
180 146
180 147
180 148
180 149
180 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
180 153
180 154
180 155
1_180_85_9_csi_a9_18.dat
180 157
180 158
180 159
180 160
180 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
180 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
180 171
1_180_75_9_csi_a9_25.dat
180 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
180 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
180 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
180 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
173 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
173 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
173 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
173 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
173 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 170 170 170 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 173 173 173 173 173]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.4296 - mae: 0.5764 - mse: 0.4296
64/86 [=====================>........] - ETA: 0s - loss: 0.3098 - mae: 0.4709 - mse: 0.3098
86/86 [==============================] - 1s 11ms/step - loss: 0.2772 - mae: 0.4404 - mse: 0.2772 - val_loss: 0.1087 - val_mae: 0.2908 - val_mse: 0.1087
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1705 - mae: 0.3626 - mse: 0.1705
64/86 [=====================>........] - ETA: 0s - loss: 0.1554 - mae: 0.3367 - mse: 0.1554
86/86 [==============================] - 1s 6ms/step - loss: 0.1410 - mae: 0.3126 - mse: 0.1410 - val_loss: 0.0974 - val_mae: 0.2746 - val_mse: 0.0974
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1328 - mae: 0.3209 - mse: 0.1328
64/86 [=====================>........] - ETA: 0s - loss: 0.1318 - mae: 0.3119 - mse: 0.1318
86/86 [==============================] - 0s 6ms/step - loss: 0.1187 - mae: 0.2868 - mse: 0.1187 - val_loss: 0.1425 - val_mae: 0.3292 - val_mse: 0.1425
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1150 - mae: 0.2870 - mse: 0.1150
64/86 [=====================>........] - ETA: 0s - loss: 0.1192 - mae: 0.2907 - mse: 0.1192
86/86 [==============================] - 0s 5ms/step - loss: 0.1119 - mae: 0.2714 - mse: 0.1119 - val_loss: 0.1638 - val_mae: 0.3630 - val_mse: 0.1638
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1012 - mae: 0.2620 - mse: 0.1012
64/86 [=====================>........] - ETA: 0s - loss: 0.1046 - mae: 0.2617 - mse: 0.1046
86/86 [==============================] - 0s 5ms/step - loss: 0.1034 - mae: 0.2605 - mse: 0.1034 - val_loss: 0.1299 - val_mae: 0.3285 - val_mse: 0.1299
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0884 - mae: 0.2248 - mse: 0.0884
64/86 [=====================>........] - ETA: 0s - loss: 0.0944 - mae: 0.2329 - mse: 0.0944
86/86 [==============================] - 0s 5ms/step - loss: 0.0979 - mae: 0.2422 - mse: 0.0979 - val_loss: 0.0851 - val_mae: 0.2587 - val_mse: 0.0851
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0852 - mae: 0.2203 - mse: 0.0852
64/86 [=====================>........] - ETA: 0s - loss: 0.0809 - mae: 0.2129 - mse: 0.0809
86/86 [==============================] - 0s 5ms/step - loss: 0.0871 - mae: 0.2191 - mse: 0.0871 - val_loss: 0.0767 - val_mae: 0.2423 - val_mse: 0.0767
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0653 - mae: 0.1941 - mse: 0.0653
64/86 [=====================>........] - ETA: 0s - loss: 0.0776 - mae: 0.2090 - mse: 0.0776
86/86 [==============================] - 0s 6ms/step - loss: 0.0772 - mae: 0.2105 - mse: 0.0772 - val_loss: 0.0827 - val_mae: 0.2604 - val_mse: 0.0827
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0887 - mae: 0.2309 - mse: 0.0887
64/86 [=====================>........] - ETA: 0s - loss: 0.0795 - mae: 0.2198 - mse: 0.0795
86/86 [==============================] - 0s 5ms/step - loss: 0.0804 - mae: 0.2182 - mse: 0.0804 - val_loss: 0.0967 - val_mae: 0.2872 - val_mse: 0.0967
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0629 - mae: 0.2000 - mse: 0.0629
64/86 [=====================>........] - ETA: 0s - loss: 0.0677 - mae: 0.2019 - mse: 0.0677
86/86 [==============================] - 0s 5ms/step - loss: 0.0750 - mae: 0.2160 - mse: 0.0750 - val_loss: 0.0888 - val_mae: 0.2732 - val_mse: 0.0888
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0878 - mae: 0.2429 - mse: 0.0878
64/86 [=====================>........] - ETA: 0s - loss: 0.0739 - mae: 0.2090 - mse: 0.0739
86/86 [==============================] - 0s 5ms/step - loss: 0.0690 - mae: 0.1980 - mse: 0.0690 - val_loss: 0.0651 - val_mae: 0.2159 - val_mse: 0.0651
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0510 - mae: 0.1725 - mse: 0.0510
64/86 [=====================>........] - ETA: 0s - loss: 0.0644 - mae: 0.1846 - mse: 0.0644
86/86 [==============================] - 0s 6ms/step - loss: 0.0718 - mae: 0.1999 - mse: 0.0718 - val_loss: 0.0601 - val_mae: 0.1999 - val_mse: 0.0601
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0672 - mae: 0.1897 - mse: 0.0672
64/86 [=====================>........] - ETA: 0s - loss: 0.0569 - mae: 0.1788 - mse: 0.0569
86/86 [==============================] - 0s 5ms/step - loss: 0.0675 - mae: 0.1957 - mse: 0.0675 - val_loss: 0.0840 - val_mae: 0.2616 - val_mse: 0.0840
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0923 - mae: 0.2317 - mse: 0.0923
64/86 [=====================>........] - ETA: 0s - loss: 0.0787 - mae: 0.2139 - mse: 0.0787
86/86 [==============================] - 0s 5ms/step - loss: 0.0781 - mae: 0.2174 - mse: 0.0781 - val_loss: 0.0928 - val_mae: 0.2749 - val_mse: 0.0928
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0777 - mae: 0.2315 - mse: 0.0777
64/86 [=====================>........] - ETA: 0s - loss: 0.0709 - mae: 0.2157 - mse: 0.0709
86/86 [==============================] - 0s 5ms/step - loss: 0.0715 - mae: 0.2127 - mse: 0.0715 - val_loss: 0.0626 - val_mae: 0.2125 - val_mse: 0.0626
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0635 - mae: 0.1968 - mse: 0.0635
64/86 [=====================>........] - ETA: 0s - loss: 0.0621 - mae: 0.1880 - mse: 0.0621
86/86 [==============================] - 0s 5ms/step - loss: 0.0635 - mae: 0.1829 - mse: 0.0635 - val_loss: 0.0498 - val_mae: 0.1678 - val_mse: 0.0498
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0411 - mae: 0.1519 - mse: 0.0411
64/86 [=====================>........] - ETA: 0s - loss: 0.0513 - mae: 0.1706 - mse: 0.0513
86/86 [==============================] - 0s 5ms/step - loss: 0.0511 - mae: 0.1680 - mse: 0.0511 - val_loss: 0.0611 - val_mae: 0.2049 - val_mse: 0.0611
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0529 - mae: 0.1759 - mse: 0.0529
64/86 [=====================>........] - ETA: 0s - loss: 0.0512 - mae: 0.1699 - mse: 0.0512
86/86 [==============================] - 0s 5ms/step - loss: 0.0519 - mae: 0.1663 - mse: 0.0519 - val_loss: 0.0780 - val_mae: 0.2387 - val_mse: 0.0780
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0686 - mae: 0.2139 - mse: 0.0686
64/86 [=====================>........] - ETA: 0s - loss: 0.0511 - mae: 0.1688 - mse: 0.0511
86/86 [==============================] - 0s 5ms/step - loss: 0.0527 - mae: 0.1716 - mse: 0.0527 - val_loss: 0.0585 - val_mae: 0.1982 - val_mse: 0.0585
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0340 - mae: 0.1280 - mse: 0.0340
64/86 [=====================>........] - ETA: 0s - loss: 0.0463 - mae: 0.1514 - mse: 0.0463
86/86 [==============================] - 0s 5ms/step - loss: 0.0468 - mae: 0.1545 - mse: 0.0468 - val_loss: 0.0468 - val_mae: 0.1629 - val_mse: 0.0468
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0538 - mae: 0.1545 - mse: 0.0538
64/86 [=====================>........] - ETA: 0s - loss: 0.0532 - mae: 0.1543 - mse: 0.0532
86/86 [==============================] - 1s 7ms/step - loss: 0.0536 - mae: 0.1617 - mse: 0.0536 - val_loss: 0.0569 - val_mae: 0.1962 - val_mse: 0.0569
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0498 - mae: 0.1709 - mse: 0.0498
64/86 [=====================>........] - ETA: 0s - loss: 0.0530 - mae: 0.1747 - mse: 0.0530
86/86 [==============================] - 0s 5ms/step - loss: 0.0552 - mae: 0.1805 - mse: 0.0552 - val_loss: 0.0726 - val_mae: 0.2310 - val_mse: 0.0726
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0496 - mae: 0.1730 - mse: 0.0496
64/86 [=====================>........] - ETA: 0s - loss: 0.0491 - mae: 0.1740 - mse: 0.0491
86/86 [==============================] - 0s 5ms/step - loss: 0.0526 - mae: 0.1735 - mse: 0.0526 - val_loss: 0.0697 - val_mae: 0.2225 - val_mse: 0.0697
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0524 - mae: 0.1903 - mse: 0.0524
64/86 [=====================>........] - ETA: 0s - loss: 0.0524 - mae: 0.1766 - mse: 0.0524
86/86 [==============================] - 0s 5ms/step - loss: 0.0473 - mae: 0.1659 - mse: 0.0473 - val_loss: 0.0561 - val_mae: 0.1872 - val_mse: 0.0561
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0393 - mae: 0.1443 - mse: 0.0393
64/86 [=====================>........] - ETA: 0s - loss: 0.0483 - mae: 0.1595 - mse: 0.0483
86/86 [==============================] - 1s 7ms/step - loss: 0.0468 - mae: 0.1613 - mse: 0.0468 - val_loss: 0.0480 - val_mae: 0.1716 - val_mse: 0.0480
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0466 - mae: 0.1630 - mse: 0.0466
64/86 [=====================>........] - ETA: 0s - loss: 0.0388 - mae: 0.1476 - mse: 0.0388
86/86 [==============================] - 1s 6ms/step - loss: 0.0369 - mae: 0.1455 - mse: 0.0369 - val_loss: 0.0708 - val_mae: 0.2238 - val_mse: 0.0708
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0309 - mae: 0.1266 - mse: 0.0309
64/86 [=====================>........] - ETA: 0s - loss: 0.0379 - mae: 0.1387 - mse: 0.0379
86/86 [==============================] - 0s 5ms/step - loss: 0.0395 - mae: 0.1457 - mse: 0.0395 - val_loss: 0.0818 - val_mae: 0.2419 - val_mse: 0.0818
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0350 - mae: 0.1395 - mse: 0.0350
64/86 [=====================>........] - ETA: 0s - loss: 0.0319 - mae: 0.1319 - mse: 0.0319
86/86 [==============================] - 0s 5ms/step - loss: 0.0358 - mae: 0.1366 - mse: 0.0358 - val_loss: 0.0586 - val_mae: 0.2081 - val_mse: 0.0586
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0259 - mae: 0.1098 - mse: 0.0259
64/86 [=====================>........] - ETA: 0s - loss: 0.0304 - mae: 0.1245 - mse: 0.0304
86/86 [==============================] - 0s 5ms/step - loss: 0.0330 - mae: 0.1331 - mse: 0.0330 - val_loss: 0.0559 - val_mae: 0.1950 - val_mse: 0.0559
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0216 - mae: 0.1014 - mse: 0.0216
64/86 [=====================>........] - ETA: 0s - loss: 0.0390 - mae: 0.1306 - mse: 0.0390
86/86 [==============================] - 0s 5ms/step - loss: 0.0358 - mae: 0.1293 - mse: 0.0358 - val_loss: 0.0730 - val_mae: 0.2201 - val_mse: 0.0730
Saving trained model...
99
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         12.10899353]
average prediction= [4.9073205]
baseline= 6.86
eachuser= [0. 0. 0. 0. 0. 2.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 6.054496765136719
['train-height-9.py', '0']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
155 7
2_155_65_9_csi_a9_19.dat
155 9
155 10
155 11
155 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
155 15
2_155_65_9_csi_a9_7.dat
155 17
155 18
155 19
2_155_65_9_csi_a9_21.dat
155 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
155 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_9_csi_a9_29.dat
165 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
165 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
165 52
1_165_65_9_csi_a9_16.dat
165 54
165 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
165 58
165 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
165 62
165 63
1_165_65_9_csi_a9_5.dat
165 65
165 66
1_165_65_9_csi_a9_18.dat
165 68
165 69
1_165_65_9_csi_a9_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_9_csi_a9_22.dat
165 90
165 91
165 92
165 93
165 94
2_165_50_9_csi_a9_26.dat
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
1_180_85_9_csi_a9_7.dat
180 132
180 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
180 140
180 141
180 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
180 146
180 147
180 148
180 149
180 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
180 153
180 154
180 155
1_180_85_9_csi_a9_18.dat
180 157
180 158
180 159
180 160
180 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
180 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
180 171
1_180_75_9_csi_a9_25.dat
180 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
180 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
180 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
180 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
173 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
173 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
173 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
173 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
173 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 170 170 170 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 173 173 173 173 173]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.4974 - mae: 0.6332 - mse: 0.4974
64/86 [=====================>........] - ETA: 0s - loss: 0.3835 - mae: 0.5455 - mse: 0.3835
86/86 [==============================] - 1s 11ms/step - loss: 0.3376 - mae: 0.5079 - mse: 0.3376 - val_loss: 0.2408 - val_mae: 0.4345 - val_mse: 0.2408
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1432 - mae: 0.3183 - mse: 0.1432
64/86 [=====================>........] - ETA: 0s - loss: 0.1295 - mae: 0.3009 - mse: 0.1295
86/86 [==============================] - 1s 6ms/step - loss: 0.1542 - mae: 0.3294 - mse: 0.1542 - val_loss: 0.0730 - val_mae: 0.2362 - val_mse: 0.0730
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1488 - mae: 0.3107 - mse: 0.1488
64/86 [=====================>........] - ETA: 0s - loss: 0.1875 - mae: 0.3413 - mse: 0.1875
86/86 [==============================] - 0s 5ms/step - loss: 0.1784 - mae: 0.3400 - mse: 0.1784 - val_loss: 0.1294 - val_mae: 0.3201 - val_mse: 0.1294
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1084 - mae: 0.2651 - mse: 0.1084
64/86 [=====================>........] - ETA: 0s - loss: 0.1032 - mae: 0.2709 - mse: 0.1032
86/86 [==============================] - 0s 5ms/step - loss: 0.1009 - mae: 0.2650 - mse: 0.1009 - val_loss: 0.2176 - val_mae: 0.4186 - val_mse: 0.2176
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1298 - mae: 0.2994 - mse: 0.1298
64/86 [=====================>........] - ETA: 0s - loss: 0.1249 - mae: 0.2989 - mse: 0.1249
86/86 [==============================] - 0s 5ms/step - loss: 0.1101 - mae: 0.2735 - mse: 0.1101 - val_loss: 0.2339 - val_mae: 0.4372 - val_mse: 0.2339
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1315 - mae: 0.2995 - mse: 0.1315
64/86 [=====================>........] - ETA: 0s - loss: 0.1247 - mae: 0.2957 - mse: 0.1247
86/86 [==============================] - 0s 5ms/step - loss: 0.1144 - mae: 0.2798 - mse: 0.1144 - val_loss: 0.1919 - val_mae: 0.3953 - val_mse: 0.1919
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0765 - mae: 0.2173 - mse: 0.0765
64/86 [=====================>........] - ETA: 0s - loss: 0.1001 - mae: 0.2479 - mse: 0.1001
86/86 [==============================] - 0s 5ms/step - loss: 0.1083 - mae: 0.2617 - mse: 0.1083 - val_loss: 0.1285 - val_mae: 0.3239 - val_mse: 0.1285
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0966 - mae: 0.2308 - mse: 0.0966
64/86 [=====================>........] - ETA: 0s - loss: 0.0980 - mae: 0.2345 - mse: 0.0980
86/86 [==============================] - 1s 6ms/step - loss: 0.0957 - mae: 0.2384 - mse: 0.0957 - val_loss: 0.1047 - val_mae: 0.2900 - val_mse: 0.1047
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1026 - mae: 0.2540 - mse: 0.1026
64/86 [=====================>........] - ETA: 0s - loss: 0.0949 - mae: 0.2484 - mse: 0.0949
86/86 [==============================] - 0s 5ms/step - loss: 0.1012 - mae: 0.2509 - mse: 0.1012 - val_loss: 0.1171 - val_mae: 0.3072 - val_mse: 0.1171
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0835 - mae: 0.2314 - mse: 0.0835
64/86 [=====================>........] - ETA: 0s - loss: 0.0725 - mae: 0.2128 - mse: 0.0725
86/86 [==============================] - 0s 5ms/step - loss: 0.0833 - mae: 0.2272 - mse: 0.0833 - val_loss: 0.1466 - val_mae: 0.3443 - val_mse: 0.1466
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0727 - mae: 0.2165 - mse: 0.0727
64/86 [=====================>........] - ETA: 0s - loss: 0.0764 - mae: 0.2158 - mse: 0.0764
86/86 [==============================] - 0s 5ms/step - loss: 0.0802 - mae: 0.2233 - mse: 0.0802 - val_loss: 0.1612 - val_mae: 0.3598 - val_mse: 0.1612
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0985 - mae: 0.2430 - mse: 0.0985
64/86 [=====================>........] - ETA: 0s - loss: 0.1034 - mae: 0.2487 - mse: 0.1034
86/86 [==============================] - 0s 4ms/step - loss: 0.0936 - mae: 0.2360 - mse: 0.0936 - val_loss: 0.1608 - val_mae: 0.3561 - val_mse: 0.1608
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0677 - mae: 0.1999 - mse: 0.0677
64/86 [=====================>........] - ETA: 0s - loss: 0.0679 - mae: 0.1957 - mse: 0.0679
86/86 [==============================] - 0s 5ms/step - loss: 0.0679 - mae: 0.1975 - mse: 0.0679 - val_loss: 0.1397 - val_mae: 0.3274 - val_mse: 0.1397
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0663 - mae: 0.2079 - mse: 0.0663
64/86 [=====================>........] - ETA: 0s - loss: 0.0572 - mae: 0.1852 - mse: 0.0572
86/86 [==============================] - 0s 5ms/step - loss: 0.0781 - mae: 0.2128 - mse: 0.0781 - val_loss: 0.1208 - val_mae: 0.3009 - val_mse: 0.1208
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0870 - mae: 0.2284 - mse: 0.0870
64/86 [=====================>........] - ETA: 0s - loss: 0.0649 - mae: 0.1925 - mse: 0.0649
86/86 [==============================] - 0s 6ms/step - loss: 0.0746 - mae: 0.2048 - mse: 0.0746 - val_loss: 0.1218 - val_mae: 0.3005 - val_mse: 0.1218
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1068 - mae: 0.2362 - mse: 0.1068
64/86 [=====================>........] - ETA: 0s - loss: 0.0827 - mae: 0.2183 - mse: 0.0827
86/86 [==============================] - 0s 5ms/step - loss: 0.0789 - mae: 0.2101 - mse: 0.0789 - val_loss: 0.1409 - val_mae: 0.3243 - val_mse: 0.1409
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0708 - mae: 0.1861 - mse: 0.0708
64/86 [=====================>........] - ETA: 0s - loss: 0.0726 - mae: 0.1938 - mse: 0.0726
86/86 [==============================] - 0s 5ms/step - loss: 0.0681 - mae: 0.1893 - mse: 0.0681 - val_loss: 0.1470 - val_mae: 0.3317 - val_mse: 0.1470
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0672 - mae: 0.1972 - mse: 0.0672
64/86 [=====================>........] - ETA: 0s - loss: 0.0985 - mae: 0.2420 - mse: 0.0985
86/86 [==============================] - 1s 6ms/step - loss: 0.0818 - mae: 0.2184 - mse: 0.0818 - val_loss: 0.1196 - val_mae: 0.2966 - val_mse: 0.1196
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0428 - mae: 0.1649 - mse: 0.0428
64/86 [=====================>........] - ETA: 0s - loss: 0.0464 - mae: 0.1715 - mse: 0.0464
86/86 [==============================] - 0s 5ms/step - loss: 0.0729 - mae: 0.1985 - mse: 0.0729 - val_loss: 0.0934 - val_mae: 0.2564 - val_mse: 0.0934
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0787 - mae: 0.1883 - mse: 0.0787
64/86 [=====================>........] - ETA: 0s - loss: 0.0731 - mae: 0.1935 - mse: 0.0731
86/86 [==============================] - 0s 5ms/step - loss: 0.0711 - mae: 0.1920 - mse: 0.0711 - val_loss: 0.1242 - val_mae: 0.3026 - val_mse: 0.1242
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0514 - mae: 0.1466 - mse: 0.0514
64/86 [=====================>........] - ETA: 0s - loss: 0.0416 - mae: 0.1415 - mse: 0.0416
86/86 [==============================] - 0s 5ms/step - loss: 0.0589 - mae: 0.1718 - mse: 0.0589 - val_loss: 0.1394 - val_mae: 0.3254 - val_mse: 0.1394
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0642 - mae: 0.1736 - mse: 0.0642
64/86 [=====================>........] - ETA: 0s - loss: 0.0651 - mae: 0.1851 - mse: 0.0651
86/86 [==============================] - 0s 5ms/step - loss: 0.0643 - mae: 0.1817 - mse: 0.0643 - val_loss: 0.1230 - val_mae: 0.2974 - val_mse: 0.1230
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0782 - mae: 0.1956 - mse: 0.0782
64/86 [=====================>........] - ETA: 0s - loss: 0.0709 - mae: 0.1922 - mse: 0.0709
86/86 [==============================] - 1s 6ms/step - loss: 0.0639 - mae: 0.1791 - mse: 0.0639 - val_loss: 0.1134 - val_mae: 0.2787 - val_mse: 0.1134
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0928 - mae: 0.2411 - mse: 0.0928
64/86 [=====================>........] - ETA: 0s - loss: 0.0616 - mae: 0.1921 - mse: 0.0616
86/86 [==============================] - 0s 5ms/step - loss: 0.0637 - mae: 0.1928 - mse: 0.0637 - val_loss: 0.1154 - val_mae: 0.2851 - val_mse: 0.1154
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0554 - mae: 0.1761 - mse: 0.0554
64/86 [=====================>........] - ETA: 0s - loss: 0.0659 - mae: 0.1868 - mse: 0.0659
86/86 [==============================] - 0s 5ms/step - loss: 0.0594 - mae: 0.1784 - mse: 0.0594 - val_loss: 0.1354 - val_mae: 0.3164 - val_mse: 0.1354
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0496 - mae: 0.1539 - mse: 0.0496
64/86 [=====================>........] - ETA: 0s - loss: 0.0533 - mae: 0.1639 - mse: 0.0533
86/86 [==============================] - 0s 6ms/step - loss: 0.0503 - mae: 0.1609 - mse: 0.0503 - val_loss: 0.1219 - val_mae: 0.2952 - val_mse: 0.1219
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0444 - mae: 0.1616 - mse: 0.0444
64/86 [=====================>........] - ETA: 0s - loss: 0.0473 - mae: 0.1596 - mse: 0.0473
86/86 [==============================] - 0s 6ms/step - loss: 0.0508 - mae: 0.1634 - mse: 0.0508 - val_loss: 0.0928 - val_mae: 0.2507 - val_mse: 0.0928
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0741 - mae: 0.1946 - mse: 0.0741
64/86 [=====================>........] - ETA: 0s - loss: 0.0636 - mae: 0.1742 - mse: 0.0636
86/86 [==============================] - 0s 5ms/step - loss: 0.0548 - mae: 0.1630 - mse: 0.0548 - val_loss: 0.1017 - val_mae: 0.2674 - val_mse: 0.1017
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0561 - mae: 0.1757 - mse: 0.0561
64/86 [=====================>........] - ETA: 0s - loss: 0.0610 - mae: 0.1849 - mse: 0.0610
86/86 [==============================] - 0s 5ms/step - loss: 0.0581 - mae: 0.1753 - mse: 0.0581 - val_loss: 0.1209 - val_mae: 0.2996 - val_mse: 0.1209
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0471 - mae: 0.1601 - mse: 0.0471
64/86 [=====================>........] - ETA: 0s - loss: 0.0569 - mae: 0.1697 - mse: 0.0569
86/86 [==============================] - 0s 5ms/step - loss: 0.0574 - mae: 0.1715 - mse: 0.0574 - val_loss: 0.1370 - val_mae: 0.3243 - val_mse: 0.1370
Saving trained model...
99
Testing...
heightdiff= [ 0.          0.          0.          0.          0.         35.42721558]
average prediction= [4.8014073]
baseline= 6.42
eachuser= [0. 0. 0. 0. 0. 4.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 8.856803894042969
['train-height-9.py', '0']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
155 7
2_155_65_9_csi_a9_19.dat
155 9
155 10
155 11
155 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
155 15
2_155_65_9_csi_a9_7.dat
155 17
155 18
155 19
2_155_65_9_csi_a9_21.dat
155 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
155 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
170 31
170 32
170 33
170 34
170 35
170 36
170 37
170 38
170 39
170 40
1_165_65_9_csi_a9_29.dat
165 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
165 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
165 52
1_165_65_9_csi_a9_16.dat
165 54
165 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
165 58
165 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
165 62
165 63
1_165_65_9_csi_a9_5.dat
165 65
165 66
1_165_65_9_csi_a9_18.dat
165 68
165 69
1_165_65_9_csi_a9_25.dat
165 71
165 72
165 73
165 74
165 75
165 76
165 77
165 78
165 79
165 80
165 81
165 82
165 83
165 84
165 85
165 86
165 87
165 88
2_165_50_9_csi_a9_22.dat
165 90
165 91
165 92
165 93
165 94
2_165_50_9_csi_a9_26.dat
165 96
165 97
165 98
165 99
165 100
175 101
175 102
175 103
175 104
175 105
175 106
175 107
175 108
175 109
175 110
175 111
175 112
175 113
175 114
175 115
175 116
175 117
175 118
175 119
175 120
175 121
175 122
175 123
175 124
175 125
175 126
175 127
175 128
175 129
175 130
1_180_85_9_csi_a9_7.dat
180 132
180 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
180 140
180 141
180 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
180 146
180 147
180 148
180 149
180 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
180 153
180 154
180 155
1_180_85_9_csi_a9_18.dat
180 157
180 158
180 159
180 160
180 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
180 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
180 171
1_180_75_9_csi_a9_25.dat
180 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
180 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
180 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
180 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
173 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
173 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
173 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
173 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
173 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[155 155 155 155 155 155 155 155 155 155 155 170 170 170 170 170 170 170
 170 170 170 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165 165
 165 165 165 165 165 165 165 165 175 175 175 175 175 175 175 175 175 175
 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175 175
 175 175 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180 180
 180 180 180 180 180 180 180 180 173 173 173 173 173]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.4966 - mae: 0.6139 - mse: 0.4966
64/86 [=====================>........] - ETA: 0s - loss: 0.4258 - mae: 0.5535 - mse: 0.4258
86/86 [==============================] - 1s 13ms/step - loss: 0.3575 - mae: 0.4947 - mse: 0.3575 - val_loss: 0.1047 - val_mae: 0.2769 - val_mse: 0.1047
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1533 - mae: 0.3426 - mse: 0.1533
64/86 [=====================>........] - ETA: 0s - loss: 0.1895 - mae: 0.3633 - mse: 0.1895
86/86 [==============================] - 0s 5ms/step - loss: 0.2081 - mae: 0.3790 - mse: 0.2081 - val_loss: 0.1022 - val_mae: 0.2936 - val_mse: 0.1022
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1391 - mae: 0.3317 - mse: 0.1391
64/86 [=====================>........] - ETA: 0s - loss: 0.1350 - mae: 0.3240 - mse: 0.1350
86/86 [==============================] - 0s 5ms/step - loss: 0.1387 - mae: 0.3294 - mse: 0.1387 - val_loss: 0.0806 - val_mae: 0.2278 - val_mse: 0.0806
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1296 - mae: 0.2993 - mse: 0.1296
64/86 [=====================>........] - ETA: 0s - loss: 0.1169 - mae: 0.2836 - mse: 0.1169
86/86 [==============================] - 0s 5ms/step - loss: 0.1152 - mae: 0.2877 - mse: 0.1152 - val_loss: 0.0913 - val_mae: 0.2021 - val_mse: 0.0913
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0917 - mae: 0.2602 - mse: 0.0917
64/86 [=====================>........] - ETA: 0s - loss: 0.1167 - mae: 0.2875 - mse: 0.1167
86/86 [==============================] - 0s 5ms/step - loss: 0.1135 - mae: 0.2808 - mse: 0.1135 - val_loss: 0.0760 - val_mae: 0.2033 - val_mse: 0.0760
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0959 - mae: 0.2633 - mse: 0.0959
64/86 [=====================>........] - ETA: 0s - loss: 0.0816 - mae: 0.2371 - mse: 0.0816
86/86 [==============================] - 0s 5ms/step - loss: 0.0775 - mae: 0.2281 - mse: 0.0775 - val_loss: 0.0540 - val_mae: 0.2027 - val_mse: 0.0540
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0887 - mae: 0.2510 - mse: 0.0887
64/86 [=====================>........] - ETA: 0s - loss: 0.0971 - mae: 0.2527 - mse: 0.0971
86/86 [==============================] - 0s 5ms/step - loss: 0.0919 - mae: 0.2449 - mse: 0.0919 - val_loss: 0.0529 - val_mae: 0.2016 - val_mse: 0.0529
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0800 - mae: 0.2155 - mse: 0.0800
64/86 [=====================>........] - ETA: 0s - loss: 0.0905 - mae: 0.2288 - mse: 0.0905
86/86 [==============================] - 0s 5ms/step - loss: 0.0936 - mae: 0.2359 - mse: 0.0936 - val_loss: 0.0543 - val_mae: 0.1993 - val_mse: 0.0543
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0703 - mae: 0.1965 - mse: 0.0703
64/86 [=====================>........] - ETA: 0s - loss: 0.0798 - mae: 0.2113 - mse: 0.0798
86/86 [==============================] - 0s 5ms/step - loss: 0.0833 - mae: 0.2192 - mse: 0.0833 - val_loss: 0.0722 - val_mae: 0.2236 - val_mse: 0.0722
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0750 - mae: 0.2329 - mse: 0.0750
64/86 [=====================>........] - ETA: 0s - loss: 0.0833 - mae: 0.2384 - mse: 0.0833
86/86 [==============================] - 0s 5ms/step - loss: 0.0840 - mae: 0.2406 - mse: 0.0840 - val_loss: 0.0758 - val_mae: 0.2319 - val_mse: 0.0758
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0804 - mae: 0.2373 - mse: 0.0804
64/86 [=====================>........] - ETA: 0s - loss: 0.0873 - mae: 0.2419 - mse: 0.0873
86/86 [==============================] - 0s 5ms/step - loss: 0.0832 - mae: 0.2356 - mse: 0.0832 - val_loss: 0.0650 - val_mae: 0.2107 - val_mse: 0.0650
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0839 - mae: 0.2381 - mse: 0.0839
64/86 [=====================>........] - ETA: 0s - loss: 0.0793 - mae: 0.2207 - mse: 0.0793
86/86 [==============================] - 0s 5ms/step - loss: 0.0772 - mae: 0.2179 - mse: 0.0772 - val_loss: 0.0623 - val_mae: 0.1900 - val_mse: 0.0623
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0785 - mae: 0.2314 - mse: 0.0785
64/86 [=====================>........] - ETA: 0s - loss: 0.0770 - mae: 0.2102 - mse: 0.0770
86/86 [==============================] - 0s 5ms/step - loss: 0.0712 - mae: 0.2020 - mse: 0.0712 - val_loss: 0.0602 - val_mae: 0.1992 - val_mse: 0.0602
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0748 - mae: 0.2207 - mse: 0.0748
64/86 [=====================>........] - ETA: 0s - loss: 0.0641 - mae: 0.1969 - mse: 0.0641
86/86 [==============================] - 0s 4ms/step - loss: 0.0639 - mae: 0.1975 - mse: 0.0639 - val_loss: 0.0667 - val_mae: 0.2203 - val_mse: 0.0667
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0638 - mae: 0.1900 - mse: 0.0638
64/86 [=====================>........] - ETA: 0s - loss: 0.0657 - mae: 0.1936 - mse: 0.0657
86/86 [==============================] - 0s 4ms/step - loss: 0.0606 - mae: 0.1900 - mse: 0.0606 - val_loss: 0.0760 - val_mae: 0.2304 - val_mse: 0.0760
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0562 - mae: 0.1899 - mse: 0.0562
64/86 [=====================>........] - ETA: 0s - loss: 0.0555 - mae: 0.1815 - mse: 0.0555
86/86 [==============================] - 0s 5ms/step - loss: 0.0561 - mae: 0.1836 - mse: 0.0561 - val_loss: 0.0687 - val_mae: 0.2143 - val_mse: 0.0687
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0585 - mae: 0.1936 - mse: 0.0585
64/86 [=====================>........] - ETA: 0s - loss: 0.0514 - mae: 0.1760 - mse: 0.0514
86/86 [==============================] - 0s 5ms/step - loss: 0.0502 - mae: 0.1771 - mse: 0.0502 - val_loss: 0.0656 - val_mae: 0.1953 - val_mse: 0.0656
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0494 - mae: 0.1678 - mse: 0.0494
64/86 [=====================>........] - ETA: 0s - loss: 0.0515 - mae: 0.1781 - mse: 0.0515
86/86 [==============================] - 0s 5ms/step - loss: 0.0474 - mae: 0.1705 - mse: 0.0474 - val_loss: 0.0719 - val_mae: 0.2066 - val_mse: 0.0719
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0502 - mae: 0.1829 - mse: 0.0502
64/86 [=====================>........] - ETA: 0s - loss: 0.0456 - mae: 0.1704 - mse: 0.0456
86/86 [==============================] - 0s 5ms/step - loss: 0.0427 - mae: 0.1641 - mse: 0.0427 - val_loss: 0.0808 - val_mae: 0.2186 - val_mse: 0.0808
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0421 - mae: 0.1624 - mse: 0.0421
64/86 [=====================>........] - ETA: 0s - loss: 0.0514 - mae: 0.1859 - mse: 0.0514
86/86 [==============================] - 0s 5ms/step - loss: 0.0576 - mae: 0.1892 - mse: 0.0576 - val_loss: 0.0800 - val_mae: 0.2165 - val_mse: 0.0800
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0407 - mae: 0.1577 - mse: 0.0407
64/86 [=====================>........] - ETA: 0s - loss: 0.0509 - mae: 0.1771 - mse: 0.0509
86/86 [==============================] - 0s 5ms/step - loss: 0.0452 - mae: 0.1616 - mse: 0.0452 - val_loss: 0.0650 - val_mae: 0.1939 - val_mse: 0.0650
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0283 - mae: 0.1285 - mse: 0.0283
64/86 [=====================>........] - ETA: 0s - loss: 0.0402 - mae: 0.1544 - mse: 0.0402
86/86 [==============================] - 1s 6ms/step - loss: 0.0497 - mae: 0.1703 - mse: 0.0497 - val_loss: 0.0716 - val_mae: 0.1943 - val_mse: 0.0716
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0468 - mae: 0.1733 - mse: 0.0468
64/86 [=====================>........] - ETA: 0s - loss: 0.0386 - mae: 0.1524 - mse: 0.0386
86/86 [==============================] - 0s 5ms/step - loss: 0.0358 - mae: 0.1508 - mse: 0.0358 - val_loss: 0.0857 - val_mae: 0.2046 - val_mse: 0.0857
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0354 - mae: 0.1404 - mse: 0.0354
64/86 [=====================>........] - ETA: 0s - loss: 0.0447 - mae: 0.1540 - mse: 0.0447
86/86 [==============================] - 0s 5ms/step - loss: 0.0429 - mae: 0.1539 - mse: 0.0429 - val_loss: 0.0867 - val_mae: 0.1986 - val_mse: 0.0867
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0271 - mae: 0.1331 - mse: 0.0271
64/86 [=====================>........] - ETA: 0s - loss: 0.0452 - mae: 0.1648 - mse: 0.0452
86/86 [==============================] - 0s 5ms/step - loss: 0.0438 - mae: 0.1638 - mse: 0.0438 - val_loss: 0.0728 - val_mae: 0.1686 - val_mse: 0.0728
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0339 - mae: 0.1399 - mse: 0.0339
64/86 [=====================>........] - ETA: 0s - loss: 0.0447 - mae: 0.1593 - mse: 0.0447
86/86 [==============================] - 0s 6ms/step - loss: 0.0443 - mae: 0.1616 - mse: 0.0443 - val_loss: 0.0717 - val_mae: 0.1762 - val_mse: 0.0717
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0230 - mae: 0.1141 - mse: 0.0230
64/86 [=====================>........] - ETA: 0s - loss: 0.0312 - mae: 0.1282 - mse: 0.0312
86/86 [==============================] - 0s 5ms/step - loss: 0.0307 - mae: 0.1319 - mse: 0.0307 - val_loss: 0.0952 - val_mae: 0.2111 - val_mse: 0.0952
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0256 - mae: 0.1143 - mse: 0.0256
64/86 [=====================>........] - ETA: 0s - loss: 0.0371 - mae: 0.1385 - mse: 0.0371
86/86 [==============================] - 0s 5ms/step - loss: 0.0368 - mae: 0.1457 - mse: 0.0368 - val_loss: 0.0784 - val_mae: 0.1797 - val_mse: 0.0784
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0457 - mae: 0.1543 - mse: 0.0457
64/86 [=====================>........] - ETA: 0s - loss: 0.0381 - mae: 0.1420 - mse: 0.0381
86/86 [==============================] - 0s 5ms/step - loss: 0.0426 - mae: 0.1572 - mse: 0.0426 - val_loss: 0.0763 - val_mae: 0.1673 - val_mse: 0.0763
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0345 - mae: 0.1511 - mse: 0.0345
64/86 [=====================>........] - ETA: 0s - loss: 0.0379 - mae: 0.1582 - mse: 0.0379
86/86 [==============================] - 0s 5ms/step - loss: 0.0412 - mae: 0.1528 - mse: 0.0412 - val_loss: 0.1030 - val_mae: 0.2100 - val_mse: 0.1030
Saving trained model...
99
Testing...
heightdiff= [0.         0.         0.         0.         0.         0.36120605]
average prediction= [6.9140882]
baseline= 5.86
eachuser= [0. 0. 0. 0. 0. 1.]
165 -:- nan
170 -:- nan
173 -:- nan
175 -:- nan
180 -:- nan
155 -:- 0.3612060546875
