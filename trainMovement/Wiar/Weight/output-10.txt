['train-weight-10.py', '1']
65 1
2_155_65_10_csi_a10_14.dat
65 3
65 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
65 9
65 10
65 11
65 12
65 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
65 17
2_155_65_10_csi_a10_13.dat
65 19
65 20
2_155_65_10_csi_a10_3.dat
65 22
65 23
65 24
2_155_65_10_csi_a10_4.dat
65 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
65 29
65 30
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
50 89
50 90
50 91
50 92
50 93
50 94
50 95
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
85 131
85 132
85 133
85 134
85 135
85 136
85 137
85 138
85 139
85 140
85 141
85 142
85 143
85 144
85 145
85 146
85 147
85 148
85 149
85 150
85 151
85 152
85 153
85 154
85 155
85 156
1_180_85_10_csi_a10_28.dat
85 158
85 159
85 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
75 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
75 178
75 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
75 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
75 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
85 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60
 60 60 60 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75
 75 85]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.5585 - mae: 0.6252 - mse: 0.5585
64/87 [=====================>........] - ETA: 0s - loss: 0.4540 - mae: 0.5487 - mse: 0.4540
87/87 [==============================] - 1s 9ms/step - loss: 0.3923 - mae: 0.5086 - mse: 0.3923 - val_loss: 0.1860 - val_mae: 0.3414 - val_mse: 0.1860
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1312 - mae: 0.2859 - mse: 0.1312
64/87 [=====================>........] - ETA: 0s - loss: 0.1652 - mae: 0.3390 - mse: 0.1652
87/87 [==============================] - 0s 5ms/step - loss: 0.1831 - mae: 0.3608 - mse: 0.1831 - val_loss: 0.1223 - val_mae: 0.3070 - val_mse: 0.1223
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1933 - mae: 0.3749 - mse: 0.1933
64/87 [=====================>........] - ETA: 0s - loss: 0.1676 - mae: 0.3562 - mse: 0.1676
87/87 [==============================] - 0s 5ms/step - loss: 0.1519 - mae: 0.3378 - mse: 0.1519 - val_loss: 0.0901 - val_mae: 0.2317 - val_mse: 0.0901
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0827 - mae: 0.2595 - mse: 0.0827
64/87 [=====================>........] - ETA: 0s - loss: 0.0806 - mae: 0.2503 - mse: 0.0806
87/87 [==============================] - 0s 6ms/step - loss: 0.0852 - mae: 0.2561 - mse: 0.0852 - val_loss: 0.0904 - val_mae: 0.2650 - val_mse: 0.0904
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0855 - mae: 0.2513 - mse: 0.0855
64/87 [=====================>........] - ETA: 0s - loss: 0.0819 - mae: 0.2510 - mse: 0.0819
87/87 [==============================] - 0s 5ms/step - loss: 0.0813 - mae: 0.2546 - mse: 0.0813 - val_loss: 0.0948 - val_mae: 0.2771 - val_mse: 0.0948
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0695 - mae: 0.2250 - mse: 0.0695
64/87 [=====================>........] - ETA: 0s - loss: 0.0711 - mae: 0.2325 - mse: 0.0711
87/87 [==============================] - 0s 5ms/step - loss: 0.0683 - mae: 0.2320 - mse: 0.0683 - val_loss: 0.0955 - val_mae: 0.2783 - val_mse: 0.0955
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0710 - mae: 0.2318 - mse: 0.0710
64/87 [=====================>........] - ETA: 0s - loss: 0.0592 - mae: 0.2152 - mse: 0.0592
87/87 [==============================] - 0s 5ms/step - loss: 0.0583 - mae: 0.2149 - mse: 0.0583 - val_loss: 0.0831 - val_mae: 0.2636 - val_mse: 0.0831
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0492 - mae: 0.1948 - mse: 0.0492
64/87 [=====================>........] - ETA: 0s - loss: 0.0590 - mae: 0.2147 - mse: 0.0590
87/87 [==============================] - 0s 5ms/step - loss: 0.0540 - mae: 0.2041 - mse: 0.0540 - val_loss: 0.0693 - val_mae: 0.2432 - val_mse: 0.0693
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0498 - mae: 0.1853 - mse: 0.0498
64/87 [=====================>........] - ETA: 0s - loss: 0.0614 - mae: 0.2065 - mse: 0.0614
87/87 [==============================] - 0s 5ms/step - loss: 0.0579 - mae: 0.2015 - mse: 0.0579 - val_loss: 0.0704 - val_mae: 0.2422 - val_mse: 0.0704
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0610 - mae: 0.2072 - mse: 0.0610
64/87 [=====================>........] - ETA: 0s - loss: 0.0535 - mae: 0.1918 - mse: 0.0535
87/87 [==============================] - 0s 5ms/step - loss: 0.0538 - mae: 0.1947 - mse: 0.0538 - val_loss: 0.0832 - val_mae: 0.2538 - val_mse: 0.0832
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0414 - mae: 0.1720 - mse: 0.0414
64/87 [=====================>........] - ETA: 0s - loss: 0.0404 - mae: 0.1722 - mse: 0.0404
87/87 [==============================] - 0s 5ms/step - loss: 0.0474 - mae: 0.1849 - mse: 0.0474 - val_loss: 0.0855 - val_mae: 0.2501 - val_mse: 0.0855
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0661 - mae: 0.2150 - mse: 0.0661
64/87 [=====================>........] - ETA: 0s - loss: 0.0564 - mae: 0.1957 - mse: 0.0564
87/87 [==============================] - 0s 5ms/step - loss: 0.0586 - mae: 0.2014 - mse: 0.0586 - val_loss: 0.0741 - val_mae: 0.2304 - val_mse: 0.0741
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0606 - mae: 0.1858 - mse: 0.0606
64/87 [=====================>........] - ETA: 0s - loss: 0.0484 - mae: 0.1714 - mse: 0.0484
87/87 [==============================] - 0s 5ms/step - loss: 0.0512 - mae: 0.1760 - mse: 0.0512 - val_loss: 0.0724 - val_mae: 0.2238 - val_mse: 0.0724
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0378 - mae: 0.1516 - mse: 0.0378
64/87 [=====================>........] - ETA: 0s - loss: 0.0599 - mae: 0.1913 - mse: 0.0599
87/87 [==============================] - 0s 5ms/step - loss: 0.0538 - mae: 0.1815 - mse: 0.0538 - val_loss: 0.0703 - val_mae: 0.2172 - val_mse: 0.0703
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0578 - mae: 0.1959 - mse: 0.0578
64/87 [=====================>........] - ETA: 0s - loss: 0.0552 - mae: 0.1960 - mse: 0.0552
87/87 [==============================] - 0s 4ms/step - loss: 0.0483 - mae: 0.1791 - mse: 0.0483 - val_loss: 0.0625 - val_mae: 0.2017 - val_mse: 0.0625
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0572 - mae: 0.1720 - mse: 0.0572
64/87 [=====================>........] - ETA: 0s - loss: 0.0561 - mae: 0.1776 - mse: 0.0561
87/87 [==============================] - 0s 5ms/step - loss: 0.0545 - mae: 0.1717 - mse: 0.0545 - val_loss: 0.0719 - val_mae: 0.2136 - val_mse: 0.0719
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0323 - mae: 0.1384 - mse: 0.0323
64/87 [=====================>........] - ETA: 0s - loss: 0.0395 - mae: 0.1544 - mse: 0.0395
87/87 [==============================] - 0s 5ms/step - loss: 0.0395 - mae: 0.1555 - mse: 0.0395 - val_loss: 0.0780 - val_mae: 0.2199 - val_mse: 0.0780
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0424 - mae: 0.1711 - mse: 0.0424
64/87 [=====================>........] - ETA: 0s - loss: 0.0514 - mae: 0.1762 - mse: 0.0514
87/87 [==============================] - 0s 5ms/step - loss: 0.0553 - mae: 0.1795 - mse: 0.0553 - val_loss: 0.0683 - val_mae: 0.2023 - val_mse: 0.0683
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0613 - mae: 0.2065 - mse: 0.0613
64/87 [=====================>........] - ETA: 0s - loss: 0.0609 - mae: 0.1953 - mse: 0.0609
87/87 [==============================] - 0s 5ms/step - loss: 0.0525 - mae: 0.1762 - mse: 0.0525 - val_loss: 0.0602 - val_mae: 0.1831 - val_mse: 0.0602
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0237 - mae: 0.1242 - mse: 0.0237
64/87 [=====================>........] - ETA: 0s - loss: 0.0324 - mae: 0.1434 - mse: 0.0324
87/87 [==============================] - 0s 5ms/step - loss: 0.0418 - mae: 0.1596 - mse: 0.0418 - val_loss: 0.0652 - val_mae: 0.1870 - val_mse: 0.0652
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0217 - mae: 0.1185 - mse: 0.0217
64/87 [=====================>........] - ETA: 0s - loss: 0.0322 - mae: 0.1363 - mse: 0.0322
87/87 [==============================] - 0s 5ms/step - loss: 0.0328 - mae: 0.1392 - mse: 0.0328 - val_loss: 0.0782 - val_mae: 0.2160 - val_mse: 0.0782
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0641 - mae: 0.2013 - mse: 0.0641
64/87 [=====================>........] - ETA: 0s - loss: 0.0472 - mae: 0.1662 - mse: 0.0472
87/87 [==============================] - 0s 5ms/step - loss: 0.0463 - mae: 0.1650 - mse: 0.0463 - val_loss: 0.0708 - val_mae: 0.2033 - val_mse: 0.0708
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0453 - mae: 0.1641 - mse: 0.0453
64/87 [=====================>........] - ETA: 0s - loss: 0.0361 - mae: 0.1368 - mse: 0.0361
87/87 [==============================] - 0s 4ms/step - loss: 0.0374 - mae: 0.1436 - mse: 0.0374 - val_loss: 0.0638 - val_mae: 0.1827 - val_mse: 0.0638
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0339 - mae: 0.1429 - mse: 0.0339
64/87 [=====================>........] - ETA: 0s - loss: 0.0420 - mae: 0.1556 - mse: 0.0420
87/87 [==============================] - 0s 5ms/step - loss: 0.0361 - mae: 0.1432 - mse: 0.0361 - val_loss: 0.0518 - val_mae: 0.1543 - val_mse: 0.0518
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0350 - mae: 0.1314 - mse: 0.0350
64/87 [=====================>........] - ETA: 0s - loss: 0.0325 - mae: 0.1271 - mse: 0.0325
87/87 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1333 - mse: 0.0344 - val_loss: 0.0698 - val_mae: 0.1927 - val_mse: 0.0698
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0561 - mae: 0.1715 - mse: 0.0561
64/87 [=====================>........] - ETA: 0s - loss: 0.0452 - mae: 0.1552 - mse: 0.0452
87/87 [==============================] - 0s 5ms/step - loss: 0.0464 - mae: 0.1569 - mse: 0.0464 - val_loss: 0.0951 - val_mae: 0.2343 - val_mse: 0.0951
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0343 - mae: 0.1353 - mse: 0.0343
64/87 [=====================>........] - ETA: 0s - loss: 0.0367 - mae: 0.1428 - mse: 0.0367
87/87 [==============================] - 0s 5ms/step - loss: 0.0336 - mae: 0.1340 - mse: 0.0336 - val_loss: 0.0580 - val_mae: 0.1567 - val_mse: 0.0580
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0298 - mae: 0.1292 - mse: 0.0298
64/87 [=====================>........] - ETA: 0s - loss: 0.0357 - mae: 0.1345 - mse: 0.0357
87/87 [==============================] - 0s 5ms/step - loss: 0.0324 - mae: 0.1327 - mse: 0.0324 - val_loss: 0.0473 - val_mae: 0.1423 - val_mse: 0.0473
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0358 - mae: 0.1270 - mse: 0.0358
64/87 [=====================>........] - ETA: 0s - loss: 0.0422 - mae: 0.1485 - mse: 0.0422
87/87 [==============================] - 1s 6ms/step - loss: 0.0385 - mae: 0.1434 - mse: 0.0385 - val_loss: 0.0617 - val_mae: 0.1766 - val_mse: 0.0617
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0256 - mae: 0.1283 - mse: 0.0256
64/87 [=====================>........] - ETA: 0s - loss: 0.0337 - mae: 0.1347 - mse: 0.0337
87/87 [==============================] - 0s 5ms/step - loss: 0.0374 - mae: 0.1360 - mse: 0.0374 - val_loss: 0.0769 - val_mae: 0.2183 - val_mse: 0.0769
Saving trained model...
98
Testing...
heightdiff= [0.         0.         0.         3.29151535 0.         0.        ]
average prediction= [6.132703]
baseline= 10.7
eachuser= [0. 0. 0. 8. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.4114394187927246
85 -:- nan
60 -:- nan
['train-weight-10.py', '1']
65 1
2_155_65_10_csi_a10_14.dat
65 3
65 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
65 9
65 10
65 11
65 12
65 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
65 17
2_155_65_10_csi_a10_13.dat
65 19
65 20
2_155_65_10_csi_a10_3.dat
65 22
65 23
65 24
2_155_65_10_csi_a10_4.dat
65 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
65 29
65 30
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
50 89
50 90
50 91
50 92
50 93
50 94
50 95
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
85 131
85 132
85 133
85 134
85 135
85 136
85 137
85 138
85 139
85 140
85 141
85 142
85 143
85 144
85 145
85 146
85 147
85 148
85 149
85 150
85 151
85 152
85 153
85 154
85 155
85 156
1_180_85_10_csi_a10_28.dat
85 158
85 159
85 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
75 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
75 178
75 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
75 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
75 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
85 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60
 60 60 60 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75
 75 85]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.5178 - mae: 0.6164 - mse: 0.5178
64/87 [=====================>........] - ETA: 0s - loss: 0.4184 - mae: 0.5362 - mse: 0.4184
87/87 [==============================] - 1s 9ms/step - loss: 0.3750 - mae: 0.5010 - mse: 0.3750 - val_loss: 0.1206 - val_mae: 0.2928 - val_mse: 0.1206
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1352 - mae: 0.2798 - mse: 0.1352
64/87 [=====================>........] - ETA: 0s - loss: 0.1369 - mae: 0.2928 - mse: 0.1369
87/87 [==============================] - 1s 6ms/step - loss: 0.1296 - mae: 0.2960 - mse: 0.1296 - val_loss: 0.0473 - val_mae: 0.1821 - val_mse: 0.0473
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0987 - mae: 0.2764 - mse: 0.0987
64/87 [=====================>........] - ETA: 0s - loss: 0.1105 - mae: 0.2828 - mse: 0.1105
87/87 [==============================] - 0s 6ms/step - loss: 0.1169 - mae: 0.2915 - mse: 0.1169 - val_loss: 0.0777 - val_mae: 0.2188 - val_mse: 0.0777
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1031 - mae: 0.2644 - mse: 0.1031
64/87 [=====================>........] - ETA: 0s - loss: 0.0983 - mae: 0.2592 - mse: 0.0983
87/87 [==============================] - 1s 6ms/step - loss: 0.0999 - mae: 0.2624 - mse: 0.0999 - val_loss: 0.0473 - val_mae: 0.2099 - val_mse: 0.0473
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0816 - mae: 0.2506 - mse: 0.0816
64/87 [=====================>........] - ETA: 0s - loss: 0.0748 - mae: 0.2421 - mse: 0.0748
87/87 [==============================] - 0s 6ms/step - loss: 0.0719 - mae: 0.2395 - mse: 0.0719 - val_loss: 0.0580 - val_mae: 0.2099 - val_mse: 0.0580
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0712 - mae: 0.2399 - mse: 0.0712
64/87 [=====================>........] - ETA: 0s - loss: 0.0677 - mae: 0.2332 - mse: 0.0677
87/87 [==============================] - 0s 6ms/step - loss: 0.0671 - mae: 0.2321 - mse: 0.0671 - val_loss: 0.0633 - val_mae: 0.2117 - val_mse: 0.0633
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0746 - mae: 0.2406 - mse: 0.0746
64/87 [=====================>........] - ETA: 0s - loss: 0.0675 - mae: 0.2304 - mse: 0.0675
87/87 [==============================] - 0s 6ms/step - loss: 0.0681 - mae: 0.2327 - mse: 0.0681 - val_loss: 0.0535 - val_mae: 0.2151 - val_mse: 0.0535
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0613 - mae: 0.2159 - mse: 0.0613
64/87 [=====================>........] - ETA: 0s - loss: 0.0554 - mae: 0.2052 - mse: 0.0554
87/87 [==============================] - 1s 6ms/step - loss: 0.0491 - mae: 0.1885 - mse: 0.0491 - val_loss: 0.0536 - val_mae: 0.2145 - val_mse: 0.0536
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0524 - mae: 0.1838 - mse: 0.0524
64/87 [=====================>........] - ETA: 0s - loss: 0.0617 - mae: 0.1940 - mse: 0.0617
87/87 [==============================] - 1s 6ms/step - loss: 0.0595 - mae: 0.1908 - mse: 0.0595 - val_loss: 0.0496 - val_mae: 0.1990 - val_mse: 0.0496
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0353 - mae: 0.1455 - mse: 0.0353
64/87 [=====================>........] - ETA: 0s - loss: 0.0414 - mae: 0.1589 - mse: 0.0414
87/87 [==============================] - 1s 6ms/step - loss: 0.0448 - mae: 0.1612 - mse: 0.0448 - val_loss: 0.0385 - val_mae: 0.1804 - val_mse: 0.0385
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0295 - mae: 0.1405 - mse: 0.0295
64/87 [=====================>........] - ETA: 0s - loss: 0.0339 - mae: 0.1469 - mse: 0.0339
87/87 [==============================] - 0s 6ms/step - loss: 0.0367 - mae: 0.1505 - mse: 0.0367 - val_loss: 0.0343 - val_mae: 0.1667 - val_mse: 0.0343
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0302 - mae: 0.1384 - mse: 0.0302
64/87 [=====================>........] - ETA: 0s - loss: 0.0291 - mae: 0.1332 - mse: 0.0291
87/87 [==============================] - 0s 6ms/step - loss: 0.0293 - mae: 0.1358 - mse: 0.0293 - val_loss: 0.0324 - val_mae: 0.1558 - val_mse: 0.0324
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0387 - mae: 0.1471 - mse: 0.0387
64/87 [=====================>........] - ETA: 0s - loss: 0.0334 - mae: 0.1365 - mse: 0.0334
87/87 [==============================] - 0s 6ms/step - loss: 0.0349 - mae: 0.1392 - mse: 0.0349 - val_loss: 0.0333 - val_mae: 0.1510 - val_mse: 0.0333
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0225 - mae: 0.0977 - mse: 0.0225
64/87 [=====================>........] - ETA: 0s - loss: 0.0322 - mae: 0.1268 - mse: 0.0322
87/87 [==============================] - 1s 6ms/step - loss: 0.0325 - mae: 0.1296 - mse: 0.0325 - val_loss: 0.0327 - val_mae: 0.1501 - val_mse: 0.0327
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0504 - mae: 0.1685 - mse: 0.0504
64/87 [=====================>........] - ETA: 0s - loss: 0.0377 - mae: 0.1377 - mse: 0.0377
87/87 [==============================] - 1s 6ms/step - loss: 0.0420 - mae: 0.1435 - mse: 0.0420 - val_loss: 0.0335 - val_mae: 0.1576 - val_mse: 0.0335
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0249 - mae: 0.1199 - mse: 0.0249
64/87 [=====================>........] - ETA: 0s - loss: 0.0281 - mae: 0.1226 - mse: 0.0281
87/87 [==============================] - 0s 6ms/step - loss: 0.0285 - mae: 0.1233 - mse: 0.0285 - val_loss: 0.0334 - val_mae: 0.1615 - val_mse: 0.0334
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0286 - mae: 0.1197 - mse: 0.0286
64/87 [=====================>........] - ETA: 0s - loss: 0.0339 - mae: 0.1300 - mse: 0.0339
87/87 [==============================] - 0s 6ms/step - loss: 0.0353 - mae: 0.1321 - mse: 0.0353 - val_loss: 0.0361 - val_mae: 0.1700 - val_mse: 0.0361
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0458 - mae: 0.1668 - mse: 0.0458
64/87 [=====================>........] - ETA: 0s - loss: 0.0423 - mae: 0.1464 - mse: 0.0423
87/87 [==============================] - 1s 6ms/step - loss: 0.0347 - mae: 0.1280 - mse: 0.0347 - val_loss: 0.0347 - val_mae: 0.1618 - val_mse: 0.0347
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0192 - mae: 0.0980 - mse: 0.0192
64/87 [=====================>........] - ETA: 0s - loss: 0.0231 - mae: 0.1025 - mse: 0.0231
87/87 [==============================] - 1s 6ms/step - loss: 0.0272 - mae: 0.1144 - mse: 0.0272 - val_loss: 0.0345 - val_mae: 0.1536 - val_mse: 0.0345
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0311 - mae: 0.1253 - mse: 0.0311
64/87 [=====================>........] - ETA: 0s - loss: 0.0259 - mae: 0.1106 - mse: 0.0259
87/87 [==============================] - 1s 6ms/step - loss: 0.0265 - mae: 0.1134 - mse: 0.0265 - val_loss: 0.0299 - val_mae: 0.1391 - val_mse: 0.0299
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0478 - mae: 0.1574 - mse: 0.0478
64/87 [=====================>........] - ETA: 0s - loss: 0.0421 - mae: 0.1373 - mse: 0.0421
87/87 [==============================] - 1s 6ms/step - loss: 0.0407 - mae: 0.1346 - mse: 0.0407 - val_loss: 0.0265 - val_mae: 0.1261 - val_mse: 0.0265
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0206 - mae: 0.0999 - mse: 0.0206
64/87 [=====================>........] - ETA: 0s - loss: 0.0329 - mae: 0.1337 - mse: 0.0329
87/87 [==============================] - 1s 6ms/step - loss: 0.0308 - mae: 0.1248 - mse: 0.0308 - val_loss: 0.0277 - val_mae: 0.1313 - val_mse: 0.0277
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0360 - mae: 0.1311 - mse: 0.0360
64/87 [=====================>........] - ETA: 0s - loss: 0.0320 - mae: 0.1276 - mse: 0.0320
87/87 [==============================] - 0s 6ms/step - loss: 0.0348 - mae: 0.1316 - mse: 0.0348 - val_loss: 0.0279 - val_mae: 0.1342 - val_mse: 0.0279
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0404 - mae: 0.1334 - mse: 0.0404
64/87 [=====================>........] - ETA: 0s - loss: 0.0428 - mae: 0.1452 - mse: 0.0428
87/87 [==============================] - 1s 6ms/step - loss: 0.0407 - mae: 0.1385 - mse: 0.0407 - val_loss: 0.0307 - val_mae: 0.1448 - val_mse: 0.0307
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0317 - mae: 0.1271 - mse: 0.0317
64/87 [=====================>........] - ETA: 0s - loss: 0.0258 - mae: 0.1158 - mse: 0.0258
87/87 [==============================] - 1s 6ms/step - loss: 0.0326 - mae: 0.1267 - mse: 0.0326 - val_loss: 0.0355 - val_mae: 0.1595 - val_mse: 0.0355
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0450 - mae: 0.1445 - mse: 0.0450
64/87 [=====================>........] - ETA: 0s - loss: 0.0353 - mae: 0.1229 - mse: 0.0353
87/87 [==============================] - 1s 6ms/step - loss: 0.0321 - mae: 0.1145 - mse: 0.0321 - val_loss: 0.0391 - val_mae: 0.1716 - val_mse: 0.0391
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0257 - mae: 0.1072 - mse: 0.0257
64/87 [=====================>........] - ETA: 0s - loss: 0.0256 - mae: 0.1119 - mse: 0.0256
87/87 [==============================] - 1s 6ms/step - loss: 0.0277 - mae: 0.1139 - mse: 0.0277 - val_loss: 0.0401 - val_mae: 0.1793 - val_mse: 0.0401
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0264 - mae: 0.1061 - mse: 0.0264
64/87 [=====================>........] - ETA: 0s - loss: 0.0286 - mae: 0.1169 - mse: 0.0286
87/87 [==============================] - 1s 6ms/step - loss: 0.0321 - mae: 0.1212 - mse: 0.0321 - val_loss: 0.0374 - val_mae: 0.1702 - val_mse: 0.0374
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0407 - mae: 0.1543 - mse: 0.0407
64/87 [=====================>........] - ETA: 0s - loss: 0.0361 - mae: 0.1327 - mse: 0.0361
87/87 [==============================] - 1s 6ms/step - loss: 0.0391 - mae: 0.1346 - mse: 0.0391 - val_loss: 0.0303 - val_mae: 0.1466 - val_mse: 0.0303
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0312 - mae: 0.1254 - mse: 0.0312
64/87 [=====================>........] - ETA: 0s - loss: 0.0268 - mae: 0.1131 - mse: 0.0268
87/87 [==============================] - 0s 6ms/step - loss: 0.0305 - mae: 0.1195 - mse: 0.0305 - val_loss: 0.0283 - val_mae: 0.1342 - val_mse: 0.0283
Saving trained model...
98
Testing...
heightdiff= [0.         0.         0.         1.66975403 0.         0.        ]
average prediction= [6.337313]
baseline= 10.7
eachuser= [0. 0. 0. 4. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.4174385070800781
85 -:- nan
60 -:- nan
['train-weight-10.py', '1']
65 1
2_155_65_10_csi_a10_14.dat
65 3
65 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
65 9
65 10
65 11
65 12
65 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
65 17
2_155_65_10_csi_a10_13.dat
65 19
65 20
2_155_65_10_csi_a10_3.dat
65 22
65 23
65 24
2_155_65_10_csi_a10_4.dat
65 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
65 29
65 30
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
50 89
50 90
50 91
50 92
50 93
50 94
50 95
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
85 131
85 132
85 133
85 134
85 135
85 136
85 137
85 138
85 139
85 140
85 141
85 142
85 143
85 144
85 145
85 146
85 147
85 148
85 149
85 150
85 151
85 152
85 153
85 154
85 155
85 156
1_180_85_10_csi_a10_28.dat
85 158
85 159
85 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
75 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
75 178
75 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
75 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
75 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
85 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60
 60 60 60 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75
 75 85]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.3201 - mae: 0.4650 - mse: 0.3201
64/87 [=====================>........] - ETA: 0s - loss: 0.3040 - mae: 0.4622 - mse: 0.3040
87/87 [==============================] - 1s 9ms/step - loss: 0.2808 - mae: 0.4473 - mse: 0.2808 - val_loss: 0.1467 - val_mae: 0.3602 - val_mse: 0.1467
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1367 - mae: 0.3312 - mse: 0.1367
64/87 [=====================>........] - ETA: 0s - loss: 0.1372 - mae: 0.3242 - mse: 0.1372
87/87 [==============================] - 0s 5ms/step - loss: 0.1394 - mae: 0.3119 - mse: 0.1394 - val_loss: 0.1259 - val_mae: 0.3140 - val_mse: 0.1259
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1461 - mae: 0.3052 - mse: 0.1461
64/87 [=====================>........] - ETA: 0s - loss: 0.1237 - mae: 0.2821 - mse: 0.1237
87/87 [==============================] - 0s 5ms/step - loss: 0.1249 - mae: 0.2909 - mse: 0.1249 - val_loss: 0.0995 - val_mae: 0.3067 - val_mse: 0.0995
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1153 - mae: 0.3180 - mse: 0.1153
64/87 [=====================>........] - ETA: 0s - loss: 0.1059 - mae: 0.3005 - mse: 0.1059
87/87 [==============================] - 0s 5ms/step - loss: 0.0993 - mae: 0.2875 - mse: 0.0993 - val_loss: 0.1082 - val_mae: 0.3028 - val_mse: 0.1082
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0829 - mae: 0.2633 - mse: 0.0829
64/87 [=====================>........] - ETA: 0s - loss: 0.0970 - mae: 0.2761 - mse: 0.0970
87/87 [==============================] - 0s 5ms/step - loss: 0.0991 - mae: 0.2788 - mse: 0.0991 - val_loss: 0.1007 - val_mae: 0.2846 - val_mse: 0.1007
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0889 - mae: 0.2678 - mse: 0.0889
64/87 [=====================>........] - ETA: 0s - loss: 0.0767 - mae: 0.2527 - mse: 0.0767
87/87 [==============================] - 0s 5ms/step - loss: 0.0802 - mae: 0.2558 - mse: 0.0802 - val_loss: 0.0685 - val_mae: 0.2446 - val_mse: 0.0685
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0977 - mae: 0.2755 - mse: 0.0977
64/87 [=====================>........] - ETA: 0s - loss: 0.0889 - mae: 0.2599 - mse: 0.0889
87/87 [==============================] - 0s 5ms/step - loss: 0.0804 - mae: 0.2468 - mse: 0.0804 - val_loss: 0.0389 - val_mae: 0.1922 - val_mse: 0.0389
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0562 - mae: 0.1959 - mse: 0.0562
64/87 [=====================>........] - ETA: 0s - loss: 0.0898 - mae: 0.2422 - mse: 0.0898
87/87 [==============================] - 0s 5ms/step - loss: 0.0905 - mae: 0.2383 - mse: 0.0905 - val_loss: 0.0336 - val_mae: 0.1745 - val_mse: 0.0336
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0702 - mae: 0.2233 - mse: 0.0702
64/87 [=====================>........] - ETA: 0s - loss: 0.0702 - mae: 0.2200 - mse: 0.0702
87/87 [==============================] - 0s 5ms/step - loss: 0.0669 - mae: 0.2173 - mse: 0.0669 - val_loss: 0.0447 - val_mae: 0.1835 - val_mse: 0.0447
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0478 - mae: 0.1995 - mse: 0.0478
64/87 [=====================>........] - ETA: 0s - loss: 0.0444 - mae: 0.1866 - mse: 0.0444
87/87 [==============================] - 0s 5ms/step - loss: 0.0517 - mae: 0.1991 - mse: 0.0517 - val_loss: 0.0557 - val_mae: 0.1870 - val_mse: 0.0557
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0915 - mae: 0.2629 - mse: 0.0915
64/87 [=====================>........] - ETA: 0s - loss: 0.0808 - mae: 0.2384 - mse: 0.0808
87/87 [==============================] - 0s 5ms/step - loss: 0.0721 - mae: 0.2226 - mse: 0.0721 - val_loss: 0.0492 - val_mae: 0.1669 - val_mse: 0.0492
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0681 - mae: 0.2199 - mse: 0.0681
64/87 [=====================>........] - ETA: 0s - loss: 0.0680 - mae: 0.2194 - mse: 0.0680
87/87 [==============================] - 0s 5ms/step - loss: 0.0579 - mae: 0.1997 - mse: 0.0579 - val_loss: 0.0366 - val_mae: 0.1386 - val_mse: 0.0366
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0577 - mae: 0.1841 - mse: 0.0577
64/87 [=====================>........] - ETA: 0s - loss: 0.0528 - mae: 0.1850 - mse: 0.0528
87/87 [==============================] - 0s 5ms/step - loss: 0.0498 - mae: 0.1785 - mse: 0.0498 - val_loss: 0.0259 - val_mae: 0.1136 - val_mse: 0.0259
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0345 - mae: 0.1555 - mse: 0.0345
64/87 [=====================>........] - ETA: 0s - loss: 0.0519 - mae: 0.1864 - mse: 0.0519
87/87 [==============================] - 0s 5ms/step - loss: 0.0524 - mae: 0.1891 - mse: 0.0524 - val_loss: 0.0250 - val_mae: 0.1129 - val_mse: 0.0250
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0518 - mae: 0.1822 - mse: 0.0518
64/87 [=====================>........] - ETA: 0s - loss: 0.0463 - mae: 0.1681 - mse: 0.0463
87/87 [==============================] - 0s 5ms/step - loss: 0.0499 - mae: 0.1742 - mse: 0.0499 - val_loss: 0.0288 - val_mae: 0.1195 - val_mse: 0.0288
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0664 - mae: 0.1873 - mse: 0.0664
64/87 [=====================>........] - ETA: 0s - loss: 0.0630 - mae: 0.1956 - mse: 0.0630
87/87 [==============================] - 1s 12ms/step - loss: 0.0552 - mae: 0.1758 - mse: 0.0552 - val_loss: 0.0347 - val_mae: 0.1266 - val_mse: 0.0347
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0526 - mae: 0.1800 - mse: 0.0526
64/87 [=====================>........] - ETA: 0s - loss: 0.0546 - mae: 0.1823 - mse: 0.0546
87/87 [==============================] - 1s 11ms/step - loss: 0.0500 - mae: 0.1752 - mse: 0.0500 - val_loss: 0.0314 - val_mae: 0.1169 - val_mse: 0.0314
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0452 - mae: 0.1625 - mse: 0.0452
64/87 [=====================>........] - ETA: 0s - loss: 0.0433 - mae: 0.1591 - mse: 0.0433
87/87 [==============================] - 1s 7ms/step - loss: 0.0420 - mae: 0.1529 - mse: 0.0420 - val_loss: 0.0325 - val_mae: 0.1150 - val_mse: 0.0325
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0445 - mae: 0.1747 - mse: 0.0445
64/87 [=====================>........] - ETA: 0s - loss: 0.0427 - mae: 0.1612 - mse: 0.0427
87/87 [==============================] - 1s 6ms/step - loss: 0.0393 - mae: 0.1546 - mse: 0.0393 - val_loss: 0.0251 - val_mae: 0.0975 - val_mse: 0.0251
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0671 - mae: 0.1881 - mse: 0.0671
64/87 [=====================>........] - ETA: 0s - loss: 0.0518 - mae: 0.1730 - mse: 0.0518
87/87 [==============================] - 1s 6ms/step - loss: 0.0531 - mae: 0.1731 - mse: 0.0531 - val_loss: 0.0273 - val_mae: 0.1016 - val_mse: 0.0273
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0465 - mae: 0.1526 - mse: 0.0465
64/87 [=====================>........] - ETA: 0s - loss: 0.0430 - mae: 0.1570 - mse: 0.0430
87/87 [==============================] - 0s 5ms/step - loss: 0.0428 - mae: 0.1546 - mse: 0.0428 - val_loss: 0.0375 - val_mae: 0.1240 - val_mse: 0.0375
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0530 - mae: 0.1720 - mse: 0.0530
64/87 [=====================>........] - ETA: 0s - loss: 0.0538 - mae: 0.1788 - mse: 0.0538
87/87 [==============================] - 1s 6ms/step - loss: 0.0499 - mae: 0.1701 - mse: 0.0499 - val_loss: 0.0361 - val_mae: 0.1204 - val_mse: 0.0361
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0527 - mae: 0.1791 - mse: 0.0527
64/87 [=====================>........] - ETA: 0s - loss: 0.0457 - mae: 0.1602 - mse: 0.0457
87/87 [==============================] - 1s 6ms/step - loss: 0.0435 - mae: 0.1567 - mse: 0.0435 - val_loss: 0.0282 - val_mae: 0.0998 - val_mse: 0.0282
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0446 - mae: 0.1639 - mse: 0.0446
64/87 [=====================>........] - ETA: 0s - loss: 0.0514 - mae: 0.1758 - mse: 0.0514
87/87 [==============================] - 0s 6ms/step - loss: 0.0487 - mae: 0.1671 - mse: 0.0487 - val_loss: 0.0278 - val_mae: 0.1000 - val_mse: 0.0278
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0292 - mae: 0.1188 - mse: 0.0292
64/87 [=====================>........] - ETA: 0s - loss: 0.0409 - mae: 0.1552 - mse: 0.0409
87/87 [==============================] - 0s 5ms/step - loss: 0.0409 - mae: 0.1502 - mse: 0.0409 - val_loss: 0.0307 - val_mae: 0.1164 - val_mse: 0.0307
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0354 - mae: 0.1426 - mse: 0.0354
64/87 [=====================>........] - ETA: 0s - loss: 0.0358 - mae: 0.1391 - mse: 0.0358
87/87 [==============================] - 0s 5ms/step - loss: 0.0441 - mae: 0.1526 - mse: 0.0441 - val_loss: 0.0300 - val_mae: 0.1197 - val_mse: 0.0300
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0461 - mae: 0.1631 - mse: 0.0461
64/87 [=====================>........] - ETA: 0s - loss: 0.0398 - mae: 0.1447 - mse: 0.0398
87/87 [==============================] - 1s 6ms/step - loss: 0.0407 - mae: 0.1486 - mse: 0.0407 - val_loss: 0.0261 - val_mae: 0.1095 - val_mse: 0.0261
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0280 - mae: 0.1224 - mse: 0.0280
64/87 [=====================>........] - ETA: 0s - loss: 0.0356 - mae: 0.1457 - mse: 0.0356
87/87 [==============================] - 0s 6ms/step - loss: 0.0343 - mae: 0.1435 - mse: 0.0343 - val_loss: 0.0272 - val_mae: 0.1082 - val_mse: 0.0272
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0345 - mae: 0.1364 - mse: 0.0345
64/87 [=====================>........] - ETA: 0s - loss: 0.0460 - mae: 0.1537 - mse: 0.0460
87/87 [==============================] - 1s 6ms/step - loss: 0.0413 - mae: 0.1469 - mse: 0.0413 - val_loss: 0.0311 - val_mae: 0.1090 - val_mse: 0.0311
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0410 - mae: 0.1679 - mse: 0.0410
64/87 [=====================>........] - ETA: 0s - loss: 0.0441 - mae: 0.1632 - mse: 0.0441
87/87 [==============================] - 1s 6ms/step - loss: 0.0384 - mae: 0.1504 - mse: 0.0384 - val_loss: 0.0259 - val_mae: 0.0948 - val_mse: 0.0259
Saving trained model...
98
Testing...
heightdiff= [0.        0.        0.        5.2156868 0.        0.       ]
average prediction= [4.691366]
baseline= 11.7
eachuser= [0. 0. 0. 5. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.0431373596191407
85 -:- nan
60 -:- nan
['train-weight-10.py', '1']
65 1
2_155_65_10_csi_a10_14.dat
65 3
65 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
65 9
65 10
65 11
65 12
65 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
65 17
2_155_65_10_csi_a10_13.dat
65 19
65 20
2_155_65_10_csi_a10_3.dat
65 22
65 23
65 24
2_155_65_10_csi_a10_4.dat
65 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
65 29
65 30
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
50 89
50 90
50 91
50 92
50 93
50 94
50 95
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
85 131
85 132
85 133
85 134
85 135
85 136
85 137
85 138
85 139
85 140
85 141
85 142
85 143
85 144
85 145
85 146
85 147
85 148
85 149
85 150
85 151
85 152
85 153
85 154
85 155
85 156
1_180_85_10_csi_a10_28.dat
85 158
85 159
85 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
75 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
75 178
75 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
75 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
75 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
85 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60
 60 60 60 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75
 75 85]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.5655 - mae: 0.6574 - mse: 0.5655
64/87 [=====================>........] - ETA: 0s - loss: 0.4890 - mae: 0.5873 - mse: 0.4890
87/87 [==============================] - 1s 9ms/step - loss: 0.4077 - mae: 0.5205 - mse: 0.4077 - val_loss: 0.2045 - val_mae: 0.3860 - val_mse: 0.2045
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2043 - mae: 0.3874 - mse: 0.2043
64/87 [=====================>........] - ETA: 0s - loss: 0.1607 - mae: 0.3414 - mse: 0.1607
87/87 [==============================] - 0s 6ms/step - loss: 0.1556 - mae: 0.3317 - mse: 0.1556 - val_loss: 0.1042 - val_mae: 0.3010 - val_mse: 0.1042
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1356 - mae: 0.3064 - mse: 0.1356
64/87 [=====================>........] - ETA: 0s - loss: 0.1634 - mae: 0.3392 - mse: 0.1634
87/87 [==============================] - 0s 6ms/step - loss: 0.1530 - mae: 0.3223 - mse: 0.1530 - val_loss: 0.0904 - val_mae: 0.2867 - val_mse: 0.0904
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0978 - mae: 0.2558 - mse: 0.0978
64/87 [=====================>........] - ETA: 0s - loss: 0.1095 - mae: 0.2800 - mse: 0.1095
87/87 [==============================] - 0s 6ms/step - loss: 0.1101 - mae: 0.2819 - mse: 0.1101 - val_loss: 0.0910 - val_mae: 0.2862 - val_mse: 0.0910
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1060 - mae: 0.2942 - mse: 0.1060
64/87 [=====================>........] - ETA: 0s - loss: 0.0962 - mae: 0.2821 - mse: 0.0962
87/87 [==============================] - 0s 5ms/step - loss: 0.1080 - mae: 0.2966 - mse: 0.1080 - val_loss: 0.1093 - val_mae: 0.2862 - val_mse: 0.1093
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1080 - mae: 0.2756 - mse: 0.1080
64/87 [=====================>........] - ETA: 0s - loss: 0.1052 - mae: 0.2858 - mse: 0.1052
87/87 [==============================] - 0s 6ms/step - loss: 0.0963 - mae: 0.2724 - mse: 0.0963 - val_loss: 0.0990 - val_mae: 0.2775 - val_mse: 0.0990
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0896 - mae: 0.2696 - mse: 0.0896
64/87 [=====================>........] - ETA: 0s - loss: 0.0900 - mae: 0.2732 - mse: 0.0900
87/87 [==============================] - 0s 6ms/step - loss: 0.0962 - mae: 0.2795 - mse: 0.0962 - val_loss: 0.0771 - val_mae: 0.2635 - val_mse: 0.0771
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0851 - mae: 0.2732 - mse: 0.0851
64/87 [=====================>........] - ETA: 0s - loss: 0.0834 - mae: 0.2687 - mse: 0.0834
87/87 [==============================] - 0s 6ms/step - loss: 0.0770 - mae: 0.2573 - mse: 0.0770 - val_loss: 0.0632 - val_mae: 0.2470 - val_mse: 0.0632
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0920 - mae: 0.2758 - mse: 0.0920
64/87 [=====================>........] - ETA: 0s - loss: 0.0789 - mae: 0.2501 - mse: 0.0789
87/87 [==============================] - 0s 5ms/step - loss: 0.0763 - mae: 0.2453 - mse: 0.0763 - val_loss: 0.0604 - val_mae: 0.2395 - val_mse: 0.0604
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0762 - mae: 0.2219 - mse: 0.0762
64/87 [=====================>........] - ETA: 0s - loss: 0.0947 - mae: 0.2503 - mse: 0.0947
87/87 [==============================] - 0s 5ms/step - loss: 0.0839 - mae: 0.2387 - mse: 0.0839 - val_loss: 0.0632 - val_mae: 0.2429 - val_mse: 0.0632
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0744 - mae: 0.2427 - mse: 0.0744
64/87 [=====================>........] - ETA: 0s - loss: 0.0705 - mae: 0.2366 - mse: 0.0705
87/87 [==============================] - 0s 5ms/step - loss: 0.0720 - mae: 0.2419 - mse: 0.0720 - val_loss: 0.0687 - val_mae: 0.2445 - val_mse: 0.0687
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0601 - mae: 0.2264 - mse: 0.0601
64/87 [=====================>........] - ETA: 0s - loss: 0.0662 - mae: 0.2367 - mse: 0.0662
87/87 [==============================] - 1s 6ms/step - loss: 0.0635 - mae: 0.2300 - mse: 0.0635 - val_loss: 0.0700 - val_mae: 0.2417 - val_mse: 0.0700
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0883 - mae: 0.2611 - mse: 0.0883
64/87 [=====================>........] - ETA: 0s - loss: 0.0700 - mae: 0.2305 - mse: 0.0700
87/87 [==============================] - 0s 6ms/step - loss: 0.0646 - mae: 0.2225 - mse: 0.0646 - val_loss: 0.0672 - val_mae: 0.2349 - val_mse: 0.0672
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0713 - mae: 0.2430 - mse: 0.0713
64/87 [=====================>........] - ETA: 0s - loss: 0.0677 - mae: 0.2374 - mse: 0.0677
87/87 [==============================] - 0s 6ms/step - loss: 0.0598 - mae: 0.2209 - mse: 0.0598 - val_loss: 0.0596 - val_mae: 0.2220 - val_mse: 0.0596
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0457 - mae: 0.1913 - mse: 0.0457
64/87 [=====================>........] - ETA: 0s - loss: 0.0555 - mae: 0.2018 - mse: 0.0555
87/87 [==============================] - 0s 5ms/step - loss: 0.0589 - mae: 0.2072 - mse: 0.0589 - val_loss: 0.0542 - val_mae: 0.2093 - val_mse: 0.0542
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0599 - mae: 0.2104 - mse: 0.0599
64/87 [=====================>........] - ETA: 0s - loss: 0.0525 - mae: 0.2001 - mse: 0.0525
87/87 [==============================] - 0s 5ms/step - loss: 0.0521 - mae: 0.1978 - mse: 0.0521 - val_loss: 0.0545 - val_mae: 0.2024 - val_mse: 0.0545
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0588 - mae: 0.2112 - mse: 0.0588
64/87 [=====================>........] - ETA: 0s - loss: 0.0543 - mae: 0.1966 - mse: 0.0543
87/87 [==============================] - 0s 5ms/step - loss: 0.0540 - mae: 0.1976 - mse: 0.0540 - val_loss: 0.0542 - val_mae: 0.1959 - val_mse: 0.0542
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0471 - mae: 0.1815 - mse: 0.0471
64/87 [=====================>........] - ETA: 0s - loss: 0.0574 - mae: 0.1973 - mse: 0.0574
87/87 [==============================] - 0s 6ms/step - loss: 0.0540 - mae: 0.1893 - mse: 0.0540 - val_loss: 0.0588 - val_mae: 0.1966 - val_mse: 0.0588
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0497 - mae: 0.1834 - mse: 0.0497
64/87 [=====================>........] - ETA: 0s - loss: 0.0453 - mae: 0.1770 - mse: 0.0453
87/87 [==============================] - 0s 5ms/step - loss: 0.0466 - mae: 0.1785 - mse: 0.0466 - val_loss: 0.0537 - val_mae: 0.1855 - val_mse: 0.0537
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0246 - mae: 0.1268 - mse: 0.0246
64/87 [=====================>........] - ETA: 0s - loss: 0.0338 - mae: 0.1404 - mse: 0.0338
87/87 [==============================] - 0s 6ms/step - loss: 0.0409 - mae: 0.1527 - mse: 0.0409 - val_loss: 0.0483 - val_mae: 0.1740 - val_mse: 0.0483
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0577 - mae: 0.1764 - mse: 0.0577
64/87 [=====================>........] - ETA: 0s - loss: 0.0602 - mae: 0.1926 - mse: 0.0602
87/87 [==============================] - 0s 5ms/step - loss: 0.0475 - mae: 0.1609 - mse: 0.0475 - val_loss: 0.0570 - val_mae: 0.1823 - val_mse: 0.0570
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0541 - mae: 0.1792 - mse: 0.0541
64/87 [=====================>........] - ETA: 0s - loss: 0.0531 - mae: 0.1778 - mse: 0.0531
87/87 [==============================] - 1s 6ms/step - loss: 0.0486 - mae: 0.1676 - mse: 0.0486 - val_loss: 0.0483 - val_mae: 0.1693 - val_mse: 0.0483
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0462 - mae: 0.1378 - mse: 0.0462
64/87 [=====================>........] - ETA: 0s - loss: 0.0432 - mae: 0.1456 - mse: 0.0432
87/87 [==============================] - 1s 6ms/step - loss: 0.0455 - mae: 0.1517 - mse: 0.0455 - val_loss: 0.0442 - val_mae: 0.1580 - val_mse: 0.0442
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0546 - mae: 0.1625 - mse: 0.0546
64/87 [=====================>........] - ETA: 0s - loss: 0.0466 - mae: 0.1563 - mse: 0.0466
87/87 [==============================] - 0s 6ms/step - loss: 0.0370 - mae: 0.1373 - mse: 0.0370 - val_loss: 0.0452 - val_mae: 0.1566 - val_mse: 0.0452
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0487 - mae: 0.1476 - mse: 0.0487
64/87 [=====================>........] - ETA: 0s - loss: 0.0503 - mae: 0.1583 - mse: 0.0503
87/87 [==============================] - 0s 5ms/step - loss: 0.0490 - mae: 0.1555 - mse: 0.0490 - val_loss: 0.0585 - val_mae: 0.1820 - val_mse: 0.0585
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0319 - mae: 0.1282 - mse: 0.0319
64/87 [=====================>........] - ETA: 0s - loss: 0.0323 - mae: 0.1320 - mse: 0.0323
87/87 [==============================] - 0s 6ms/step - loss: 0.0322 - mae: 0.1311 - mse: 0.0322 - val_loss: 0.0577 - val_mae: 0.1829 - val_mse: 0.0577
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0676 - mae: 0.1865 - mse: 0.0676
64/87 [=====================>........] - ETA: 0s - loss: 0.0488 - mae: 0.1524 - mse: 0.0488
87/87 [==============================] - 0s 6ms/step - loss: 0.0484 - mae: 0.1495 - mse: 0.0484 - val_loss: 0.0433 - val_mae: 0.1611 - val_mse: 0.0433
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0501 - mae: 0.1493 - mse: 0.0501
64/87 [=====================>........] - ETA: 0s - loss: 0.0544 - mae: 0.1458 - mse: 0.0544
87/87 [==============================] - 0s 6ms/step - loss: 0.0479 - mae: 0.1348 - mse: 0.0479 - val_loss: 0.0545 - val_mae: 0.1828 - val_mse: 0.0545
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0392 - mae: 0.1494 - mse: 0.0392
64/87 [=====================>........] - ETA: 0s - loss: 0.0407 - mae: 0.1535 - mse: 0.0407
87/87 [==============================] - 0s 6ms/step - loss: 0.0408 - mae: 0.1491 - mse: 0.0408 - val_loss: 0.0636 - val_mae: 0.1872 - val_mse: 0.0636
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0332 - mae: 0.1335 - mse: 0.0332
64/87 [=====================>........] - ETA: 0s - loss: 0.0413 - mae: 0.1509 - mse: 0.0413
87/87 [==============================] - 0s 6ms/step - loss: 0.0389 - mae: 0.1422 - mse: 0.0389 - val_loss: 0.0505 - val_mae: 0.1606 - val_mse: 0.0505
Saving trained model...
98
Testing...
heightdiff= [0.         0.         0.         3.32545853 0.         0.        ]
average prediction= [4.7666035]
baseline= 10.1
eachuser= [0. 0. 0. 5. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.6650917053222656
85 -:- nan
60 -:- nan
['train-weight-10.py', '1']
65 1
2_155_65_10_csi_a10_14.dat
65 3
65 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
65 9
65 10
65 11
65 12
65 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
65 17
2_155_65_10_csi_a10_13.dat
65 19
65 20
2_155_65_10_csi_a10_3.dat
65 22
65 23
65 24
2_155_65_10_csi_a10_4.dat
65 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
65 29
65 30
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
50 89
50 90
50 91
50 92
50 93
50 94
50 95
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
85 131
85 132
85 133
85 134
85 135
85 136
85 137
85 138
85 139
85 140
85 141
85 142
85 143
85 144
85 145
85 146
85 147
85 148
85 149
85 150
85 151
85 152
85 153
85 154
85 155
85 156
1_180_85_10_csi_a10_28.dat
85 158
85 159
85 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
75 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
75 178
75 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
75 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
75 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
85 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60
 60 60 60 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75
 75 85]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.4451 - mae: 0.5780 - mse: 0.4451
64/87 [=====================>........] - ETA: 0s - loss: 0.3515 - mae: 0.4995 - mse: 0.3515
87/87 [==============================] - 1s 9ms/step - loss: 0.3041 - mae: 0.4566 - mse: 0.3041 - val_loss: 0.1773 - val_mae: 0.3482 - val_mse: 0.1773
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1762 - mae: 0.3741 - mse: 0.1762
64/87 [=====================>........] - ETA: 0s - loss: 0.1619 - mae: 0.3513 - mse: 0.1619
87/87 [==============================] - 0s 6ms/step - loss: 0.1450 - mae: 0.3208 - mse: 0.1450 - val_loss: 0.1092 - val_mae: 0.2936 - val_mse: 0.1092
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1609 - mae: 0.3239 - mse: 0.1609
64/87 [=====================>........] - ETA: 0s - loss: 0.1286 - mae: 0.2920 - mse: 0.1286
87/87 [==============================] - 0s 6ms/step - loss: 0.1284 - mae: 0.2958 - mse: 0.1284 - val_loss: 0.0965 - val_mae: 0.2809 - val_mse: 0.0965
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0846 - mae: 0.2598 - mse: 0.0846
64/87 [=====================>........] - ETA: 0s - loss: 0.0882 - mae: 0.2646 - mse: 0.0882
87/87 [==============================] - 0s 6ms/step - loss: 0.0963 - mae: 0.2806 - mse: 0.0963 - val_loss: 0.1104 - val_mae: 0.2722 - val_mse: 0.1104
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0877 - mae: 0.2647 - mse: 0.0877
64/87 [=====================>........] - ETA: 0s - loss: 0.0853 - mae: 0.2630 - mse: 0.0853
87/87 [==============================] - 0s 6ms/step - loss: 0.1019 - mae: 0.2865 - mse: 0.1019 - val_loss: 0.1052 - val_mae: 0.2622 - val_mse: 0.1052
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0815 - mae: 0.2592 - mse: 0.0815
64/87 [=====================>........] - ETA: 0s - loss: 0.0798 - mae: 0.2551 - mse: 0.0798
87/87 [==============================] - 0s 6ms/step - loss: 0.0824 - mae: 0.2605 - mse: 0.0824 - val_loss: 0.0718 - val_mae: 0.2465 - val_mse: 0.0718
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0631 - mae: 0.2271 - mse: 0.0631
64/87 [=====================>........] - ETA: 0s - loss: 0.0632 - mae: 0.2205 - mse: 0.0632
87/87 [==============================] - 0s 6ms/step - loss: 0.0564 - mae: 0.2077 - mse: 0.0564 - val_loss: 0.0579 - val_mae: 0.2307 - val_mse: 0.0579
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0600 - mae: 0.2105 - mse: 0.0600
64/87 [=====================>........] - ETA: 0s - loss: 0.0750 - mae: 0.2130 - mse: 0.0750
87/87 [==============================] - 0s 6ms/step - loss: 0.0671 - mae: 0.2019 - mse: 0.0671 - val_loss: 0.0537 - val_mae: 0.2172 - val_mse: 0.0537
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0622 - mae: 0.1961 - mse: 0.0622
64/87 [=====================>........] - ETA: 0s - loss: 0.0556 - mae: 0.1922 - mse: 0.0556
87/87 [==============================] - 0s 6ms/step - loss: 0.0520 - mae: 0.1879 - mse: 0.0520 - val_loss: 0.0555 - val_mae: 0.2118 - val_mse: 0.0555
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0435 - mae: 0.1565 - mse: 0.0435
64/87 [=====================>........] - ETA: 0s - loss: 0.0489 - mae: 0.1721 - mse: 0.0489
87/87 [==============================] - 0s 6ms/step - loss: 0.0553 - mae: 0.1837 - mse: 0.0553 - val_loss: 0.0643 - val_mae: 0.2090 - val_mse: 0.0643
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0739 - mae: 0.2294 - mse: 0.0739
64/87 [=====================>........] - ETA: 0s - loss: 0.0666 - mae: 0.2178 - mse: 0.0666
87/87 [==============================] - 0s 6ms/step - loss: 0.0638 - mae: 0.2123 - mse: 0.0638 - val_loss: 0.0708 - val_mae: 0.2060 - val_mse: 0.0708
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0691 - mae: 0.2098 - mse: 0.0691
64/87 [=====================>........] - ETA: 0s - loss: 0.0561 - mae: 0.1794 - mse: 0.0561
87/87 [==============================] - 1s 6ms/step - loss: 0.0542 - mae: 0.1787 - mse: 0.0542 - val_loss: 0.0559 - val_mae: 0.1958 - val_mse: 0.0559
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0436 - mae: 0.1704 - mse: 0.0436
64/87 [=====================>........] - ETA: 0s - loss: 0.0412 - mae: 0.1559 - mse: 0.0412
87/87 [==============================] - 1s 6ms/step - loss: 0.0568 - mae: 0.1800 - mse: 0.0568 - val_loss: 0.0509 - val_mae: 0.1942 - val_mse: 0.0509
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0486 - mae: 0.1712 - mse: 0.0486
64/87 [=====================>........] - ETA: 0s - loss: 0.0572 - mae: 0.1793 - mse: 0.0572
87/87 [==============================] - 0s 6ms/step - loss: 0.0541 - mae: 0.1768 - mse: 0.0541 - val_loss: 0.0503 - val_mae: 0.1887 - val_mse: 0.0503
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0629 - mae: 0.1730 - mse: 0.0629
64/87 [=====================>........] - ETA: 0s - loss: 0.0518 - mae: 0.1624 - mse: 0.0518
87/87 [==============================] - 0s 6ms/step - loss: 0.0523 - mae: 0.1693 - mse: 0.0523 - val_loss: 0.0611 - val_mae: 0.1990 - val_mse: 0.0611
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0403 - mae: 0.1597 - mse: 0.0403
64/87 [=====================>........] - ETA: 0s - loss: 0.0427 - mae: 0.1593 - mse: 0.0427
87/87 [==============================] - 0s 6ms/step - loss: 0.0528 - mae: 0.1761 - mse: 0.0528 - val_loss: 0.0626 - val_mae: 0.1997 - val_mse: 0.0626
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0573 - mae: 0.1876 - mse: 0.0573
64/87 [=====================>........] - ETA: 0s - loss: 0.0486 - mae: 0.1688 - mse: 0.0486
87/87 [==============================] - 0s 6ms/step - loss: 0.0527 - mae: 0.1772 - mse: 0.0527 - val_loss: 0.0526 - val_mae: 0.1923 - val_mse: 0.0526
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0413 - mae: 0.1436 - mse: 0.0413
64/87 [=====================>........] - ETA: 0s - loss: 0.0444 - mae: 0.1574 - mse: 0.0444
87/87 [==============================] - 0s 5ms/step - loss: 0.0469 - mae: 0.1639 - mse: 0.0469 - val_loss: 0.0475 - val_mae: 0.1867 - val_mse: 0.0475
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0463 - mae: 0.1627 - mse: 0.0463
64/87 [=====================>........] - ETA: 0s - loss: 0.0357 - mae: 0.1403 - mse: 0.0357
87/87 [==============================] - 0s 6ms/step - loss: 0.0386 - mae: 0.1463 - mse: 0.0386 - val_loss: 0.0469 - val_mae: 0.1896 - val_mse: 0.0469
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0373 - mae: 0.1433 - mse: 0.0373
64/87 [=====================>........] - ETA: 0s - loss: 0.0536 - mae: 0.1757 - mse: 0.0536
87/87 [==============================] - 1s 6ms/step - loss: 0.0515 - mae: 0.1708 - mse: 0.0515 - val_loss: 0.0498 - val_mae: 0.1922 - val_mse: 0.0498
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0462 - mae: 0.1467 - mse: 0.0462
64/87 [=====================>........] - ETA: 0s - loss: 0.0438 - mae: 0.1481 - mse: 0.0438
87/87 [==============================] - 0s 5ms/step - loss: 0.0400 - mae: 0.1413 - mse: 0.0400 - val_loss: 0.0487 - val_mae: 0.1882 - val_mse: 0.0487
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0281 - mae: 0.1243 - mse: 0.0281
64/87 [=====================>........] - ETA: 0s - loss: 0.0335 - mae: 0.1350 - mse: 0.0335
87/87 [==============================] - 0s 6ms/step - loss: 0.0408 - mae: 0.1505 - mse: 0.0408 - val_loss: 0.0442 - val_mae: 0.1794 - val_mse: 0.0442
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0225 - mae: 0.1118 - mse: 0.0225
64/87 [=====================>........] - ETA: 0s - loss: 0.0288 - mae: 0.1288 - mse: 0.0288
87/87 [==============================] - 0s 5ms/step - loss: 0.0364 - mae: 0.1375 - mse: 0.0364 - val_loss: 0.0456 - val_mae: 0.1773 - val_mse: 0.0456
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0474 - mae: 0.1632 - mse: 0.0474
64/87 [=====================>........] - ETA: 0s - loss: 0.0436 - mae: 0.1561 - mse: 0.0436
87/87 [==============================] - 1s 6ms/step - loss: 0.0407 - mae: 0.1503 - mse: 0.0407 - val_loss: 0.0549 - val_mae: 0.1831 - val_mse: 0.0549
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0491 - mae: 0.1778 - mse: 0.0491
64/87 [=====================>........] - ETA: 0s - loss: 0.0401 - mae: 0.1568 - mse: 0.0401
87/87 [==============================] - 1s 6ms/step - loss: 0.0369 - mae: 0.1470 - mse: 0.0369 - val_loss: 0.0549 - val_mae: 0.1822 - val_mse: 0.0549
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0336 - mae: 0.1384 - mse: 0.0336
64/87 [=====================>........] - ETA: 0s - loss: 0.0395 - mae: 0.1510 - mse: 0.0395
87/87 [==============================] - 0s 5ms/step - loss: 0.0388 - mae: 0.1465 - mse: 0.0388 - val_loss: 0.0441 - val_mae: 0.1745 - val_mse: 0.0441
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0474 - mae: 0.1581 - mse: 0.0474
64/87 [=====================>........] - ETA: 0s - loss: 0.0378 - mae: 0.1413 - mse: 0.0378
87/87 [==============================] - 1s 6ms/step - loss: 0.0368 - mae: 0.1401 - mse: 0.0368 - val_loss: 0.0516 - val_mae: 0.1851 - val_mse: 0.0516
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0293 - mae: 0.1164 - mse: 0.0293
64/87 [=====================>........] - ETA: 0s - loss: 0.0408 - mae: 0.1449 - mse: 0.0408
87/87 [==============================] - 0s 6ms/step - loss: 0.0421 - mae: 0.1485 - mse: 0.0421 - val_loss: 0.0537 - val_mae: 0.1873 - val_mse: 0.0537
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0350 - mae: 0.1470 - mse: 0.0350
64/87 [=====================>........] - ETA: 0s - loss: 0.0271 - mae: 0.1195 - mse: 0.0271
87/87 [==============================] - 0s 5ms/step - loss: 0.0258 - mae: 0.1179 - mse: 0.0258 - val_loss: 0.0509 - val_mae: 0.1808 - val_mse: 0.0509
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0231 - mae: 0.1070 - mse: 0.0231
64/87 [=====================>........] - ETA: 0s - loss: 0.0306 - mae: 0.1290 - mse: 0.0306
87/87 [==============================] - 0s 6ms/step - loss: 0.0329 - mae: 0.1332 - mse: 0.0329 - val_loss: 0.0462 - val_mae: 0.1726 - val_mse: 0.0462
Saving trained model...
98
Testing...
heightdiff= [0.         0.         0.         8.15423584 0.         0.        ]
average prediction= [4.7703967]
baseline= 10.9
eachuser= [0. 0. 0. 7. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.1648908342633928
85 -:- nan
60 -:- nan
['train-weight-10.py', '1']
65 1
2_155_65_10_csi_a10_14.dat
65 3
65 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
65 9
65 10
65 11
65 12
65 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
65 17
2_155_65_10_csi_a10_13.dat
65 19
65 20
2_155_65_10_csi_a10_3.dat
65 22
65 23
65 24
2_155_65_10_csi_a10_4.dat
65 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
65 29
65 30
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
50 89
50 90
50 91
50 92
50 93
50 94
50 95
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
85 131
85 132
85 133
85 134
85 135
85 136
85 137
85 138
85 139
85 140
85 141
85 142
85 143
85 144
85 145
85 146
85 147
85 148
85 149
85 150
85 151
85 152
85 153
85 154
85 155
85 156
1_180_85_10_csi_a10_28.dat
85 158
85 159
85 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
75 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
75 178
75 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
75 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
75 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
85 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60
 60 60 60 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75
 75 85]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2963 - mae: 0.4651 - mse: 0.2963
64/87 [=====================>........] - ETA: 0s - loss: 0.2910 - mae: 0.4539 - mse: 0.2910
87/87 [==============================] - 1s 9ms/step - loss: 0.2602 - mae: 0.4223 - mse: 0.2602 - val_loss: 0.1712 - val_mae: 0.3534 - val_mse: 0.1712
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1175 - mae: 0.2594 - mse: 0.1175
64/87 [=====================>........] - ETA: 0s - loss: 0.1399 - mae: 0.2990 - mse: 0.1399
87/87 [==============================] - 0s 6ms/step - loss: 0.1457 - mae: 0.3125 - mse: 0.1457 - val_loss: 0.1404 - val_mae: 0.3480 - val_mse: 0.1404
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1585 - mae: 0.3456 - mse: 0.1585
64/87 [=====================>........] - ETA: 0s - loss: 0.1428 - mae: 0.3252 - mse: 0.1428
87/87 [==============================] - 0s 6ms/step - loss: 0.1454 - mae: 0.3333 - mse: 0.1454 - val_loss: 0.0943 - val_mae: 0.2846 - val_mse: 0.0943
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0932 - mae: 0.2743 - mse: 0.0932
64/87 [=====================>........] - ETA: 0s - loss: 0.0775 - mae: 0.2338 - mse: 0.0775
87/87 [==============================] - 0s 6ms/step - loss: 0.0854 - mae: 0.2514 - mse: 0.0854 - val_loss: 0.0746 - val_mae: 0.2437 - val_mse: 0.0746
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0879 - mae: 0.2676 - mse: 0.0879
64/87 [=====================>........] - ETA: 0s - loss: 0.0763 - mae: 0.2434 - mse: 0.0763
87/87 [==============================] - 0s 6ms/step - loss: 0.0796 - mae: 0.2520 - mse: 0.0796 - val_loss: 0.0589 - val_mae: 0.2164 - val_mse: 0.0589
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0950 - mae: 0.2532 - mse: 0.0950
64/87 [=====================>........] - ETA: 0s - loss: 0.0772 - mae: 0.2340 - mse: 0.0772
87/87 [==============================] - 1s 6ms/step - loss: 0.0763 - mae: 0.2298 - mse: 0.0763 - val_loss: 0.0509 - val_mae: 0.1976 - val_mse: 0.0509
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0923 - mae: 0.2589 - mse: 0.0923
64/87 [=====================>........] - ETA: 0s - loss: 0.0735 - mae: 0.2257 - mse: 0.0735
87/87 [==============================] - 0s 6ms/step - loss: 0.0656 - mae: 0.2075 - mse: 0.0656 - val_loss: 0.0483 - val_mae: 0.1844 - val_mse: 0.0483
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0626 - mae: 0.2103 - mse: 0.0626
64/87 [=====================>........] - ETA: 0s - loss: 0.0658 - mae: 0.2164 - mse: 0.0658
87/87 [==============================] - 0s 5ms/step - loss: 0.0567 - mae: 0.1971 - mse: 0.0567 - val_loss: 0.0443 - val_mae: 0.1749 - val_mse: 0.0443
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0549 - mae: 0.1881 - mse: 0.0549
64/87 [=====================>........] - ETA: 0s - loss: 0.0595 - mae: 0.1952 - mse: 0.0595
87/87 [==============================] - 0s 6ms/step - loss: 0.0566 - mae: 0.1881 - mse: 0.0566 - val_loss: 0.0424 - val_mae: 0.1683 - val_mse: 0.0424
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0687 - mae: 0.1798 - mse: 0.0687
64/87 [=====================>........] - ETA: 0s - loss: 0.0649 - mae: 0.1914 - mse: 0.0649
87/87 [==============================] - 0s 6ms/step - loss: 0.0617 - mae: 0.1861 - mse: 0.0617 - val_loss: 0.0419 - val_mae: 0.1614 - val_mse: 0.0419
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0555 - mae: 0.1859 - mse: 0.0555
64/87 [=====================>........] - ETA: 0s - loss: 0.0540 - mae: 0.1829 - mse: 0.0540
87/87 [==============================] - 0s 5ms/step - loss: 0.0479 - mae: 0.1679 - mse: 0.0479 - val_loss: 0.0405 - val_mae: 0.1554 - val_mse: 0.0405
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0555 - mae: 0.1881 - mse: 0.0555
64/87 [=====================>........] - ETA: 0s - loss: 0.0510 - mae: 0.1757 - mse: 0.0510
87/87 [==============================] - 0s 6ms/step - loss: 0.0460 - mae: 0.1641 - mse: 0.0460 - val_loss: 0.0389 - val_mae: 0.1518 - val_mse: 0.0389
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0358 - mae: 0.1488 - mse: 0.0358
64/87 [=====================>........] - ETA: 0s - loss: 0.0420 - mae: 0.1562 - mse: 0.0420
87/87 [==============================] - 0s 6ms/step - loss: 0.0497 - mae: 0.1725 - mse: 0.0497 - val_loss: 0.0377 - val_mae: 0.1495 - val_mse: 0.0377
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0401 - mae: 0.1468 - mse: 0.0401
64/87 [=====================>........] - ETA: 0s - loss: 0.0511 - mae: 0.1730 - mse: 0.0511
87/87 [==============================] - 0s 6ms/step - loss: 0.0446 - mae: 0.1617 - mse: 0.0446 - val_loss: 0.0402 - val_mae: 0.1498 - val_mse: 0.0402
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0660 - mae: 0.1810 - mse: 0.0660
64/87 [=====================>........] - ETA: 0s - loss: 0.0586 - mae: 0.1794 - mse: 0.0586
87/87 [==============================] - 0s 6ms/step - loss: 0.0494 - mae: 0.1653 - mse: 0.0494 - val_loss: 0.0368 - val_mae: 0.1441 - val_mse: 0.0368
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0720 - mae: 0.2139 - mse: 0.0720
64/87 [=====================>........] - ETA: 0s - loss: 0.0593 - mae: 0.1857 - mse: 0.0593
87/87 [==============================] - 0s 5ms/step - loss: 0.0513 - mae: 0.1717 - mse: 0.0513 - val_loss: 0.0373 - val_mae: 0.1564 - val_mse: 0.0373
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0630 - mae: 0.1845 - mse: 0.0630
64/87 [=====================>........] - ETA: 0s - loss: 0.0455 - mae: 0.1582 - mse: 0.0455
87/87 [==============================] - 0s 6ms/step - loss: 0.0510 - mae: 0.1703 - mse: 0.0510 - val_loss: 0.0353 - val_mae: 0.1495 - val_mse: 0.0353
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0541 - mae: 0.1809 - mse: 0.0541
64/87 [=====================>........] - ETA: 0s - loss: 0.0460 - mae: 0.1570 - mse: 0.0460
87/87 [==============================] - 0s 6ms/step - loss: 0.0473 - mae: 0.1557 - mse: 0.0473 - val_loss: 0.0409 - val_mae: 0.1357 - val_mse: 0.0409
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0481 - mae: 0.1620 - mse: 0.0481
64/87 [=====================>........] - ETA: 0s - loss: 0.0411 - mae: 0.1487 - mse: 0.0411
87/87 [==============================] - 0s 6ms/step - loss: 0.0477 - mae: 0.1512 - mse: 0.0477 - val_loss: 0.0353 - val_mae: 0.1323 - val_mse: 0.0353
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0292 - mae: 0.1168 - mse: 0.0292
64/87 [=====================>........] - ETA: 0s - loss: 0.0289 - mae: 0.1228 - mse: 0.0289
87/87 [==============================] - 0s 6ms/step - loss: 0.0311 - mae: 0.1328 - mse: 0.0311 - val_loss: 0.0356 - val_mae: 0.1423 - val_mse: 0.0356
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0325 - mae: 0.1447 - mse: 0.0325
64/87 [=====================>........] - ETA: 0s - loss: 0.0309 - mae: 0.1409 - mse: 0.0309
87/87 [==============================] - 0s 5ms/step - loss: 0.0377 - mae: 0.1515 - mse: 0.0377 - val_loss: 0.0350 - val_mae: 0.1393 - val_mse: 0.0350
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0442 - mae: 0.1495 - mse: 0.0442
64/87 [=====================>........] - ETA: 0s - loss: 0.0434 - mae: 0.1484 - mse: 0.0434
87/87 [==============================] - 0s 5ms/step - loss: 0.0489 - mae: 0.1573 - mse: 0.0489 - val_loss: 0.0330 - val_mae: 0.1301 - val_mse: 0.0330
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0313 - mae: 0.1335 - mse: 0.0313
64/87 [=====================>........] - ETA: 0s - loss: 0.0418 - mae: 0.1534 - mse: 0.0418
87/87 [==============================] - 0s 6ms/step - loss: 0.0432 - mae: 0.1515 - mse: 0.0432 - val_loss: 0.0329 - val_mae: 0.1308 - val_mse: 0.0329
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0427 - mae: 0.1583 - mse: 0.0427
64/87 [=====================>........] - ETA: 0s - loss: 0.0417 - mae: 0.1517 - mse: 0.0417
87/87 [==============================] - 0s 5ms/step - loss: 0.0385 - mae: 0.1419 - mse: 0.0385 - val_loss: 0.0337 - val_mae: 0.1333 - val_mse: 0.0337
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0454 - mae: 0.1481 - mse: 0.0454
64/87 [=====================>........] - ETA: 0s - loss: 0.0415 - mae: 0.1420 - mse: 0.0415
87/87 [==============================] - 0s 5ms/step - loss: 0.0400 - mae: 0.1444 - mse: 0.0400 - val_loss: 0.0353 - val_mae: 0.1404 - val_mse: 0.0353
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0465 - mae: 0.1532 - mse: 0.0465
64/87 [=====================>........] - ETA: 0s - loss: 0.0449 - mae: 0.1545 - mse: 0.0449
87/87 [==============================] - 0s 5ms/step - loss: 0.0425 - mae: 0.1527 - mse: 0.0425 - val_loss: 0.0324 - val_mae: 0.1307 - val_mse: 0.0324
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0281 - mae: 0.1378 - mse: 0.0281
64/87 [=====================>........] - ETA: 0s - loss: 0.0342 - mae: 0.1416 - mse: 0.0342
87/87 [==============================] - 0s 5ms/step - loss: 0.0371 - mae: 0.1420 - mse: 0.0371 - val_loss: 0.0331 - val_mae: 0.1238 - val_mse: 0.0331
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0550 - mae: 0.1615 - mse: 0.0550
64/87 [=====================>........] - ETA: 0s - loss: 0.0386 - mae: 0.1351 - mse: 0.0386
87/87 [==============================] - 0s 5ms/step - loss: 0.0393 - mae: 0.1391 - mse: 0.0393 - val_loss: 0.0323 - val_mae: 0.1349 - val_mse: 0.0323
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0342 - mae: 0.1347 - mse: 0.0342
64/87 [=====================>........] - ETA: 0s - loss: 0.0358 - mae: 0.1414 - mse: 0.0358
87/87 [==============================] - 0s 6ms/step - loss: 0.0319 - mae: 0.1319 - mse: 0.0319 - val_loss: 0.0326 - val_mae: 0.1383 - val_mse: 0.0326
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0301 - mae: 0.1308 - mse: 0.0301
64/87 [=====================>........] - ETA: 0s - loss: 0.0272 - mae: 0.1206 - mse: 0.0272
87/87 [==============================] - 0s 6ms/step - loss: 0.0382 - mae: 0.1389 - mse: 0.0382 - val_loss: 0.0314 - val_mae: 0.1233 - val_mse: 0.0314
Saving trained model...
98
Testing...
heightdiff= [0.         0.         0.         4.19604492 0.         0.        ]
average prediction= [5.3981767]
baseline= 12.1
eachuser= [0. 0. 0. 7. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.5994349888392857
85 -:- nan
60 -:- nan
['train-weight-10.py', '1']
65 1
2_155_65_10_csi_a10_14.dat
65 3
65 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
65 9
65 10
65 11
65 12
65 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
65 17
2_155_65_10_csi_a10_13.dat
65 19
65 20
2_155_65_10_csi_a10_3.dat
65 22
65 23
65 24
2_155_65_10_csi_a10_4.dat
65 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
65 29
65 30
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
50 89
50 90
50 91
50 92
50 93
50 94
50 95
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
85 131
85 132
85 133
85 134
85 135
85 136
85 137
85 138
85 139
85 140
85 141
85 142
85 143
85 144
85 145
85 146
85 147
85 148
85 149
85 150
85 151
85 152
85 153
85 154
85 155
85 156
1_180_85_10_csi_a10_28.dat
85 158
85 159
85 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
75 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
75 178
75 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
75 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
75 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
85 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60
 60 60 60 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75
 75 85]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.3961 - mae: 0.5243 - mse: 0.3961
64/87 [=====================>........] - ETA: 0s - loss: 0.3838 - mae: 0.5222 - mse: 0.3838
87/87 [==============================] - 1s 10ms/step - loss: 0.3477 - mae: 0.4871 - mse: 0.3477 - val_loss: 0.1253 - val_mae: 0.3199 - val_mse: 0.1253
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.2035 - mae: 0.3484 - mse: 0.2035
64/87 [=====================>........] - ETA: 0s - loss: 0.1987 - mae: 0.3602 - mse: 0.1987
87/87 [==============================] - 1s 6ms/step - loss: 0.2110 - mae: 0.3792 - mse: 0.2110 - val_loss: 0.1023 - val_mae: 0.2478 - val_mse: 0.1023
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1566 - mae: 0.3219 - mse: 0.1566
64/87 [=====================>........] - ETA: 0s - loss: 0.1711 - mae: 0.3490 - mse: 0.1711
87/87 [==============================] - 0s 6ms/step - loss: 0.1710 - mae: 0.3477 - mse: 0.1710 - val_loss: 0.0694 - val_mae: 0.2135 - val_mse: 0.0694
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1586 - mae: 0.3166 - mse: 0.1586
64/87 [=====================>........] - ETA: 0s - loss: 0.1447 - mae: 0.3138 - mse: 0.1447
87/87 [==============================] - 0s 6ms/step - loss: 0.1340 - mae: 0.2990 - mse: 0.1340 - val_loss: 0.0545 - val_mae: 0.1902 - val_mse: 0.0545
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1259 - mae: 0.2892 - mse: 0.1259
64/87 [=====================>........] - ETA: 0s - loss: 0.1049 - mae: 0.2597 - mse: 0.1049
87/87 [==============================] - 0s 6ms/step - loss: 0.0953 - mae: 0.2500 - mse: 0.0953 - val_loss: 0.0379 - val_mae: 0.1638 - val_mse: 0.0379
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0835 - mae: 0.2447 - mse: 0.0835
64/87 [=====================>........] - ETA: 0s - loss: 0.0721 - mae: 0.2198 - mse: 0.0721
87/87 [==============================] - 0s 6ms/step - loss: 0.0805 - mae: 0.2383 - mse: 0.0805 - val_loss: 0.0313 - val_mae: 0.1687 - val_mse: 0.0313
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0705 - mae: 0.2313 - mse: 0.0705
64/87 [=====================>........] - ETA: 0s - loss: 0.0620 - mae: 0.2088 - mse: 0.0620
87/87 [==============================] - 0s 6ms/step - loss: 0.0563 - mae: 0.1982 - mse: 0.0563 - val_loss: 0.0387 - val_mae: 0.1770 - val_mse: 0.0387
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0629 - mae: 0.1995 - mse: 0.0629
64/87 [=====================>........] - ETA: 0s - loss: 0.0585 - mae: 0.1992 - mse: 0.0585
87/87 [==============================] - 0s 6ms/step - loss: 0.0578 - mae: 0.1972 - mse: 0.0578 - val_loss: 0.0366 - val_mae: 0.1617 - val_mse: 0.0366
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0703 - mae: 0.2124 - mse: 0.0703
64/87 [=====================>........] - ETA: 0s - loss: 0.0550 - mae: 0.1817 - mse: 0.0550
87/87 [==============================] - 0s 6ms/step - loss: 0.0649 - mae: 0.1970 - mse: 0.0649 - val_loss: 0.0354 - val_mae: 0.1554 - val_mse: 0.0354
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0543 - mae: 0.1854 - mse: 0.0543
64/87 [=====================>........] - ETA: 0s - loss: 0.0476 - mae: 0.1771 - mse: 0.0476
87/87 [==============================] - 0s 6ms/step - loss: 0.0545 - mae: 0.1899 - mse: 0.0545 - val_loss: 0.0334 - val_mae: 0.1487 - val_mse: 0.0334
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0300 - mae: 0.1439 - mse: 0.0300
64/87 [=====================>........] - ETA: 0s - loss: 0.0451 - mae: 0.1730 - mse: 0.0451
87/87 [==============================] - 0s 5ms/step - loss: 0.0505 - mae: 0.1760 - mse: 0.0505 - val_loss: 0.0320 - val_mae: 0.1464 - val_mse: 0.0320
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0562 - mae: 0.1815 - mse: 0.0562
64/87 [=====================>........] - ETA: 0s - loss: 0.0589 - mae: 0.1824 - mse: 0.0589
87/87 [==============================] - 0s 5ms/step - loss: 0.0541 - mae: 0.1797 - mse: 0.0541 - val_loss: 0.0246 - val_mae: 0.1327 - val_mse: 0.0246
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0428 - mae: 0.1568 - mse: 0.0428
64/87 [=====================>........] - ETA: 0s - loss: 0.0458 - mae: 0.1575 - mse: 0.0458
87/87 [==============================] - 0s 5ms/step - loss: 0.0471 - mae: 0.1653 - mse: 0.0471 - val_loss: 0.0204 - val_mae: 0.1243 - val_mse: 0.0204
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0351 - mae: 0.1460 - mse: 0.0351
64/87 [=====================>........] - ETA: 0s - loss: 0.0523 - mae: 0.1829 - mse: 0.0523
87/87 [==============================] - 0s 5ms/step - loss: 0.0523 - mae: 0.1786 - mse: 0.0523 - val_loss: 0.0197 - val_mae: 0.1180 - val_mse: 0.0197
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0591 - mae: 0.1948 - mse: 0.0591
64/87 [=====================>........] - ETA: 0s - loss: 0.0414 - mae: 0.1585 - mse: 0.0414
87/87 [==============================] - 0s 5ms/step - loss: 0.0396 - mae: 0.1543 - mse: 0.0396 - val_loss: 0.0198 - val_mae: 0.1146 - val_mse: 0.0198
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0594 - mae: 0.1752 - mse: 0.0594
64/87 [=====================>........] - ETA: 0s - loss: 0.0480 - mae: 0.1639 - mse: 0.0480
87/87 [==============================] - 0s 5ms/step - loss: 0.0435 - mae: 0.1566 - mse: 0.0435 - val_loss: 0.0295 - val_mae: 0.1317 - val_mse: 0.0295
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0392 - mae: 0.1599 - mse: 0.0392
64/87 [=====================>........] - ETA: 0s - loss: 0.0327 - mae: 0.1379 - mse: 0.0327
87/87 [==============================] - 0s 5ms/step - loss: 0.0364 - mae: 0.1441 - mse: 0.0364 - val_loss: 0.0267 - val_mae: 0.1196 - val_mse: 0.0267
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0544 - mae: 0.1815 - mse: 0.0544
64/87 [=====================>........] - ETA: 0s - loss: 0.0451 - mae: 0.1635 - mse: 0.0451
87/87 [==============================] - 0s 5ms/step - loss: 0.0398 - mae: 0.1563 - mse: 0.0398 - val_loss: 0.0254 - val_mae: 0.1192 - val_mse: 0.0254
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0341 - mae: 0.1435 - mse: 0.0341
64/87 [=====================>........] - ETA: 0s - loss: 0.0371 - mae: 0.1527 - mse: 0.0371
87/87 [==============================] - 0s 6ms/step - loss: 0.0372 - mae: 0.1492 - mse: 0.0372 - val_loss: 0.0343 - val_mae: 0.1284 - val_mse: 0.0343
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0509 - mae: 0.1659 - mse: 0.0509
64/87 [=====================>........] - ETA: 0s - loss: 0.0395 - mae: 0.1499 - mse: 0.0395
87/87 [==============================] - 0s 5ms/step - loss: 0.0399 - mae: 0.1471 - mse: 0.0399 - val_loss: 0.0281 - val_mae: 0.1183 - val_mse: 0.0281
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0442 - mae: 0.1556 - mse: 0.0442
64/87 [=====================>........] - ETA: 0s - loss: 0.0490 - mae: 0.1661 - mse: 0.0490
87/87 [==============================] - 0s 5ms/step - loss: 0.0418 - mae: 0.1512 - mse: 0.0418 - val_loss: 0.0201 - val_mae: 0.1079 - val_mse: 0.0201
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0389 - mae: 0.1496 - mse: 0.0389
64/87 [=====================>........] - ETA: 0s - loss: 0.0385 - mae: 0.1497 - mse: 0.0385
87/87 [==============================] - 0s 5ms/step - loss: 0.0365 - mae: 0.1453 - mse: 0.0365 - val_loss: 0.0196 - val_mae: 0.1046 - val_mse: 0.0196
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0406 - mae: 0.1494 - mse: 0.0406
64/87 [=====================>........] - ETA: 0s - loss: 0.0331 - mae: 0.1302 - mse: 0.0331
87/87 [==============================] - 0s 6ms/step - loss: 0.0313 - mae: 0.1293 - mse: 0.0313 - val_loss: 0.0308 - val_mae: 0.1217 - val_mse: 0.0308
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0468 - mae: 0.1672 - mse: 0.0468
64/87 [=====================>........] - ETA: 0s - loss: 0.0386 - mae: 0.1466 - mse: 0.0386
87/87 [==============================] - 1s 6ms/step - loss: 0.0362 - mae: 0.1422 - mse: 0.0362 - val_loss: 0.0257 - val_mae: 0.1105 - val_mse: 0.0257
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0276 - mae: 0.1200 - mse: 0.0276
64/87 [=====================>........] - ETA: 0s - loss: 0.0315 - mae: 0.1296 - mse: 0.0315
87/87 [==============================] - 0s 5ms/step - loss: 0.0357 - mae: 0.1376 - mse: 0.0357 - val_loss: 0.0216 - val_mae: 0.1141 - val_mse: 0.0216
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0419 - mae: 0.1560 - mse: 0.0419
64/87 [=====================>........] - ETA: 0s - loss: 0.0373 - mae: 0.1415 - mse: 0.0373
87/87 [==============================] - 0s 5ms/step - loss: 0.0348 - mae: 0.1360 - mse: 0.0348 - val_loss: 0.0322 - val_mae: 0.1199 - val_mse: 0.0322
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0318 - mae: 0.1348 - mse: 0.0318
64/87 [=====================>........] - ETA: 0s - loss: 0.0329 - mae: 0.1336 - mse: 0.0329
87/87 [==============================] - 0s 6ms/step - loss: 0.0337 - mae: 0.1325 - mse: 0.0337 - val_loss: 0.0331 - val_mae: 0.1208 - val_mse: 0.0331
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0460 - mae: 0.1469 - mse: 0.0460
64/87 [=====================>........] - ETA: 0s - loss: 0.0339 - mae: 0.1276 - mse: 0.0339
87/87 [==============================] - 0s 5ms/step - loss: 0.0319 - mae: 0.1267 - mse: 0.0319 - val_loss: 0.0217 - val_mae: 0.1134 - val_mse: 0.0217
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0192 - mae: 0.0985 - mse: 0.0192
64/87 [=====================>........] - ETA: 0s - loss: 0.0293 - mae: 0.1256 - mse: 0.0293
87/87 [==============================] - 0s 5ms/step - loss: 0.0328 - mae: 0.1315 - mse: 0.0328 - val_loss: 0.0244 - val_mae: 0.1099 - val_mse: 0.0244
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0300 - mae: 0.1376 - mse: 0.0300
64/87 [=====================>........] - ETA: 0s - loss: 0.0275 - mae: 0.1276 - mse: 0.0275
87/87 [==============================] - 0s 6ms/step - loss: 0.0291 - mae: 0.1306 - mse: 0.0291 - val_loss: 0.0276 - val_mae: 0.1142 - val_mse: 0.0276
Saving trained model...
98
Testing...
heightdiff= [0.         0.         0.         3.32205963 0.         0.        ]
average prediction= [5.40339]
baseline= 11.5
eachuser= [0. 0. 0. 6. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.5536766052246094
85 -:- nan
60 -:- nan
['train-weight-10.py', '1']
65 1
2_155_65_10_csi_a10_14.dat
65 3
65 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
65 9
65 10
65 11
65 12
65 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
65 17
2_155_65_10_csi_a10_13.dat
65 19
65 20
2_155_65_10_csi_a10_3.dat
65 22
65 23
65 24
2_155_65_10_csi_a10_4.dat
65 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
65 29
65 30
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
50 89
50 90
50 91
50 92
50 93
50 94
50 95
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
85 131
85 132
85 133
85 134
85 135
85 136
85 137
85 138
85 139
85 140
85 141
85 142
85 143
85 144
85 145
85 146
85 147
85 148
85 149
85 150
85 151
85 152
85 153
85 154
85 155
85 156
1_180_85_10_csi_a10_28.dat
85 158
85 159
85 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
75 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
75 178
75 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
75 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
75 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
85 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60
 60 60 60 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75
 75 85]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.4407 - mae: 0.5420 - mse: 0.4407
64/87 [=====================>........] - ETA: 0s - loss: 0.3168 - mae: 0.4507 - mse: 0.3168
87/87 [==============================] - 1s 9ms/step - loss: 0.2868 - mae: 0.4262 - mse: 0.2868 - val_loss: 0.1130 - val_mae: 0.2773 - val_mse: 0.1130
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1680 - mae: 0.3490 - mse: 0.1680
64/87 [=====================>........] - ETA: 0s - loss: 0.1632 - mae: 0.3494 - mse: 0.1632
87/87 [==============================] - 0s 6ms/step - loss: 0.1843 - mae: 0.3708 - mse: 0.1843 - val_loss: 0.0912 - val_mae: 0.2500 - val_mse: 0.0912
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1747 - mae: 0.3480 - mse: 0.1747
64/87 [=====================>........] - ETA: 0s - loss: 0.1427 - mae: 0.3240 - mse: 0.1427
87/87 [==============================] - 1s 6ms/step - loss: 0.1345 - mae: 0.3097 - mse: 0.1345 - val_loss: 0.0952 - val_mae: 0.2701 - val_mse: 0.0952
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1082 - mae: 0.2911 - mse: 0.1082
64/87 [=====================>........] - ETA: 0s - loss: 0.1007 - mae: 0.2749 - mse: 0.1007
87/87 [==============================] - 0s 5ms/step - loss: 0.0998 - mae: 0.2770 - mse: 0.0998 - val_loss: 0.1203 - val_mae: 0.2818 - val_mse: 0.1203
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0883 - mae: 0.2554 - mse: 0.0883
64/87 [=====================>........] - ETA: 0s - loss: 0.0994 - mae: 0.2760 - mse: 0.0994
87/87 [==============================] - 0s 6ms/step - loss: 0.0962 - mae: 0.2729 - mse: 0.0962 - val_loss: 0.1090 - val_mae: 0.2744 - val_mse: 0.1090
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0680 - mae: 0.2306 - mse: 0.0680
64/87 [=====================>........] - ETA: 0s - loss: 0.0895 - mae: 0.2693 - mse: 0.0895
87/87 [==============================] - 0s 6ms/step - loss: 0.0827 - mae: 0.2605 - mse: 0.0827 - val_loss: 0.0896 - val_mae: 0.2495 - val_mse: 0.0896
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0910 - mae: 0.2522 - mse: 0.0910
64/87 [=====================>........] - ETA: 0s - loss: 0.0851 - mae: 0.2511 - mse: 0.0851
87/87 [==============================] - 0s 6ms/step - loss: 0.0844 - mae: 0.2538 - mse: 0.0844 - val_loss: 0.0789 - val_mae: 0.2380 - val_mse: 0.0789
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0983 - mae: 0.2851 - mse: 0.0983
64/87 [=====================>........] - ETA: 0s - loss: 0.0806 - mae: 0.2544 - mse: 0.0806
87/87 [==============================] - 1s 6ms/step - loss: 0.0788 - mae: 0.2490 - mse: 0.0788 - val_loss: 0.0834 - val_mae: 0.2403 - val_mse: 0.0834
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0647 - mae: 0.2154 - mse: 0.0647
64/87 [=====================>........] - ETA: 0s - loss: 0.0716 - mae: 0.2236 - mse: 0.0716
87/87 [==============================] - 0s 6ms/step - loss: 0.0717 - mae: 0.2269 - mse: 0.0717 - val_loss: 0.0880 - val_mae: 0.2448 - val_mse: 0.0880
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0481 - mae: 0.2006 - mse: 0.0481
64/87 [=====================>........] - ETA: 0s - loss: 0.0595 - mae: 0.2174 - mse: 0.0595
87/87 [==============================] - 0s 5ms/step - loss: 0.0560 - mae: 0.2106 - mse: 0.0560 - val_loss: 0.0905 - val_mae: 0.2481 - val_mse: 0.0905
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0558 - mae: 0.1945 - mse: 0.0558
64/87 [=====================>........] - ETA: 0s - loss: 0.0562 - mae: 0.2031 - mse: 0.0562
87/87 [==============================] - 0s 6ms/step - loss: 0.0586 - mae: 0.2073 - mse: 0.0586 - val_loss: 0.0842 - val_mae: 0.2415 - val_mse: 0.0842
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0502 - mae: 0.1915 - mse: 0.0502
64/87 [=====================>........] - ETA: 0s - loss: 0.0530 - mae: 0.1937 - mse: 0.0530
87/87 [==============================] - 0s 5ms/step - loss: 0.0508 - mae: 0.1899 - mse: 0.0508 - val_loss: 0.0797 - val_mae: 0.2303 - val_mse: 0.0797
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0532 - mae: 0.1956 - mse: 0.0532
64/87 [=====================>........] - ETA: 0s - loss: 0.0564 - mae: 0.1940 - mse: 0.0564
87/87 [==============================] - 0s 6ms/step - loss: 0.0488 - mae: 0.1799 - mse: 0.0488 - val_loss: 0.0853 - val_mae: 0.2270 - val_mse: 0.0853
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0493 - mae: 0.1611 - mse: 0.0493
64/87 [=====================>........] - ETA: 0s - loss: 0.0462 - mae: 0.1619 - mse: 0.0462
87/87 [==============================] - 0s 5ms/step - loss: 0.0487 - mae: 0.1645 - mse: 0.0487 - val_loss: 0.1056 - val_mae: 0.2445 - val_mse: 0.1056
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0278 - mae: 0.1291 - mse: 0.0278
64/87 [=====================>........] - ETA: 0s - loss: 0.0379 - mae: 0.1450 - mse: 0.0379
87/87 [==============================] - 0s 6ms/step - loss: 0.0442 - mae: 0.1637 - mse: 0.0442 - val_loss: 0.1039 - val_mae: 0.2398 - val_mse: 0.1039
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0341 - mae: 0.1413 - mse: 0.0341
64/87 [=====================>........] - ETA: 0s - loss: 0.0447 - mae: 0.1588 - mse: 0.0447
87/87 [==============================] - 0s 6ms/step - loss: 0.0463 - mae: 0.1635 - mse: 0.0463 - val_loss: 0.0912 - val_mae: 0.2204 - val_mse: 0.0912
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0452 - mae: 0.1518 - mse: 0.0452
64/87 [=====================>........] - ETA: 0s - loss: 0.0378 - mae: 0.1397 - mse: 0.0378
87/87 [==============================] - 0s 5ms/step - loss: 0.0440 - mae: 0.1547 - mse: 0.0440 - val_loss: 0.1011 - val_mae: 0.2237 - val_mse: 0.1011
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0436 - mae: 0.1604 - mse: 0.0436
64/87 [=====================>........] - ETA: 0s - loss: 0.0500 - mae: 0.1703 - mse: 0.0500
87/87 [==============================] - 1s 6ms/step - loss: 0.0495 - mae: 0.1654 - mse: 0.0495 - val_loss: 0.1154 - val_mae: 0.2301 - val_mse: 0.1154
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0241 - mae: 0.1080 - mse: 0.0241
64/87 [=====================>........] - ETA: 0s - loss: 0.0235 - mae: 0.1118 - mse: 0.0235
87/87 [==============================] - 0s 6ms/step - loss: 0.0300 - mae: 0.1277 - mse: 0.0300 - val_loss: 0.1086 - val_mae: 0.2196 - val_mse: 0.1086
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0299 - mae: 0.1086 - mse: 0.0299
64/87 [=====================>........] - ETA: 0s - loss: 0.0306 - mae: 0.1258 - mse: 0.0306
87/87 [==============================] - 0s 6ms/step - loss: 0.0389 - mae: 0.1443 - mse: 0.0389 - val_loss: 0.0851 - val_mae: 0.1882 - val_mse: 0.0851
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0320 - mae: 0.1289 - mse: 0.0320
64/87 [=====================>........] - ETA: 0s - loss: 0.0459 - mae: 0.1612 - mse: 0.0459
87/87 [==============================] - 0s 5ms/step - loss: 0.0445 - mae: 0.1479 - mse: 0.0445 - val_loss: 0.0919 - val_mae: 0.2051 - val_mse: 0.0919
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0485 - mae: 0.1526 - mse: 0.0485
64/87 [=====================>........] - ETA: 0s - loss: 0.0410 - mae: 0.1454 - mse: 0.0410
87/87 [==============================] - 0s 5ms/step - loss: 0.0354 - mae: 0.1357 - mse: 0.0354 - val_loss: 0.1182 - val_mae: 0.2465 - val_mse: 0.1182
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0369 - mae: 0.1495 - mse: 0.0369
64/87 [=====================>........] - ETA: 0s - loss: 0.0370 - mae: 0.1460 - mse: 0.0370
87/87 [==============================] - 0s 5ms/step - loss: 0.0373 - mae: 0.1470 - mse: 0.0373 - val_loss: 0.0923 - val_mae: 0.1947 - val_mse: 0.0923
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0299 - mae: 0.1157 - mse: 0.0299
64/87 [=====================>........] - ETA: 0s - loss: 0.0298 - mae: 0.1239 - mse: 0.0298
87/87 [==============================] - 0s 5ms/step - loss: 0.0403 - mae: 0.1472 - mse: 0.0403 - val_loss: 0.0743 - val_mae: 0.1606 - val_mse: 0.0743
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0446 - mae: 0.1433 - mse: 0.0446
64/87 [=====================>........] - ETA: 0s - loss: 0.0347 - mae: 0.1325 - mse: 0.0347
87/87 [==============================] - 1s 6ms/step - loss: 0.0371 - mae: 0.1355 - mse: 0.0371 - val_loss: 0.0797 - val_mae: 0.1777 - val_mse: 0.0797
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0220 - mae: 0.0998 - mse: 0.0220
64/87 [=====================>........] - ETA: 0s - loss: 0.0270 - mae: 0.1164 - mse: 0.0270
87/87 [==============================] - 0s 6ms/step - loss: 0.0321 - mae: 0.1269 - mse: 0.0321 - val_loss: 0.0970 - val_mae: 0.2186 - val_mse: 0.0970
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0250 - mae: 0.1283 - mse: 0.0250
64/87 [=====================>........] - ETA: 0s - loss: 0.0292 - mae: 0.1320 - mse: 0.0292
87/87 [==============================] - 0s 5ms/step - loss: 0.0300 - mae: 0.1345 - mse: 0.0300 - val_loss: 0.0935 - val_mae: 0.2138 - val_mse: 0.0935
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0268 - mae: 0.1104 - mse: 0.0268
64/87 [=====================>........] - ETA: 0s - loss: 0.0324 - mae: 0.1234 - mse: 0.0324
87/87 [==============================] - 1s 6ms/step - loss: 0.0303 - mae: 0.1222 - mse: 0.0303 - val_loss: 0.0743 - val_mae: 0.1737 - val_mse: 0.0743
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0677 - mae: 0.1957 - mse: 0.0677
64/87 [=====================>........] - ETA: 0s - loss: 0.0522 - mae: 0.1609 - mse: 0.0522
87/87 [==============================] - 0s 6ms/step - loss: 0.0427 - mae: 0.1438 - mse: 0.0427 - val_loss: 0.0789 - val_mae: 0.1803 - val_mse: 0.0789
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0433 - mae: 0.1481 - mse: 0.0433
64/87 [=====================>........] - ETA: 0s - loss: 0.0289 - mae: 0.1181 - mse: 0.0289
87/87 [==============================] - 0s 5ms/step - loss: 0.0255 - mae: 0.1125 - mse: 0.0255 - val_loss: 0.0943 - val_mae: 0.2047 - val_mse: 0.0943
Saving trained model...
98
Testing...
heightdiff= [0.        0.        0.        9.7086792 0.        0.       ]
average prediction= [8.19971]
baseline= 11.9
eachuser= [0. 0. 0. 5. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.94173583984375
85 -:- nan
60 -:- nan
['train-weight-10.py', '1']
65 1
2_155_65_10_csi_a10_14.dat
65 3
65 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
65 9
65 10
65 11
65 12
65 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
65 17
2_155_65_10_csi_a10_13.dat
65 19
65 20
2_155_65_10_csi_a10_3.dat
65 22
65 23
65 24
2_155_65_10_csi_a10_4.dat
65 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
65 29
65 30
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
50 89
50 90
50 91
50 92
50 93
50 94
50 95
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
85 131
85 132
85 133
85 134
85 135
85 136
85 137
85 138
85 139
85 140
85 141
85 142
85 143
85 144
85 145
85 146
85 147
85 148
85 149
85 150
85 151
85 152
85 153
85 154
85 155
85 156
1_180_85_10_csi_a10_28.dat
85 158
85 159
85 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
75 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
75 178
75 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
75 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
75 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
85 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60
 60 60 60 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75
 75 85]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.4334 - mae: 0.5663 - mse: 0.4334
64/87 [=====================>........] - ETA: 0s - loss: 0.3268 - mae: 0.4622 - mse: 0.3268
87/87 [==============================] - 1s 9ms/step - loss: 0.2989 - mae: 0.4392 - mse: 0.2989 - val_loss: 0.3337 - val_mae: 0.5170 - val_mse: 0.3337
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1315 - mae: 0.3071 - mse: 0.1315
64/87 [=====================>........] - ETA: 0s - loss: 0.1312 - mae: 0.3041 - mse: 0.1312
87/87 [==============================] - 1s 6ms/step - loss: 0.1404 - mae: 0.3159 - mse: 0.1404 - val_loss: 0.1351 - val_mae: 0.2961 - val_mse: 0.1351
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1691 - mae: 0.3513 - mse: 0.1691
64/87 [=====================>........] - ETA: 0s - loss: 0.1679 - mae: 0.3656 - mse: 0.1679
87/87 [==============================] - 1s 6ms/step - loss: 0.1570 - mae: 0.3454 - mse: 0.1570 - val_loss: 0.1601 - val_mae: 0.3236 - val_mse: 0.1601
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1020 - mae: 0.2714 - mse: 0.1020
64/87 [=====================>........] - ETA: 0s - loss: 0.0924 - mae: 0.2478 - mse: 0.0924
87/87 [==============================] - 0s 6ms/step - loss: 0.1029 - mae: 0.2620 - mse: 0.1029 - val_loss: 0.1986 - val_mae: 0.3703 - val_mse: 0.1986
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1063 - mae: 0.2549 - mse: 0.1063
64/87 [=====================>........] - ETA: 0s - loss: 0.0975 - mae: 0.2466 - mse: 0.0975
87/87 [==============================] - 0s 6ms/step - loss: 0.0883 - mae: 0.2377 - mse: 0.0883 - val_loss: 0.1642 - val_mae: 0.3389 - val_mse: 0.1642
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0853 - mae: 0.2426 - mse: 0.0853
64/87 [=====================>........] - ETA: 0s - loss: 0.0854 - mae: 0.2399 - mse: 0.0854
87/87 [==============================] - 0s 6ms/step - loss: 0.0752 - mae: 0.2259 - mse: 0.0752 - val_loss: 0.0961 - val_mae: 0.2898 - val_mse: 0.0961
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0737 - mae: 0.2324 - mse: 0.0737
64/87 [=====================>........] - ETA: 0s - loss: 0.0591 - mae: 0.2058 - mse: 0.0591
87/87 [==============================] - 0s 6ms/step - loss: 0.0520 - mae: 0.1902 - mse: 0.0520 - val_loss: 0.0679 - val_mae: 0.2558 - val_mse: 0.0679
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0668 - mae: 0.2142 - mse: 0.0668
64/87 [=====================>........] - ETA: 0s - loss: 0.0597 - mae: 0.1973 - mse: 0.0597
87/87 [==============================] - 0s 5ms/step - loss: 0.0649 - mae: 0.2058 - mse: 0.0649 - val_loss: 0.0687 - val_mae: 0.2588 - val_mse: 0.0687
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0456 - mae: 0.1577 - mse: 0.0456
64/87 [=====================>........] - ETA: 0s - loss: 0.0546 - mae: 0.1791 - mse: 0.0546
87/87 [==============================] - 0s 6ms/step - loss: 0.0578 - mae: 0.1880 - mse: 0.0578 - val_loss: 0.0792 - val_mae: 0.2755 - val_mse: 0.0792
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0658 - mae: 0.1926 - mse: 0.0658
64/87 [=====================>........] - ETA: 0s - loss: 0.0562 - mae: 0.1854 - mse: 0.0562
87/87 [==============================] - 0s 5ms/step - loss: 0.0587 - mae: 0.1880 - mse: 0.0587 - val_loss: 0.0826 - val_mae: 0.2782 - val_mse: 0.0826
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0499 - mae: 0.1876 - mse: 0.0499
64/87 [=====================>........] - ETA: 0s - loss: 0.0465 - mae: 0.1796 - mse: 0.0465
87/87 [==============================] - 0s 6ms/step - loss: 0.0449 - mae: 0.1705 - mse: 0.0449 - val_loss: 0.0717 - val_mae: 0.2616 - val_mse: 0.0717
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0337 - mae: 0.1312 - mse: 0.0337
64/87 [=====================>........] - ETA: 0s - loss: 0.0390 - mae: 0.1464 - mse: 0.0390
87/87 [==============================] - 0s 6ms/step - loss: 0.0396 - mae: 0.1507 - mse: 0.0396 - val_loss: 0.0606 - val_mae: 0.2390 - val_mse: 0.0606
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0602 - mae: 0.1664 - mse: 0.0602
64/87 [=====================>........] - ETA: 0s - loss: 0.0519 - mae: 0.1622 - mse: 0.0519
87/87 [==============================] - 0s 5ms/step - loss: 0.0535 - mae: 0.1704 - mse: 0.0535 - val_loss: 0.0672 - val_mae: 0.2516 - val_mse: 0.0672
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0410 - mae: 0.1466 - mse: 0.0410
64/87 [=====================>........] - ETA: 0s - loss: 0.0492 - mae: 0.1712 - mse: 0.0492
87/87 [==============================] - 0s 5ms/step - loss: 0.0454 - mae: 0.1665 - mse: 0.0454 - val_loss: 0.0782 - val_mae: 0.2685 - val_mse: 0.0782
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0507 - mae: 0.1765 - mse: 0.0507
64/87 [=====================>........] - ETA: 0s - loss: 0.0571 - mae: 0.1789 - mse: 0.0571
87/87 [==============================] - 0s 5ms/step - loss: 0.0511 - mae: 0.1699 - mse: 0.0511 - val_loss: 0.0764 - val_mae: 0.2674 - val_mse: 0.0764
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0506 - mae: 0.1608 - mse: 0.0506
64/87 [=====================>........] - ETA: 0s - loss: 0.0514 - mae: 0.1662 - mse: 0.0514
87/87 [==============================] - 0s 6ms/step - loss: 0.0471 - mae: 0.1601 - mse: 0.0471 - val_loss: 0.0748 - val_mae: 0.2661 - val_mse: 0.0748
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0305 - mae: 0.1313 - mse: 0.0305
64/87 [=====================>........] - ETA: 0s - loss: 0.0362 - mae: 0.1491 - mse: 0.0362
87/87 [==============================] - 1s 6ms/step - loss: 0.0381 - mae: 0.1509 - mse: 0.0381 - val_loss: 0.0627 - val_mae: 0.2457 - val_mse: 0.0627
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0433 - mae: 0.1496 - mse: 0.0433
64/87 [=====================>........] - ETA: 0s - loss: 0.0562 - mae: 0.1698 - mse: 0.0562
87/87 [==============================] - 1s 6ms/step - loss: 0.0504 - mae: 0.1593 - mse: 0.0504 - val_loss: 0.0637 - val_mae: 0.2477 - val_mse: 0.0637
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0451 - mae: 0.1591 - mse: 0.0451
64/87 [=====================>........] - ETA: 0s - loss: 0.0335 - mae: 0.1390 - mse: 0.0335
87/87 [==============================] - 0s 5ms/step - loss: 0.0380 - mae: 0.1499 - mse: 0.0380 - val_loss: 0.0657 - val_mae: 0.2501 - val_mse: 0.0657
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0426 - mae: 0.1463 - mse: 0.0426
64/87 [=====================>........] - ETA: 0s - loss: 0.0390 - mae: 0.1437 - mse: 0.0390
87/87 [==============================] - 0s 6ms/step - loss: 0.0443 - mae: 0.1540 - mse: 0.0443 - val_loss: 0.0747 - val_mae: 0.2625 - val_mse: 0.0747
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0439 - mae: 0.1679 - mse: 0.0439
64/87 [=====================>........] - ETA: 0s - loss: 0.0371 - mae: 0.1464 - mse: 0.0371
87/87 [==============================] - 0s 5ms/step - loss: 0.0368 - mae: 0.1461 - mse: 0.0368 - val_loss: 0.0774 - val_mae: 0.2651 - val_mse: 0.0774
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0336 - mae: 0.1363 - mse: 0.0336
64/87 [=====================>........] - ETA: 0s - loss: 0.0378 - mae: 0.1452 - mse: 0.0378
87/87 [==============================] - 0s 5ms/step - loss: 0.0353 - mae: 0.1372 - mse: 0.0353 - val_loss: 0.0684 - val_mae: 0.2508 - val_mse: 0.0684
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0391 - mae: 0.1566 - mse: 0.0391
64/87 [=====================>........] - ETA: 0s - loss: 0.0343 - mae: 0.1435 - mse: 0.0343
87/87 [==============================] - 0s 6ms/step - loss: 0.0358 - mae: 0.1444 - mse: 0.0358 - val_loss: 0.0630 - val_mae: 0.2415 - val_mse: 0.0630
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0290 - mae: 0.1337 - mse: 0.0290
64/87 [=====================>........] - ETA: 0s - loss: 0.0338 - mae: 0.1387 - mse: 0.0338
87/87 [==============================] - 0s 5ms/step - loss: 0.0340 - mae: 0.1386 - mse: 0.0340 - val_loss: 0.0687 - val_mae: 0.2541 - val_mse: 0.0687
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0540 - mae: 0.1799 - mse: 0.0540
64/87 [=====================>........] - ETA: 0s - loss: 0.0420 - mae: 0.1576 - mse: 0.0420
87/87 [==============================] - 0s 5ms/step - loss: 0.0403 - mae: 0.1526 - mse: 0.0403 - val_loss: 0.0702 - val_mae: 0.2572 - val_mse: 0.0702
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0495 - mae: 0.1712 - mse: 0.0495
64/87 [=====================>........] - ETA: 0s - loss: 0.0393 - mae: 0.1487 - mse: 0.0393
87/87 [==============================] - 1s 6ms/step - loss: 0.0362 - mae: 0.1408 - mse: 0.0362 - val_loss: 0.0649 - val_mae: 0.2456 - val_mse: 0.0649
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0404 - mae: 0.1466 - mse: 0.0404
64/87 [=====================>........] - ETA: 0s - loss: 0.0375 - mae: 0.1399 - mse: 0.0375
87/87 [==============================] - 0s 5ms/step - loss: 0.0361 - mae: 0.1378 - mse: 0.0361 - val_loss: 0.0704 - val_mae: 0.2571 - val_mse: 0.0704
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0320 - mae: 0.1287 - mse: 0.0320
64/87 [=====================>........] - ETA: 0s - loss: 0.0295 - mae: 0.1245 - mse: 0.0295
87/87 [==============================] - 0s 5ms/step - loss: 0.0331 - mae: 0.1281 - mse: 0.0331 - val_loss: 0.0705 - val_mae: 0.2563 - val_mse: 0.0705
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0349 - mae: 0.1506 - mse: 0.0349
64/87 [=====================>........] - ETA: 0s - loss: 0.0371 - mae: 0.1503 - mse: 0.0371
87/87 [==============================] - 0s 6ms/step - loss: 0.0335 - mae: 0.1418 - mse: 0.0335 - val_loss: 0.0691 - val_mae: 0.2520 - val_mse: 0.0691
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0363 - mae: 0.1546 - mse: 0.0363
64/87 [=====================>........] - ETA: 0s - loss: 0.0347 - mae: 0.1370 - mse: 0.0347
87/87 [==============================] - 0s 5ms/step - loss: 0.0339 - mae: 0.1353 - mse: 0.0339 - val_loss: 0.0741 - val_mae: 0.2581 - val_mse: 0.0741
Saving trained model...
98
Testing...
heightdiff= [0.         0.         0.         9.81630707 0.         0.        ]
average prediction= [5.6191926]
baseline= 12.3
eachuser= [0. 0. 0. 8. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.2270383834838867
85 -:- nan
60 -:- nan
['train-weight-10.py', '1']
65 1
2_155_65_10_csi_a10_14.dat
65 3
65 4
2_155_65_10_csi_a10_26.dat
2_155_65_10_csi_a10_22.dat
2_155_65_10_csi_a10_23.dat
2_155_65_10_csi_a10_2.dat
65 9
65 10
65 11
65 12
65 13
2_155_65_10_csi_a10_28.dat
2_155_65_10_csi_a10_27.dat
2_155_65_10_csi_a10_15.dat
65 17
2_155_65_10_csi_a10_13.dat
65 19
65 20
2_155_65_10_csi_a10_3.dat
65 22
65 23
65 24
2_155_65_10_csi_a10_4.dat
65 26
2_155_65_10_csi_a10_12.dat
2_155_65_10_csi_a10_1.dat
65 29
65 30
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_10_csi_a10_13.dat
1_165_65_10_csi_a10_28.dat
1_165_65_10_csi_a10_4.dat
1_165_65_10_csi_a10_16.dat
1_165_65_10_csi_a10_12.dat
1_165_65_10_csi_a10_18.dat
1_165_65_10_csi_a10_15.dat
1_165_65_10_csi_a10_6.dat
1_165_65_10_csi_a10_7.dat
1_165_65_10_csi_a10_2.dat
1_165_65_10_csi_a10_24.dat
1_165_65_10_csi_a10_26.dat
1_165_65_10_csi_a10_9.dat
1_165_65_10_csi_a10_23.dat
1_165_65_10_csi_a10_19.dat
1_165_65_10_csi_a10_21.dat
1_165_65_10_csi_a10_1.dat
1_165_65_10_csi_a10_3.dat
1_165_65_10_csi_a10_17.dat
1_165_65_10_csi_a10_14.dat
1_165_65_10_csi_a10_10.dat
1_165_65_10_csi_a10_29.dat
1_165_65_10_csi_a10_20.dat
1_165_65_10_csi_a10_5.dat
1_165_65_10_csi_a10_22.dat
1_165_65_10_csi_a10_27.dat
1_165_65_10_csi_a10_8.dat
1_165_65_10_csi_a10_11.dat
1_165_65_10_csi_a10_30.dat
1_165_65_10_csi_a10_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
50 89
50 90
50 91
50 92
50 93
50 94
50 95
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
85 131
85 132
85 133
85 134
85 135
85 136
85 137
85 138
85 139
85 140
85 141
85 142
85 143
85 144
85 145
85 146
85 147
85 148
85 149
85 150
85 151
85 152
85 153
85 154
85 155
85 156
1_180_85_10_csi_a10_28.dat
85 158
85 159
85 160
1_180_75_10_csi_a10_19.dat
1_180_75_10_csi_a10_26.dat
1_180_75_10_csi_a10_2.dat
1_180_75_10_csi_a10_10.dat
75 165
1_180_75_10_csi_a10_22.dat
1_180_75_10_csi_a10_8.dat
1_180_75_10_csi_a10_14.dat
1_180_75_10_csi_a10_21.dat
1_180_75_10_csi_a10_13.dat
1_180_75_10_csi_a10_15.dat
1_180_75_10_csi_a10_29.dat
1_180_75_10_csi_a10_11.dat
1_180_75_10_csi_a10_9.dat
1_180_75_10_csi_a10_5.dat
1_180_75_10_csi_a10_25.dat
1_180_75_10_csi_a10_3.dat
75 178
75 179
1_180_75_10_csi_a10_7.dat
1_180_75_10_csi_a10_27.dat
75 182
1_180_75_10_csi_a10_16.dat
1_180_75_10_csi_a10_28.dat
1_180_75_10_csi_a10_24.dat
1_180_75_10_csi_a10_30.dat
1_180_75_10_csi_a10_12.dat
1_180_75_10_csi_a10_17.dat
1_180_75_10_csi_a10_18.dat
75 190
1_173_85_10_csi_a10_25.dat
1_173_85_10_csi_a10_22.dat
1_173_85_10_csi_a10_7.dat
1_173_85_10_csi_a10_18.dat
1_173_85_10_csi_a10_14.dat
1_173_85_10_csi_a10_1.dat
1_173_85_10_csi_a10_15.dat
1_173_85_10_csi_a10_24.dat
1_173_85_10_csi_a10_23.dat
1_173_85_10_csi_a10_19.dat
1_173_85_10_csi_a10_21.dat
1_173_85_10_csi_a10_5.dat
1_173_85_10_csi_a10_4.dat
1_173_85_10_csi_a10_30.dat
1_173_85_10_csi_a10_12.dat
1_173_85_10_csi_a10_17.dat
1_173_85_10_csi_a10_2.dat
1_173_85_10_csi_a10_11.dat
1_173_85_10_csi_a10_10.dat
1_173_85_10_csi_a10_29.dat
1_173_85_10_csi_a10_9.dat
1_173_85_10_csi_a10_20.dat
1_173_85_10_csi_a10_16.dat
85 214
1_173_85_10_csi_a10_27.dat
1_173_85_10_csi_a10_6.dat
1_173_85_10_csi_a10_28.dat
1_173_85_10_csi_a10_3.dat
1_173_85_10_csi_a10_13.dat
1_173_85_10_csi_a10_26.dat
(122, 30, 3)
(122, 435, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60
 60 60 60 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75
 75 85]
(122, 435, 30, 3, 1)

Loaded dataset of 122 samples, each sized (435, 30, 3, 1)


Train on 97 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 435, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 435, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 435, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 435, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 435, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 435, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 435, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 87 samples, validate on 10 samples
Epoch 1/30

32/87 [==========>...................] - ETA: 0s - loss: 0.3081 - mae: 0.4502 - mse: 0.3081
64/87 [=====================>........] - ETA: 0s - loss: 0.2714 - mae: 0.4290 - mse: 0.2714
87/87 [==============================] - 1s 9ms/step - loss: 0.2624 - mae: 0.4282 - mse: 0.2624 - val_loss: 0.1187 - val_mae: 0.2927 - val_mse: 0.1187
Epoch 2/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1138 - mae: 0.2749 - mse: 0.1138
64/87 [=====================>........] - ETA: 0s - loss: 0.1305 - mae: 0.3070 - mse: 0.1305
87/87 [==============================] - 0s 5ms/step - loss: 0.1330 - mae: 0.3140 - mse: 0.1330 - val_loss: 0.1403 - val_mae: 0.3492 - val_mse: 0.1403
Epoch 3/30

32/87 [==========>...................] - ETA: 0s - loss: 0.1065 - mae: 0.2856 - mse: 0.1065
64/87 [=====================>........] - ETA: 0s - loss: 0.1295 - mae: 0.3122 - mse: 0.1295
87/87 [==============================] - 0s 5ms/step - loss: 0.1312 - mae: 0.3164 - mse: 0.1312 - val_loss: 0.0912 - val_mae: 0.2768 - val_mse: 0.0912
Epoch 4/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0983 - mae: 0.2648 - mse: 0.0983
64/87 [=====================>........] - ETA: 0s - loss: 0.0941 - mae: 0.2674 - mse: 0.0941
87/87 [==============================] - 0s 5ms/step - loss: 0.0889 - mae: 0.2599 - mse: 0.0889 - val_loss: 0.0772 - val_mae: 0.2399 - val_mse: 0.0772
Epoch 5/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0746 - mae: 0.2416 - mse: 0.0746
64/87 [=====================>........] - ETA: 0s - loss: 0.0756 - mae: 0.2439 - mse: 0.0756
87/87 [==============================] - 0s 5ms/step - loss: 0.0807 - mae: 0.2508 - mse: 0.0807 - val_loss: 0.0729 - val_mae: 0.2236 - val_mse: 0.0729
Epoch 6/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0896 - mae: 0.2551 - mse: 0.0896
64/87 [=====================>........] - ETA: 0s - loss: 0.0708 - mae: 0.2235 - mse: 0.0708
87/87 [==============================] - 0s 5ms/step - loss: 0.0625 - mae: 0.2073 - mse: 0.0625 - val_loss: 0.0586 - val_mae: 0.2095 - val_mse: 0.0586
Epoch 7/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0591 - mae: 0.2131 - mse: 0.0591
64/87 [=====================>........] - ETA: 0s - loss: 0.0575 - mae: 0.1992 - mse: 0.0575
87/87 [==============================] - 0s 5ms/step - loss: 0.0506 - mae: 0.1870 - mse: 0.0506 - val_loss: 0.0587 - val_mae: 0.2004 - val_mse: 0.0587
Epoch 8/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0711 - mae: 0.2114 - mse: 0.0711
64/87 [=====================>........] - ETA: 0s - loss: 0.0627 - mae: 0.1920 - mse: 0.0627
87/87 [==============================] - 0s 5ms/step - loss: 0.0565 - mae: 0.1823 - mse: 0.0565 - val_loss: 0.0549 - val_mae: 0.1873 - val_mse: 0.0549
Epoch 9/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0708 - mae: 0.2211 - mse: 0.0708
64/87 [=====================>........] - ETA: 0s - loss: 0.0526 - mae: 0.1839 - mse: 0.0526
87/87 [==============================] - 0s 5ms/step - loss: 0.0507 - mae: 0.1772 - mse: 0.0507 - val_loss: 0.0522 - val_mae: 0.1830 - val_mse: 0.0522
Epoch 10/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0483 - mae: 0.1754 - mse: 0.0483
64/87 [=====================>........] - ETA: 0s - loss: 0.0417 - mae: 0.1615 - mse: 0.0417
87/87 [==============================] - 0s 5ms/step - loss: 0.0476 - mae: 0.1693 - mse: 0.0476 - val_loss: 0.0523 - val_mae: 0.1820 - val_mse: 0.0523
Epoch 11/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0586 - mae: 0.1865 - mse: 0.0586
64/87 [=====================>........] - ETA: 0s - loss: 0.0495 - mae: 0.1742 - mse: 0.0495
87/87 [==============================] - 0s 5ms/step - loss: 0.0447 - mae: 0.1649 - mse: 0.0447 - val_loss: 0.0511 - val_mae: 0.1783 - val_mse: 0.0511
Epoch 12/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0504 - mae: 0.1668 - mse: 0.0504
64/87 [=====================>........] - ETA: 0s - loss: 0.0513 - mae: 0.1729 - mse: 0.0513
87/87 [==============================] - 0s 5ms/step - loss: 0.0447 - mae: 0.1602 - mse: 0.0447 - val_loss: 0.0516 - val_mae: 0.1797 - val_mse: 0.0516
Epoch 13/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0285 - mae: 0.1226 - mse: 0.0285
64/87 [=====================>........] - ETA: 0s - loss: 0.0380 - mae: 0.1361 - mse: 0.0380
87/87 [==============================] - 0s 5ms/step - loss: 0.0403 - mae: 0.1424 - mse: 0.0403 - val_loss: 0.0496 - val_mae: 0.1769 - val_mse: 0.0496
Epoch 14/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0507 - mae: 0.1775 - mse: 0.0507
64/87 [=====================>........] - ETA: 0s - loss: 0.0467 - mae: 0.1719 - mse: 0.0467
87/87 [==============================] - 0s 5ms/step - loss: 0.0428 - mae: 0.1571 - mse: 0.0428 - val_loss: 0.0476 - val_mae: 0.1738 - val_mse: 0.0476
Epoch 15/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0503 - mae: 0.1773 - mse: 0.0503
64/87 [=====================>........] - ETA: 0s - loss: 0.0578 - mae: 0.1876 - mse: 0.0578
87/87 [==============================] - 0s 5ms/step - loss: 0.0512 - mae: 0.1764 - mse: 0.0512 - val_loss: 0.0486 - val_mae: 0.1716 - val_mse: 0.0486
Epoch 16/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0352 - mae: 0.1384 - mse: 0.0352
64/87 [=====================>........] - ETA: 0s - loss: 0.0460 - mae: 0.1594 - mse: 0.0460
87/87 [==============================] - 0s 5ms/step - loss: 0.0484 - mae: 0.1674 - mse: 0.0484 - val_loss: 0.0423 - val_mae: 0.1580 - val_mse: 0.0423
Epoch 17/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0344 - mae: 0.1402 - mse: 0.0344
64/87 [=====================>........] - ETA: 0s - loss: 0.0487 - mae: 0.1689 - mse: 0.0487
87/87 [==============================] - 1s 8ms/step - loss: 0.0421 - mae: 0.1524 - mse: 0.0421 - val_loss: 0.0355 - val_mae: 0.1444 - val_mse: 0.0355
Epoch 18/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0248 - mae: 0.1160 - mse: 0.0248
64/87 [=====================>........] - ETA: 0s - loss: 0.0279 - mae: 0.1209 - mse: 0.0279
87/87 [==============================] - 1s 12ms/step - loss: 0.0320 - mae: 0.1345 - mse: 0.0320 - val_loss: 0.0305 - val_mae: 0.1355 - val_mse: 0.0305
Epoch 19/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0437 - mae: 0.1528 - mse: 0.0437
64/87 [=====================>........] - ETA: 0s - loss: 0.0529 - mae: 0.1596 - mse: 0.0529
87/87 [==============================] - 1s 9ms/step - loss: 0.0509 - mae: 0.1568 - mse: 0.0509 - val_loss: 0.0336 - val_mae: 0.1384 - val_mse: 0.0336
Epoch 20/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0218 - mae: 0.1103 - mse: 0.0218
64/87 [=====================>........] - ETA: 0s - loss: 0.0307 - mae: 0.1296 - mse: 0.0307
87/87 [==============================] - 1s 6ms/step - loss: 0.0330 - mae: 0.1356 - mse: 0.0330 - val_loss: 0.0445 - val_mae: 0.1453 - val_mse: 0.0445
Epoch 21/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0364 - mae: 0.1542 - mse: 0.0364
64/87 [=====================>........] - ETA: 0s - loss: 0.0386 - mae: 0.1546 - mse: 0.0386
87/87 [==============================] - 1s 6ms/step - loss: 0.0397 - mae: 0.1593 - mse: 0.0397 - val_loss: 0.0367 - val_mae: 0.1366 - val_mse: 0.0367
Epoch 22/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0196 - mae: 0.1018 - mse: 0.0196
64/87 [=====================>........] - ETA: 0s - loss: 0.0284 - mae: 0.1275 - mse: 0.0284
87/87 [==============================] - 1s 6ms/step - loss: 0.0299 - mae: 0.1317 - mse: 0.0299 - val_loss: 0.0292 - val_mae: 0.1280 - val_mse: 0.0292
Epoch 23/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0417 - mae: 0.1697 - mse: 0.0417
64/87 [=====================>........] - ETA: 0s - loss: 0.0303 - mae: 0.1321 - mse: 0.0303
87/87 [==============================] - 0s 6ms/step - loss: 0.0317 - mae: 0.1335 - mse: 0.0317 - val_loss: 0.0300 - val_mae: 0.1296 - val_mse: 0.0300
Epoch 24/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0363 - mae: 0.1340 - mse: 0.0363
64/87 [=====================>........] - ETA: 0s - loss: 0.0317 - mae: 0.1313 - mse: 0.0317
87/87 [==============================] - 0s 5ms/step - loss: 0.0385 - mae: 0.1411 - mse: 0.0385 - val_loss: 0.0404 - val_mae: 0.1433 - val_mse: 0.0404
Epoch 25/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0418 - mae: 0.1588 - mse: 0.0418
64/87 [=====================>........] - ETA: 0s - loss: 0.0357 - mae: 0.1451 - mse: 0.0357
87/87 [==============================] - 0s 6ms/step - loss: 0.0333 - mae: 0.1412 - mse: 0.0333 - val_loss: 0.0466 - val_mae: 0.1506 - val_mse: 0.0466
Epoch 26/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0310 - mae: 0.1473 - mse: 0.0310
64/87 [=====================>........] - ETA: 0s - loss: 0.0360 - mae: 0.1458 - mse: 0.0360
87/87 [==============================] - 1s 6ms/step - loss: 0.0327 - mae: 0.1328 - mse: 0.0327 - val_loss: 0.0396 - val_mae: 0.1419 - val_mse: 0.0396
Epoch 27/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0385 - mae: 0.1467 - mse: 0.0385
64/87 [=====================>........] - ETA: 0s - loss: 0.0278 - mae: 0.1191 - mse: 0.0278
87/87 [==============================] - 0s 6ms/step - loss: 0.0334 - mae: 0.1331 - mse: 0.0334 - val_loss: 0.0325 - val_mae: 0.1312 - val_mse: 0.0325
Epoch 28/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0436 - mae: 0.1686 - mse: 0.0436
64/87 [=====================>........] - ETA: 0s - loss: 0.0388 - mae: 0.1538 - mse: 0.0388
87/87 [==============================] - 0s 6ms/step - loss: 0.0339 - mae: 0.1410 - mse: 0.0339 - val_loss: 0.0226 - val_mae: 0.1144 - val_mse: 0.0226
Epoch 29/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0371 - mae: 0.1351 - mse: 0.0371
64/87 [=====================>........] - ETA: 0s - loss: 0.0390 - mae: 0.1340 - mse: 0.0390
87/87 [==============================] - 0s 6ms/step - loss: 0.0340 - mae: 0.1242 - mse: 0.0340 - val_loss: 0.0369 - val_mae: 0.1309 - val_mse: 0.0369
Epoch 30/30

32/87 [==========>...................] - ETA: 0s - loss: 0.0364 - mae: 0.1384 - mse: 0.0364
64/87 [=====================>........] - ETA: 0s - loss: 0.0346 - mae: 0.1379 - mse: 0.0346
87/87 [==============================] - 0s 6ms/step - loss: 0.0374 - mae: 0.1457 - mse: 0.0374 - val_loss: 0.0483 - val_mae: 0.1453 - val_mse: 0.0483
Saving trained model...
98
Testing...
heightdiff= [0.         0.         0.         3.23755646 0.         0.        ]
average prediction= [7.312066]
baseline= 9.7
eachuser= [0. 0. 0. 4. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.8093891143798828
85 -:- nan
60 -:- nan
