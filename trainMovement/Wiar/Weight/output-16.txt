['train-weight-16.py', '1']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
65 5
65 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
65 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
65 16
65 17
2_155_65_16_csi_a16_21.dat
65 19
65 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
65 25
2_155_65_16_csi_a16_10.dat
65 27
65 28
2_155_65_16_csi_a16_19.dat
65 30
60 31
2_170_60_16_csi_a16_3.dat
60 33
60 34
60 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
60 38
60 39
60 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_16_csi_a16_19.dat
50 90
2_165_50_16_csi_a16_3.dat
50 92
50 93
2_165_50_16_csi_a16_27.dat
50 95
50 96
50 97
50 98
50 99
50 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
70 105
1_175_70_16_csi_a16_23.dat
70 107
70 108
70 109
1_175_70_16_csi_a16_14.dat
70 111
70 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
70 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
70 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
70 123
70 124
1_175_70_16_csi_a16_5.dat
70 126
1_175_70_16_csi_a16_18.dat
70 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
85 132
1_180_85_16_csi_a16_7.dat
85 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
85 139
85 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
85 148
1_180_85_16_csi_a16_17.dat
85 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
85 155
85 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
75 161
75 162
75 163
1_180_75_16_csi_a16_18.dat
75 165
75 166
1_180_75_16_csi_a16_16.dat
75 168
1_180_75_16_csi_a16_9.dat
75 170
75 171
1_180_75_16_csi_a16_17.dat
75 173
75 174
1_180_75_16_csi_a16_13.dat
75 176
1_180_75_16_csi_a16_10.dat
75 178
75 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
75 183
1_180_75_16_csi_a16_5.dat
75 185
75 186
75 187
75 188
75 189
75 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
85 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
85 197
85 198
85 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
85 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
85 205
85 206
85 207
85 208
85 209
85 210
1_173_85_16_csi_a16_29.dat
85 212
1_173_85_16_csi_a16_17.dat
85 214
85 215
1_173_85_16_csi_a16_22.dat
85 217
1_173_85_16_csi_a16_26.dat
85 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70
 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75
 75 75 75 75 75 75 75 75 75 75 75 75 75 85 85 85 85 85 85 85 85 85 85 85
 85 85 85 85 85]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.4012 - mae: 0.5159 - mse: 0.4012
64/72 [=========================>....] - ETA: 0s - loss: 0.3136 - mae: 0.4526 - mse: 0.3136
72/72 [==============================] - 1s 13ms/step - loss: 0.3127 - mae: 0.4607 - mse: 0.3127 - val_loss: 0.1701 - val_mae: 0.3837 - val_mse: 0.1701
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1747 - mae: 0.3709 - mse: 0.1747
64/72 [=========================>....] - ETA: 0s - loss: 0.1425 - mae: 0.3172 - mse: 0.1425
72/72 [==============================] - 0s 6ms/step - loss: 0.1457 - mae: 0.3234 - mse: 0.1457 - val_loss: 0.0717 - val_mae: 0.2060 - val_mse: 0.0717
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1547 - mae: 0.3202 - mse: 0.1547
64/72 [=========================>....] - ETA: 0s - loss: 0.1399 - mae: 0.3125 - mse: 0.1399
72/72 [==============================] - 0s 6ms/step - loss: 0.1343 - mae: 0.3080 - mse: 0.1343 - val_loss: 0.0524 - val_mae: 0.1871 - val_mse: 0.0524
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.0849 - mae: 0.2389 - mse: 0.0849
64/72 [=========================>....] - ETA: 0s - loss: 0.0811 - mae: 0.2405 - mse: 0.0811
72/72 [==============================] - 0s 5ms/step - loss: 0.0816 - mae: 0.2422 - mse: 0.0816 - val_loss: 0.0538 - val_mae: 0.1759 - val_mse: 0.0538
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.0520 - mae: 0.1767 - mse: 0.0520
64/72 [=========================>....] - ETA: 0s - loss: 0.0512 - mae: 0.1702 - mse: 0.0512
72/72 [==============================] - 0s 6ms/step - loss: 0.0480 - mae: 0.1651 - mse: 0.0480 - val_loss: 0.0291 - val_mae: 0.1341 - val_mse: 0.0291
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0426 - mae: 0.1491 - mse: 0.0426
64/72 [=========================>....] - ETA: 0s - loss: 0.0430 - mae: 0.1557 - mse: 0.0430
72/72 [==============================] - 0s 6ms/step - loss: 0.0416 - mae: 0.1550 - mse: 0.0416 - val_loss: 0.0220 - val_mae: 0.1260 - val_mse: 0.0220
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0579 - mae: 0.1943 - mse: 0.0579
64/72 [=========================>....] - ETA: 0s - loss: 0.0509 - mae: 0.1814 - mse: 0.0509
72/72 [==============================] - 0s 5ms/step - loss: 0.0475 - mae: 0.1748 - mse: 0.0475 - val_loss: 0.0234 - val_mae: 0.1337 - val_mse: 0.0234
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0915 - mae: 0.2379 - mse: 0.0915
64/72 [=========================>....] - ETA: 0s - loss: 0.0642 - mae: 0.1999 - mse: 0.0642
72/72 [==============================] - 0s 5ms/step - loss: 0.0636 - mae: 0.1972 - mse: 0.0636 - val_loss: 0.0372 - val_mae: 0.1694 - val_mse: 0.0372
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0486 - mae: 0.1716 - mse: 0.0486
64/72 [=========================>....] - ETA: 0s - loss: 0.0419 - mae: 0.1605 - mse: 0.0419
72/72 [==============================] - 0s 5ms/step - loss: 0.0395 - mae: 0.1564 - mse: 0.0395 - val_loss: 0.0399 - val_mae: 0.1672 - val_mse: 0.0399
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0459 - mae: 0.1743 - mse: 0.0459
64/72 [=========================>....] - ETA: 0s - loss: 0.0331 - mae: 0.1391 - mse: 0.0331
72/72 [==============================] - 0s 6ms/step - loss: 0.0341 - mae: 0.1438 - mse: 0.0341 - val_loss: 0.0197 - val_mae: 0.1125 - val_mse: 0.0197
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0394 - mae: 0.1436 - mse: 0.0394
64/72 [=========================>....] - ETA: 0s - loss: 0.0331 - mae: 0.1366 - mse: 0.0331
72/72 [==============================] - 0s 7ms/step - loss: 0.0328 - mae: 0.1374 - mse: 0.0328 - val_loss: 0.0137 - val_mae: 0.0913 - val_mse: 0.0137
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0243 - mae: 0.1168 - mse: 0.0243
64/72 [=========================>....] - ETA: 0s - loss: 0.0326 - mae: 0.1367 - mse: 0.0326
72/72 [==============================] - 0s 6ms/step - loss: 0.0307 - mae: 0.1330 - mse: 0.0307 - val_loss: 0.0254 - val_mae: 0.1240 - val_mse: 0.0254
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0305 - mae: 0.1213 - mse: 0.0305
64/72 [=========================>....] - ETA: 0s - loss: 0.0366 - mae: 0.1430 - mse: 0.0366
72/72 [==============================] - 0s 6ms/step - loss: 0.0359 - mae: 0.1399 - mse: 0.0359 - val_loss: 0.0420 - val_mae: 0.1592 - val_mse: 0.0420
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0279 - mae: 0.1352 - mse: 0.0279
64/72 [=========================>....] - ETA: 0s - loss: 0.0334 - mae: 0.1400 - mse: 0.0334
72/72 [==============================] - 1s 7ms/step - loss: 0.0324 - mae: 0.1384 - mse: 0.0324 - val_loss: 0.0283 - val_mae: 0.1242 - val_mse: 0.0283
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0184 - mae: 0.1039 - mse: 0.0184
64/72 [=========================>....] - ETA: 0s - loss: 0.0189 - mae: 0.1058 - mse: 0.0189
72/72 [==============================] - 0s 6ms/step - loss: 0.0182 - mae: 0.1054 - mse: 0.0182 - val_loss: 0.0161 - val_mae: 0.0984 - val_mse: 0.0161
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0252 - mae: 0.1224 - mse: 0.0252
64/72 [=========================>....] - ETA: 0s - loss: 0.0204 - mae: 0.1060 - mse: 0.0204
72/72 [==============================] - 0s 6ms/step - loss: 0.0202 - mae: 0.1075 - mse: 0.0202 - val_loss: 0.0185 - val_mae: 0.1093 - val_mse: 0.0185
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0161 - mae: 0.0963 - mse: 0.0161
64/72 [=========================>....] - ETA: 0s - loss: 0.0150 - mae: 0.0923 - mse: 0.0150
72/72 [==============================] - 0s 6ms/step - loss: 0.0161 - mae: 0.0976 - mse: 0.0161 - val_loss: 0.0205 - val_mae: 0.1182 - val_mse: 0.0205
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0206 - mae: 0.1096 - mse: 0.0206
64/72 [=========================>....] - ETA: 0s - loss: 0.0161 - mae: 0.0955 - mse: 0.0161
72/72 [==============================] - 0s 6ms/step - loss: 0.0167 - mae: 0.0953 - mse: 0.0167 - val_loss: 0.0295 - val_mae: 0.1431 - val_mse: 0.0295
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0239 - mae: 0.1141 - mse: 0.0239
64/72 [=========================>....] - ETA: 0s - loss: 0.0221 - mae: 0.1149 - mse: 0.0221
72/72 [==============================] - 0s 7ms/step - loss: 0.0215 - mae: 0.1143 - mse: 0.0215 - val_loss: 0.0178 - val_mae: 0.1116 - val_mse: 0.0178
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0160 - mae: 0.0965 - mse: 0.0160
64/72 [=========================>....] - ETA: 0s - loss: 0.0238 - mae: 0.1137 - mse: 0.0238
72/72 [==============================] - 0s 5ms/step - loss: 0.0232 - mae: 0.1123 - mse: 0.0232 - val_loss: 0.0130 - val_mae: 0.0923 - val_mse: 0.0130
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0139 - mae: 0.0922 - mse: 0.0139
64/72 [=========================>....] - ETA: 0s - loss: 0.0191 - mae: 0.1008 - mse: 0.0191
72/72 [==============================] - 0s 6ms/step - loss: 0.0182 - mae: 0.0979 - mse: 0.0182 - val_loss: 0.0405 - val_mae: 0.1648 - val_mse: 0.0405
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0235 - mae: 0.1234 - mse: 0.0235
64/72 [=========================>....] - ETA: 0s - loss: 0.0190 - mae: 0.1075 - mse: 0.0190
72/72 [==============================] - 0s 6ms/step - loss: 0.0175 - mae: 0.1017 - mse: 0.0175 - val_loss: 0.0387 - val_mae: 0.1547 - val_mse: 0.0387
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0191 - mae: 0.1022 - mse: 0.0191
64/72 [=========================>....] - ETA: 0s - loss: 0.0187 - mae: 0.1018 - mse: 0.0187
72/72 [==============================] - 0s 5ms/step - loss: 0.0172 - mae: 0.0974 - mse: 0.0172 - val_loss: 0.0179 - val_mae: 0.0873 - val_mse: 0.0179
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0231 - mae: 0.1200 - mse: 0.0231
64/72 [=========================>....] - ETA: 0s - loss: 0.0230 - mae: 0.1126 - mse: 0.0230
72/72 [==============================] - 0s 5ms/step - loss: 0.0221 - mae: 0.1101 - mse: 0.0221 - val_loss: 0.0132 - val_mae: 0.0725 - val_mse: 0.0132
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0330 - mae: 0.1500 - mse: 0.0330
64/72 [=========================>....] - ETA: 0s - loss: 0.0235 - mae: 0.1165 - mse: 0.0235
72/72 [==============================] - 0s 6ms/step - loss: 0.0217 - mae: 0.1117 - mse: 0.0217 - val_loss: 0.0366 - val_mae: 0.1515 - val_mse: 0.0366
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0190 - mae: 0.1018 - mse: 0.0190
64/72 [=========================>....] - ETA: 0s - loss: 0.0161 - mae: 0.0975 - mse: 0.0161
72/72 [==============================] - 0s 5ms/step - loss: 0.0177 - mae: 0.1019 - mse: 0.0177 - val_loss: 0.0320 - val_mae: 0.1424 - val_mse: 0.0320
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0147 - mae: 0.0909 - mse: 0.0147
64/72 [=========================>....] - ETA: 0s - loss: 0.0145 - mae: 0.0919 - mse: 0.0145
72/72 [==============================] - 0s 5ms/step - loss: 0.0152 - mae: 0.0955 - mse: 0.0152 - val_loss: 0.0078 - val_mae: 0.0592 - val_mse: 0.0078
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0327 - mae: 0.1393 - mse: 0.0327
64/72 [=========================>....] - ETA: 0s - loss: 0.0276 - mae: 0.1247 - mse: 0.0276
72/72 [==============================] - 0s 5ms/step - loss: 0.0273 - mae: 0.1243 - mse: 0.0273 - val_loss: 0.0186 - val_mae: 0.1050 - val_mse: 0.0186
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0155 - mae: 0.0948 - mse: 0.0155
64/72 [=========================>....] - ETA: 0s - loss: 0.0161 - mae: 0.0943 - mse: 0.0161
72/72 [==============================] - 0s 6ms/step - loss: 0.0172 - mae: 0.0978 - mse: 0.0172 - val_loss: 0.0361 - val_mae: 0.1574 - val_mse: 0.0361
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0133 - mae: 0.0928 - mse: 0.0133
64/72 [=========================>....] - ETA: 0s - loss: 0.0160 - mae: 0.0971 - mse: 0.0160
72/72 [==============================] - 0s 6ms/step - loss: 0.0154 - mae: 0.0968 - mse: 0.0154 - val_loss: 0.0182 - val_mae: 0.0999 - val_mse: 0.0182
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         3.95892715 0.         0.        ]
average prediction= [3.6118205]
baseline= 9.880952380952381
eachuser= [0. 0. 0. 3. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.3196423848470051
85 -:- nan
60 -:- nan
['train-weight-16.py', '1']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
65 5
65 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
65 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
65 16
65 17
2_155_65_16_csi_a16_21.dat
65 19
65 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
65 25
2_155_65_16_csi_a16_10.dat
65 27
65 28
2_155_65_16_csi_a16_19.dat
65 30
60 31
2_170_60_16_csi_a16_3.dat
60 33
60 34
60 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
60 38
60 39
60 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_16_csi_a16_19.dat
50 90
2_165_50_16_csi_a16_3.dat
50 92
50 93
2_165_50_16_csi_a16_27.dat
50 95
50 96
50 97
50 98
50 99
50 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
70 105
1_175_70_16_csi_a16_23.dat
70 107
70 108
70 109
1_175_70_16_csi_a16_14.dat
70 111
70 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
70 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
70 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
70 123
70 124
1_175_70_16_csi_a16_5.dat
70 126
1_175_70_16_csi_a16_18.dat
70 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
85 132
1_180_85_16_csi_a16_7.dat
85 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
85 139
85 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
85 148
1_180_85_16_csi_a16_17.dat
85 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
85 155
85 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
75 161
75 162
75 163
1_180_75_16_csi_a16_18.dat
75 165
75 166
1_180_75_16_csi_a16_16.dat
75 168
1_180_75_16_csi_a16_9.dat
75 170
75 171
1_180_75_16_csi_a16_17.dat
75 173
75 174
1_180_75_16_csi_a16_13.dat
75 176
1_180_75_16_csi_a16_10.dat
75 178
75 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
75 183
1_180_75_16_csi_a16_5.dat
75 185
75 186
75 187
75 188
75 189
75 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
85 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
85 197
85 198
85 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
85 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
85 205
85 206
85 207
85 208
85 209
85 210
1_173_85_16_csi_a16_29.dat
85 212
1_173_85_16_csi_a16_17.dat
85 214
85 215
1_173_85_16_csi_a16_22.dat
85 217
1_173_85_16_csi_a16_26.dat
85 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70
 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75
 75 75 75 75 75 75 75 75 75 75 75 75 75 85 85 85 85 85 85 85 85 85 85 85
 85 85 85 85 85]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.4370 - mae: 0.5604 - mse: 0.4370
64/72 [=========================>....] - ETA: 0s - loss: 0.3013 - mae: 0.4510 - mse: 0.3013
72/72 [==============================] - 1s 11ms/step - loss: 0.2821 - mae: 0.4324 - mse: 0.2821 - val_loss: 0.1012 - val_mae: 0.2709 - val_mse: 0.1012
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1020 - mae: 0.2588 - mse: 0.1020
64/72 [=========================>....] - ETA: 0s - loss: 0.1357 - mae: 0.3028 - mse: 0.1357
72/72 [==============================] - 0s 7ms/step - loss: 0.1377 - mae: 0.3072 - mse: 0.1377 - val_loss: 0.0224 - val_mae: 0.1221 - val_mse: 0.0224
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1507 - mae: 0.3202 - mse: 0.1507
64/72 [=========================>....] - ETA: 0s - loss: 0.1328 - mae: 0.3039 - mse: 0.1328
72/72 [==============================] - 0s 7ms/step - loss: 0.1233 - mae: 0.2891 - mse: 0.1233 - val_loss: 0.0506 - val_mae: 0.1871 - val_mse: 0.0506
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.0657 - mae: 0.2155 - mse: 0.0657
64/72 [=========================>....] - ETA: 0s - loss: 0.0672 - mae: 0.2234 - mse: 0.0672
72/72 [==============================] - 0s 7ms/step - loss: 0.0640 - mae: 0.2186 - mse: 0.0640 - val_loss: 0.0903 - val_mae: 0.2808 - val_mse: 0.0903
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.0716 - mae: 0.2330 - mse: 0.0716
64/72 [=========================>....] - ETA: 0s - loss: 0.0702 - mae: 0.2265 - mse: 0.0702
72/72 [==============================] - 1s 7ms/step - loss: 0.0682 - mae: 0.2233 - mse: 0.0682 - val_loss: 0.0881 - val_mae: 0.2802 - val_mse: 0.0881
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0539 - mae: 0.1768 - mse: 0.0539
64/72 [=========================>....] - ETA: 0s - loss: 0.0554 - mae: 0.1823 - mse: 0.0554
72/72 [==============================] - 0s 7ms/step - loss: 0.0538 - mae: 0.1794 - mse: 0.0538 - val_loss: 0.0373 - val_mae: 0.1660 - val_mse: 0.0373
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0380 - mae: 0.1487 - mse: 0.0380
64/72 [=========================>....] - ETA: 0s - loss: 0.0425 - mae: 0.1551 - mse: 0.0425
72/72 [==============================] - 0s 6ms/step - loss: 0.0459 - mae: 0.1601 - mse: 0.0459 - val_loss: 0.0197 - val_mae: 0.1161 - val_mse: 0.0197
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0313 - mae: 0.1435 - mse: 0.0313
64/72 [=========================>....] - ETA: 0s - loss: 0.0445 - mae: 0.1625 - mse: 0.0445
72/72 [==============================] - 0s 7ms/step - loss: 0.0445 - mae: 0.1632 - mse: 0.0445 - val_loss: 0.0301 - val_mae: 0.1469 - val_mse: 0.0301
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0437 - mae: 0.1473 - mse: 0.0437
64/72 [=========================>....] - ETA: 0s - loss: 0.0414 - mae: 0.1529 - mse: 0.0414
72/72 [==============================] - 0s 7ms/step - loss: 0.0424 - mae: 0.1578 - mse: 0.0424 - val_loss: 0.0336 - val_mae: 0.1580 - val_mse: 0.0336
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0281 - mae: 0.1255 - mse: 0.0281
64/72 [=========================>....] - ETA: 0s - loss: 0.0319 - mae: 0.1408 - mse: 0.0319
72/72 [==============================] - 0s 6ms/step - loss: 0.0352 - mae: 0.1491 - mse: 0.0352 - val_loss: 0.0290 - val_mae: 0.1468 - val_mse: 0.0290
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0247 - mae: 0.1300 - mse: 0.0247
64/72 [=========================>....] - ETA: 0s - loss: 0.0281 - mae: 0.1294 - mse: 0.0281
72/72 [==============================] - 0s 7ms/step - loss: 0.0302 - mae: 0.1339 - mse: 0.0302 - val_loss: 0.0258 - val_mae: 0.1385 - val_mse: 0.0258
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0365 - mae: 0.1334 - mse: 0.0365
64/72 [=========================>....] - ETA: 0s - loss: 0.0266 - mae: 0.1167 - mse: 0.0266
72/72 [==============================] - 0s 7ms/step - loss: 0.0318 - mae: 0.1289 - mse: 0.0318 - val_loss: 0.0212 - val_mae: 0.1266 - val_mse: 0.0212
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0372 - mae: 0.1431 - mse: 0.0372
64/72 [=========================>....] - ETA: 0s - loss: 0.0285 - mae: 0.1243 - mse: 0.0285
72/72 [==============================] - 1s 7ms/step - loss: 0.0290 - mae: 0.1252 - mse: 0.0290 - val_loss: 0.0287 - val_mae: 0.1492 - val_mse: 0.0287
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0225 - mae: 0.1105 - mse: 0.0225
64/72 [=========================>....] - ETA: 0s - loss: 0.0277 - mae: 0.1235 - mse: 0.0277
72/72 [==============================] - 0s 7ms/step - loss: 0.0288 - mae: 0.1273 - mse: 0.0288 - val_loss: 0.0576 - val_mae: 0.2157 - val_mse: 0.0576
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0202 - mae: 0.1065 - mse: 0.0202
64/72 [=========================>....] - ETA: 0s - loss: 0.0301 - mae: 0.1339 - mse: 0.0301
72/72 [==============================] - 0s 7ms/step - loss: 0.0312 - mae: 0.1370 - mse: 0.0312 - val_loss: 0.0495 - val_mae: 0.1976 - val_mse: 0.0495
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0366 - mae: 0.1342 - mse: 0.0366
64/72 [=========================>....] - ETA: 0s - loss: 0.0257 - mae: 0.1159 - mse: 0.0257
72/72 [==============================] - 1s 7ms/step - loss: 0.0259 - mae: 0.1179 - mse: 0.0259 - val_loss: 0.0218 - val_mae: 0.1320 - val_mse: 0.0218
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0139 - mae: 0.0934 - mse: 0.0139
64/72 [=========================>....] - ETA: 0s - loss: 0.0168 - mae: 0.1011 - mse: 0.0168
72/72 [==============================] - 1s 7ms/step - loss: 0.0177 - mae: 0.1034 - mse: 0.0177 - val_loss: 0.0212 - val_mae: 0.1321 - val_mse: 0.0212
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0413 - mae: 0.1533 - mse: 0.0413
64/72 [=========================>....] - ETA: 0s - loss: 0.0323 - mae: 0.1354 - mse: 0.0323
72/72 [==============================] - 0s 6ms/step - loss: 0.0313 - mae: 0.1345 - mse: 0.0313 - val_loss: 0.0299 - val_mae: 0.1579 - val_mse: 0.0299
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0324 - mae: 0.1381 - mse: 0.0324
64/72 [=========================>....] - ETA: 0s - loss: 0.0299 - mae: 0.1292 - mse: 0.0299
72/72 [==============================] - 0s 7ms/step - loss: 0.0295 - mae: 0.1298 - mse: 0.0295 - val_loss: 0.0439 - val_mae: 0.1895 - val_mse: 0.0439
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0208 - mae: 0.1094 - mse: 0.0208
64/72 [=========================>....] - ETA: 0s - loss: 0.0209 - mae: 0.1089 - mse: 0.0209
72/72 [==============================] - 0s 6ms/step - loss: 0.0204 - mae: 0.1077 - mse: 0.0204 - val_loss: 0.0595 - val_mae: 0.2265 - val_mse: 0.0595
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0196 - mae: 0.1116 - mse: 0.0196
64/72 [=========================>....] - ETA: 0s - loss: 0.0222 - mae: 0.1115 - mse: 0.0222
72/72 [==============================] - 0s 6ms/step - loss: 0.0214 - mae: 0.1105 - mse: 0.0214 - val_loss: 0.0273 - val_mae: 0.1507 - val_mse: 0.0273
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0142 - mae: 0.0950 - mse: 0.0142
64/72 [=========================>....] - ETA: 0s - loss: 0.0276 - mae: 0.1208 - mse: 0.0276
72/72 [==============================] - 0s 6ms/step - loss: 0.0261 - mae: 0.1182 - mse: 0.0261 - val_loss: 0.0184 - val_mae: 0.1210 - val_mse: 0.0184
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0212 - mae: 0.1108 - mse: 0.0212
64/72 [=========================>....] - ETA: 0s - loss: 0.0179 - mae: 0.1001 - mse: 0.0179
72/72 [==============================] - 0s 6ms/step - loss: 0.0196 - mae: 0.1072 - mse: 0.0196 - val_loss: 0.0430 - val_mae: 0.1843 - val_mse: 0.0430
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0175 - mae: 0.1068 - mse: 0.0175
64/72 [=========================>....] - ETA: 0s - loss: 0.0207 - mae: 0.1136 - mse: 0.0207
72/72 [==============================] - 0s 6ms/step - loss: 0.0190 - mae: 0.1080 - mse: 0.0190 - val_loss: 0.0467 - val_mae: 0.1940 - val_mse: 0.0467
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0188 - mae: 0.1131 - mse: 0.0188
64/72 [=========================>....] - ETA: 0s - loss: 0.0172 - mae: 0.1069 - mse: 0.0172
72/72 [==============================] - 0s 7ms/step - loss: 0.0165 - mae: 0.1039 - mse: 0.0165 - val_loss: 0.0260 - val_mae: 0.1394 - val_mse: 0.0260
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0178 - mae: 0.0969 - mse: 0.0178
64/72 [=========================>....] - ETA: 0s - loss: 0.0193 - mae: 0.0996 - mse: 0.0193
72/72 [==============================] - 0s 6ms/step - loss: 0.0186 - mae: 0.0989 - mse: 0.0186 - val_loss: 0.0358 - val_mae: 0.1696 - val_mse: 0.0358
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0136 - mae: 0.0921 - mse: 0.0136
64/72 [=========================>....] - ETA: 0s - loss: 0.0131 - mae: 0.0904 - mse: 0.0131
72/72 [==============================] - 0s 6ms/step - loss: 0.0145 - mae: 0.0952 - mse: 0.0145 - val_loss: 0.0384 - val_mae: 0.1788 - val_mse: 0.0384
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0108 - mae: 0.0797 - mse: 0.0108
64/72 [=========================>....] - ETA: 0s - loss: 0.0123 - mae: 0.0839 - mse: 0.0123
72/72 [==============================] - 0s 6ms/step - loss: 0.0135 - mae: 0.0887 - mse: 0.0135 - val_loss: 0.0286 - val_mae: 0.1510 - val_mse: 0.0286
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0135 - mae: 0.0869 - mse: 0.0135
64/72 [=========================>....] - ETA: 0s - loss: 0.0160 - mae: 0.0926 - mse: 0.0160
72/72 [==============================] - 0s 6ms/step - loss: 0.0147 - mae: 0.0885 - mse: 0.0147 - val_loss: 0.0173 - val_mae: 0.1163 - val_mse: 0.0173
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0138 - mae: 0.0878 - mse: 0.0138
64/72 [=========================>....] - ETA: 0s - loss: 0.0158 - mae: 0.0983 - mse: 0.0158
72/72 [==============================] - 0s 7ms/step - loss: 0.0161 - mae: 0.0986 - mse: 0.0161 - val_loss: 0.0245 - val_mae: 0.1387 - val_mse: 0.0245
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         5.70489883 0.         0.        ]
average prediction= [2.3902726]
baseline= 12.5
eachuser= [0. 0. 0. 7. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.8149855477469308
85 -:- nan
60 -:- nan
['train-weight-16.py', '1']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
65 5
65 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
65 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
65 16
65 17
2_155_65_16_csi_a16_21.dat
65 19
65 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
65 25
2_155_65_16_csi_a16_10.dat
65 27
65 28
2_155_65_16_csi_a16_19.dat
65 30
60 31
2_170_60_16_csi_a16_3.dat
60 33
60 34
60 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
60 38
60 39
60 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_16_csi_a16_19.dat
50 90
2_165_50_16_csi_a16_3.dat
50 92
50 93
2_165_50_16_csi_a16_27.dat
50 95
50 96
50 97
50 98
50 99
50 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
70 105
1_175_70_16_csi_a16_23.dat
70 107
70 108
70 109
1_175_70_16_csi_a16_14.dat
70 111
70 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
70 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
70 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
70 123
70 124
1_175_70_16_csi_a16_5.dat
70 126
1_175_70_16_csi_a16_18.dat
70 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
85 132
1_180_85_16_csi_a16_7.dat
85 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
85 139
85 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
85 148
1_180_85_16_csi_a16_17.dat
85 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
85 155
85 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
75 161
75 162
75 163
1_180_75_16_csi_a16_18.dat
75 165
75 166
1_180_75_16_csi_a16_16.dat
75 168
1_180_75_16_csi_a16_9.dat
75 170
75 171
1_180_75_16_csi_a16_17.dat
75 173
75 174
1_180_75_16_csi_a16_13.dat
75 176
1_180_75_16_csi_a16_10.dat
75 178
75 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
75 183
1_180_75_16_csi_a16_5.dat
75 185
75 186
75 187
75 188
75 189
75 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
85 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
85 197
85 198
85 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
85 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
85 205
85 206
85 207
85 208
85 209
85 210
1_173_85_16_csi_a16_29.dat
85 212
1_173_85_16_csi_a16_17.dat
85 214
85 215
1_173_85_16_csi_a16_22.dat
85 217
1_173_85_16_csi_a16_26.dat
85 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70
 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75
 75 75 75 75 75 75 75 75 75 75 75 75 75 85 85 85 85 85 85 85 85 85 85 85
 85 85 85 85 85]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.3831 - mae: 0.4942 - mse: 0.3831
64/72 [=========================>....] - ETA: 0s - loss: 0.3262 - mae: 0.4549 - mse: 0.3262
72/72 [==============================] - 1s 11ms/step - loss: 0.3074 - mae: 0.4452 - mse: 0.3074 - val_loss: 0.2053 - val_mae: 0.4035 - val_mse: 0.2053
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1111 - mae: 0.2972 - mse: 0.1111
64/72 [=========================>....] - ETA: 0s - loss: 0.1135 - mae: 0.3012 - mse: 0.1135
72/72 [==============================] - 1s 7ms/step - loss: 0.1099 - mae: 0.2921 - mse: 0.1099 - val_loss: 0.0956 - val_mae: 0.2706 - val_mse: 0.0956
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1211 - mae: 0.2676 - mse: 0.1211
64/72 [=========================>....] - ETA: 0s - loss: 0.1258 - mae: 0.2745 - mse: 0.1258
72/72 [==============================] - 0s 7ms/step - loss: 0.1234 - mae: 0.2714 - mse: 0.1234 - val_loss: 0.0768 - val_mae: 0.2711 - val_mse: 0.0768
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.1111 - mae: 0.2884 - mse: 0.1111
64/72 [=========================>....] - ETA: 0s - loss: 0.0919 - mae: 0.2645 - mse: 0.0919
72/72 [==============================] - 0s 7ms/step - loss: 0.0911 - mae: 0.2654 - mse: 0.0911 - val_loss: 0.1106 - val_mae: 0.3106 - val_mse: 0.1106
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.0694 - mae: 0.2414 - mse: 0.0694
64/72 [=========================>....] - ETA: 0s - loss: 0.0654 - mae: 0.2303 - mse: 0.0654
72/72 [==============================] - 0s 7ms/step - loss: 0.0657 - mae: 0.2308 - mse: 0.0657 - val_loss: 0.1074 - val_mae: 0.2979 - val_mse: 0.1074
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0823 - mae: 0.2500 - mse: 0.0823
64/72 [=========================>....] - ETA: 0s - loss: 0.0675 - mae: 0.2280 - mse: 0.0675
72/72 [==============================] - 0s 7ms/step - loss: 0.0647 - mae: 0.2235 - mse: 0.0647 - val_loss: 0.0666 - val_mae: 0.2448 - val_mse: 0.0666
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0699 - mae: 0.2179 - mse: 0.0699
64/72 [=========================>....] - ETA: 0s - loss: 0.0616 - mae: 0.1999 - mse: 0.0616
72/72 [==============================] - 0s 7ms/step - loss: 0.0577 - mae: 0.1936 - mse: 0.0577 - val_loss: 0.0419 - val_mae: 0.1855 - val_mse: 0.0419
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0370 - mae: 0.1495 - mse: 0.0370
64/72 [=========================>....] - ETA: 0s - loss: 0.0403 - mae: 0.1584 - mse: 0.0403
72/72 [==============================] - 0s 7ms/step - loss: 0.0452 - mae: 0.1667 - mse: 0.0452 - val_loss: 0.0402 - val_mae: 0.1783 - val_mse: 0.0402
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0402 - mae: 0.1577 - mse: 0.0402
64/72 [=========================>....] - ETA: 0s - loss: 0.0367 - mae: 0.1496 - mse: 0.0367
72/72 [==============================] - 0s 7ms/step - loss: 0.0377 - mae: 0.1526 - mse: 0.0377 - val_loss: 0.0573 - val_mae: 0.2138 - val_mse: 0.0573
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0411 - mae: 0.1490 - mse: 0.0411
64/72 [=========================>....] - ETA: 0s - loss: 0.0390 - mae: 0.1457 - mse: 0.0390
72/72 [==============================] - 0s 7ms/step - loss: 0.0416 - mae: 0.1542 - mse: 0.0416 - val_loss: 0.0578 - val_mae: 0.2154 - val_mse: 0.0578
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0366 - mae: 0.1503 - mse: 0.0366
64/72 [=========================>....] - ETA: 0s - loss: 0.0394 - mae: 0.1534 - mse: 0.0394
72/72 [==============================] - 0s 7ms/step - loss: 0.0377 - mae: 0.1501 - mse: 0.0377 - val_loss: 0.0426 - val_mae: 0.1872 - val_mse: 0.0426
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0288 - mae: 0.1282 - mse: 0.0288
64/72 [=========================>....] - ETA: 0s - loss: 0.0347 - mae: 0.1305 - mse: 0.0347
72/72 [==============================] - 1s 7ms/step - loss: 0.0322 - mae: 0.1271 - mse: 0.0322 - val_loss: 0.0347 - val_mae: 0.1633 - val_mse: 0.0347
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0367 - mae: 0.1456 - mse: 0.0367
64/72 [=========================>....] - ETA: 0s - loss: 0.0439 - mae: 0.1485 - mse: 0.0439
72/72 [==============================] - 0s 6ms/step - loss: 0.0429 - mae: 0.1473 - mse: 0.0429 - val_loss: 0.0409 - val_mae: 0.1854 - val_mse: 0.0409
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0206 - mae: 0.1057 - mse: 0.0206
64/72 [=========================>....] - ETA: 0s - loss: 0.0347 - mae: 0.1369 - mse: 0.0347
72/72 [==============================] - 0s 6ms/step - loss: 0.0381 - mae: 0.1455 - mse: 0.0381 - val_loss: 0.0594 - val_mae: 0.2184 - val_mse: 0.0594
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0419 - mae: 0.1574 - mse: 0.0419
64/72 [=========================>....] - ETA: 0s - loss: 0.0298 - mae: 0.1295 - mse: 0.0298
72/72 [==============================] - 0s 7ms/step - loss: 0.0333 - mae: 0.1327 - mse: 0.0333 - val_loss: 0.0712 - val_mae: 0.2331 - val_mse: 0.0712
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0311 - mae: 0.1429 - mse: 0.0311
64/72 [=========================>....] - ETA: 0s - loss: 0.0284 - mae: 0.1362 - mse: 0.0284
72/72 [==============================] - 0s 7ms/step - loss: 0.0285 - mae: 0.1363 - mse: 0.0285 - val_loss: 0.0739 - val_mae: 0.2319 - val_mse: 0.0739
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0302 - mae: 0.1343 - mse: 0.0302
64/72 [=========================>....] - ETA: 0s - loss: 0.0308 - mae: 0.1321 - mse: 0.0308
72/72 [==============================] - 0s 7ms/step - loss: 0.0299 - mae: 0.1313 - mse: 0.0299 - val_loss: 0.0603 - val_mae: 0.2089 - val_mse: 0.0603
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0272 - mae: 0.1323 - mse: 0.0272
64/72 [=========================>....] - ETA: 0s - loss: 0.0218 - mae: 0.1157 - mse: 0.0218
72/72 [==============================] - 0s 7ms/step - loss: 0.0209 - mae: 0.1130 - mse: 0.0209 - val_loss: 0.0380 - val_mae: 0.1689 - val_mse: 0.0380
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0387 - mae: 0.1530 - mse: 0.0387
64/72 [=========================>....] - ETA: 0s - loss: 0.0302 - mae: 0.1288 - mse: 0.0302
72/72 [==============================] - 0s 7ms/step - loss: 0.0286 - mae: 0.1248 - mse: 0.0286 - val_loss: 0.0211 - val_mae: 0.1219 - val_mse: 0.0211
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0124 - mae: 0.0868 - mse: 0.0124
64/72 [=========================>....] - ETA: 0s - loss: 0.0161 - mae: 0.0977 - mse: 0.0161
72/72 [==============================] - 0s 7ms/step - loss: 0.0205 - mae: 0.1041 - mse: 0.0205 - val_loss: 0.0226 - val_mae: 0.1334 - val_mse: 0.0226
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0280 - mae: 0.1088 - mse: 0.0280
64/72 [=========================>....] - ETA: 0s - loss: 0.0239 - mae: 0.1066 - mse: 0.0239
72/72 [==============================] - 0s 7ms/step - loss: 0.0238 - mae: 0.1086 - mse: 0.0238 - val_loss: 0.0532 - val_mae: 0.2065 - val_mse: 0.0532
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0357 - mae: 0.1499 - mse: 0.0357
64/72 [=========================>....] - ETA: 0s - loss: 0.0295 - mae: 0.1357 - mse: 0.0295
72/72 [==============================] - 0s 6ms/step - loss: 0.0318 - mae: 0.1418 - mse: 0.0318 - val_loss: 0.0429 - val_mae: 0.1864 - val_mse: 0.0429
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0187 - mae: 0.1015 - mse: 0.0187
64/72 [=========================>....] - ETA: 0s - loss: 0.0175 - mae: 0.1022 - mse: 0.0175
72/72 [==============================] - 0s 6ms/step - loss: 0.0181 - mae: 0.1041 - mse: 0.0181 - val_loss: 0.0275 - val_mae: 0.1493 - val_mse: 0.0275
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0128 - mae: 0.0899 - mse: 0.0128
64/72 [=========================>....] - ETA: 0s - loss: 0.0145 - mae: 0.0910 - mse: 0.0145
72/72 [==============================] - 0s 7ms/step - loss: 0.0154 - mae: 0.0937 - mse: 0.0154 - val_loss: 0.0223 - val_mae: 0.1337 - val_mse: 0.0223
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0217 - mae: 0.1074 - mse: 0.0217
64/72 [=========================>....] - ETA: 0s - loss: 0.0182 - mae: 0.0967 - mse: 0.0182
72/72 [==============================] - 0s 7ms/step - loss: 0.0187 - mae: 0.0955 - mse: 0.0187 - val_loss: 0.0177 - val_mae: 0.1217 - val_mse: 0.0177
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0246 - mae: 0.1042 - mse: 0.0246
64/72 [=========================>....] - ETA: 0s - loss: 0.0232 - mae: 0.1139 - mse: 0.0232
72/72 [==============================] - 0s 7ms/step - loss: 0.0232 - mae: 0.1149 - mse: 0.0232 - val_loss: 0.0267 - val_mae: 0.1478 - val_mse: 0.0267
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0120 - mae: 0.0841 - mse: 0.0120
64/72 [=========================>....] - ETA: 0s - loss: 0.0122 - mae: 0.0862 - mse: 0.0122
72/72 [==============================] - 0s 7ms/step - loss: 0.0150 - mae: 0.0934 - mse: 0.0150 - val_loss: 0.0194 - val_mae: 0.1268 - val_mse: 0.0194
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0184 - mae: 0.0946 - mse: 0.0184
64/72 [=========================>....] - ETA: 0s - loss: 0.0182 - mae: 0.1023 - mse: 0.0182
72/72 [==============================] - 0s 6ms/step - loss: 0.0181 - mae: 0.1034 - mse: 0.0181 - val_loss: 0.0097 - val_mae: 0.0880 - val_mse: 0.0097
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0140 - mae: 0.0889 - mse: 0.0140
64/72 [=========================>....] - ETA: 0s - loss: 0.0145 - mae: 0.0956 - mse: 0.0145
72/72 [==============================] - 0s 7ms/step - loss: 0.0162 - mae: 0.1004 - mse: 0.0162 - val_loss: 0.0165 - val_mae: 0.1108 - val_mse: 0.0165
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0157 - mae: 0.0983 - mse: 0.0157
64/72 [=========================>....] - ETA: 0s - loss: 0.0163 - mae: 0.0988 - mse: 0.0163
72/72 [==============================] - 0s 7ms/step - loss: 0.0170 - mae: 0.1014 - mse: 0.0170 - val_loss: 0.0468 - val_mae: 0.1950 - val_mse: 0.0468
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         4.21498489 0.         0.        ]
average prediction= [3.7141628]
baseline= 7.976190476190476
eachuser= [0. 0. 0. 4. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.053746223449707
85 -:- nan
60 -:- nan
['train-weight-16.py', '1']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
65 5
65 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
65 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
65 16
65 17
2_155_65_16_csi_a16_21.dat
65 19
65 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
65 25
2_155_65_16_csi_a16_10.dat
65 27
65 28
2_155_65_16_csi_a16_19.dat
65 30
60 31
2_170_60_16_csi_a16_3.dat
60 33
60 34
60 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
60 38
60 39
60 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_16_csi_a16_19.dat
50 90
2_165_50_16_csi_a16_3.dat
50 92
50 93
2_165_50_16_csi_a16_27.dat
50 95
50 96
50 97
50 98
50 99
50 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
70 105
1_175_70_16_csi_a16_23.dat
70 107
70 108
70 109
1_175_70_16_csi_a16_14.dat
70 111
70 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
70 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
70 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
70 123
70 124
1_175_70_16_csi_a16_5.dat
70 126
1_175_70_16_csi_a16_18.dat
70 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
85 132
1_180_85_16_csi_a16_7.dat
85 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
85 139
85 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
85 148
1_180_85_16_csi_a16_17.dat
85 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
85 155
85 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
75 161
75 162
75 163
1_180_75_16_csi_a16_18.dat
75 165
75 166
1_180_75_16_csi_a16_16.dat
75 168
1_180_75_16_csi_a16_9.dat
75 170
75 171
1_180_75_16_csi_a16_17.dat
75 173
75 174
1_180_75_16_csi_a16_13.dat
75 176
1_180_75_16_csi_a16_10.dat
75 178
75 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
75 183
1_180_75_16_csi_a16_5.dat
75 185
75 186
75 187
75 188
75 189
75 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
85 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
85 197
85 198
85 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
85 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
85 205
85 206
85 207
85 208
85 209
85 210
1_173_85_16_csi_a16_29.dat
85 212
1_173_85_16_csi_a16_17.dat
85 214
85 215
1_173_85_16_csi_a16_22.dat
85 217
1_173_85_16_csi_a16_26.dat
85 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70
 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75
 75 75 75 75 75 75 75 75 75 75 75 75 75 85 85 85 85 85 85 85 85 85 85 85
 85 85 85 85 85]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.3812 - mae: 0.5139 - mse: 0.3812
64/72 [=========================>....] - ETA: 0s - loss: 0.3003 - mae: 0.4628 - mse: 0.3003
72/72 [==============================] - 1s 10ms/step - loss: 0.2832 - mae: 0.4506 - mse: 0.2832 - val_loss: 0.1324 - val_mae: 0.3348 - val_mse: 0.1324
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1642 - mae: 0.3456 - mse: 0.1642
64/72 [=========================>....] - ETA: 0s - loss: 0.1514 - mae: 0.3143 - mse: 0.1514
72/72 [==============================] - 0s 6ms/step - loss: 0.1488 - mae: 0.3178 - mse: 0.1488 - val_loss: 0.1115 - val_mae: 0.3092 - val_mse: 0.1115
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1170 - mae: 0.3014 - mse: 0.1170
64/72 [=========================>....] - ETA: 0s - loss: 0.1052 - mae: 0.2803 - mse: 0.1052
72/72 [==============================] - 0s 6ms/step - loss: 0.1044 - mae: 0.2817 - mse: 0.1044 - val_loss: 0.1189 - val_mae: 0.2813 - val_mse: 0.1189
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.0936 - mae: 0.2676 - mse: 0.0936
64/72 [=========================>....] - ETA: 0s - loss: 0.1067 - mae: 0.2814 - mse: 0.1067
72/72 [==============================] - 0s 6ms/step - loss: 0.1022 - mae: 0.2752 - mse: 0.1022 - val_loss: 0.1225 - val_mae: 0.2700 - val_mse: 0.1225
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.1119 - mae: 0.2910 - mse: 0.1119
64/72 [=========================>....] - ETA: 0s - loss: 0.0965 - mae: 0.2732 - mse: 0.0965
72/72 [==============================] - 0s 6ms/step - loss: 0.0885 - mae: 0.2587 - mse: 0.0885 - val_loss: 0.0750 - val_mae: 0.2365 - val_mse: 0.0750
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0727 - mae: 0.2310 - mse: 0.0727
64/72 [=========================>....] - ETA: 0s - loss: 0.0811 - mae: 0.2503 - mse: 0.0811
72/72 [==============================] - 0s 6ms/step - loss: 0.0781 - mae: 0.2461 - mse: 0.0781 - val_loss: 0.0519 - val_mae: 0.2090 - val_mse: 0.0519
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0748 - mae: 0.2382 - mse: 0.0748
64/72 [=========================>....] - ETA: 0s - loss: 0.0652 - mae: 0.2144 - mse: 0.0652
72/72 [==============================] - 0s 6ms/step - loss: 0.0667 - mae: 0.2096 - mse: 0.0667 - val_loss: 0.0504 - val_mae: 0.1866 - val_mse: 0.0504
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0410 - mae: 0.1653 - mse: 0.0410
64/72 [=========================>....] - ETA: 0s - loss: 0.0501 - mae: 0.1822 - mse: 0.0501
72/72 [==============================] - 1s 12ms/step - loss: 0.0507 - mae: 0.1817 - mse: 0.0507 - val_loss: 0.0595 - val_mae: 0.2049 - val_mse: 0.0595
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0529 - mae: 0.1774 - mse: 0.0529
64/72 [=========================>....] - ETA: 0s - loss: 0.0463 - mae: 0.1660 - mse: 0.0463
72/72 [==============================] - 1s 15ms/step - loss: 0.0479 - mae: 0.1705 - mse: 0.0479 - val_loss: 0.0548 - val_mae: 0.1990 - val_mse: 0.0548
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0343 - mae: 0.1432 - mse: 0.0343
64/72 [=========================>....] - ETA: 0s - loss: 0.0348 - mae: 0.1520 - mse: 0.0348
72/72 [==============================] - 1s 9ms/step - loss: 0.0342 - mae: 0.1518 - mse: 0.0342 - val_loss: 0.0408 - val_mae: 0.1699 - val_mse: 0.0408
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0323 - mae: 0.1387 - mse: 0.0323
64/72 [=========================>....] - ETA: 0s - loss: 0.0389 - mae: 0.1664 - mse: 0.0389
72/72 [==============================] - 0s 7ms/step - loss: 0.0379 - mae: 0.1645 - mse: 0.0379 - val_loss: 0.0382 - val_mae: 0.1687 - val_mse: 0.0382
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0488 - mae: 0.1747 - mse: 0.0488
64/72 [=========================>....] - ETA: 0s - loss: 0.0517 - mae: 0.1830 - mse: 0.0517
72/72 [==============================] - 0s 6ms/step - loss: 0.0470 - mae: 0.1712 - mse: 0.0470 - val_loss: 0.0394 - val_mae: 0.1709 - val_mse: 0.0394
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0401 - mae: 0.1679 - mse: 0.0401
64/72 [=========================>....] - ETA: 0s - loss: 0.0394 - mae: 0.1649 - mse: 0.0394
72/72 [==============================] - 0s 7ms/step - loss: 0.0384 - mae: 0.1608 - mse: 0.0384 - val_loss: 0.0651 - val_mae: 0.2057 - val_mse: 0.0651
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0325 - mae: 0.1561 - mse: 0.0325
64/72 [=========================>....] - ETA: 0s - loss: 0.0313 - mae: 0.1498 - mse: 0.0313
72/72 [==============================] - 0s 7ms/step - loss: 0.0296 - mae: 0.1441 - mse: 0.0296 - val_loss: 0.0544 - val_mae: 0.1947 - val_mse: 0.0544
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0226 - mae: 0.1147 - mse: 0.0226
64/72 [=========================>....] - ETA: 0s - loss: 0.0248 - mae: 0.1222 - mse: 0.0248
72/72 [==============================] - 1s 7ms/step - loss: 0.0247 - mae: 0.1242 - mse: 0.0247 - val_loss: 0.0313 - val_mae: 0.1519 - val_mse: 0.0313
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0268 - mae: 0.1259 - mse: 0.0268
64/72 [=========================>....] - ETA: 0s - loss: 0.0286 - mae: 0.1331 - mse: 0.0286
72/72 [==============================] - 0s 7ms/step - loss: 0.0275 - mae: 0.1281 - mse: 0.0275 - val_loss: 0.0380 - val_mae: 0.1675 - val_mse: 0.0380
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0189 - mae: 0.1187 - mse: 0.0189
64/72 [=========================>....] - ETA: 0s - loss: 0.0222 - mae: 0.1251 - mse: 0.0222
72/72 [==============================] - 0s 6ms/step - loss: 0.0227 - mae: 0.1249 - mse: 0.0227 - val_loss: 0.0565 - val_mae: 0.1894 - val_mse: 0.0565
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0120 - mae: 0.0896 - mse: 0.0120
64/72 [=========================>....] - ETA: 0s - loss: 0.0208 - mae: 0.1156 - mse: 0.0208
72/72 [==============================] - 0s 7ms/step - loss: 0.0244 - mae: 0.1238 - mse: 0.0244 - val_loss: 0.0485 - val_mae: 0.1794 - val_mse: 0.0485
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0192 - mae: 0.1074 - mse: 0.0192
64/72 [=========================>....] - ETA: 0s - loss: 0.0218 - mae: 0.1115 - mse: 0.0218
72/72 [==============================] - 0s 7ms/step - loss: 0.0207 - mae: 0.1085 - mse: 0.0207 - val_loss: 0.0371 - val_mae: 0.1618 - val_mse: 0.0371
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0136 - mae: 0.0965 - mse: 0.0136
64/72 [=========================>....] - ETA: 0s - loss: 0.0215 - mae: 0.1146 - mse: 0.0215
72/72 [==============================] - 0s 7ms/step - loss: 0.0200 - mae: 0.1107 - mse: 0.0200 - val_loss: 0.0463 - val_mae: 0.1712 - val_mse: 0.0463
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0176 - mae: 0.1020 - mse: 0.0176
64/72 [=========================>....] - ETA: 0s - loss: 0.0145 - mae: 0.0927 - mse: 0.0145
72/72 [==============================] - 1s 7ms/step - loss: 0.0157 - mae: 0.0989 - mse: 0.0157 - val_loss: 0.0518 - val_mae: 0.1792 - val_mse: 0.0518
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0229 - mae: 0.1149 - mse: 0.0229
64/72 [=========================>....] - ETA: 0s - loss: 0.0217 - mae: 0.1189 - mse: 0.0217
72/72 [==============================] - 0s 7ms/step - loss: 0.0210 - mae: 0.1172 - mse: 0.0210 - val_loss: 0.0446 - val_mae: 0.1694 - val_mse: 0.0446
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0159 - mae: 0.0982 - mse: 0.0159
64/72 [=========================>....] - ETA: 0s - loss: 0.0170 - mae: 0.1012 - mse: 0.0170
72/72 [==============================] - 0s 7ms/step - loss: 0.0175 - mae: 0.1029 - mse: 0.0175 - val_loss: 0.0350 - val_mae: 0.1529 - val_mse: 0.0350
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0176 - mae: 0.1014 - mse: 0.0176
64/72 [=========================>....] - ETA: 0s - loss: 0.0207 - mae: 0.1087 - mse: 0.0207
72/72 [==============================] - 0s 6ms/step - loss: 0.0232 - mae: 0.1159 - mse: 0.0232 - val_loss: 0.0353 - val_mae: 0.1497 - val_mse: 0.0353
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0119 - mae: 0.0924 - mse: 0.0119
64/72 [=========================>....] - ETA: 0s - loss: 0.0171 - mae: 0.1044 - mse: 0.0171
72/72 [==============================] - 0s 7ms/step - loss: 0.0168 - mae: 0.1048 - mse: 0.0168 - val_loss: 0.0327 - val_mae: 0.1459 - val_mse: 0.0327
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0113 - mae: 0.0872 - mse: 0.0113
64/72 [=========================>....] - ETA: 0s - loss: 0.0120 - mae: 0.0879 - mse: 0.0120
72/72 [==============================] - 0s 7ms/step - loss: 0.0133 - mae: 0.0918 - mse: 0.0133 - val_loss: 0.0367 - val_mae: 0.1527 - val_mse: 0.0367
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0130 - mae: 0.0878 - mse: 0.0130
64/72 [=========================>....] - ETA: 0s - loss: 0.0160 - mae: 0.0952 - mse: 0.0160
72/72 [==============================] - 0s 6ms/step - loss: 0.0161 - mae: 0.0971 - mse: 0.0161 - val_loss: 0.0486 - val_mae: 0.1752 - val_mse: 0.0486
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0281 - mae: 0.1234 - mse: 0.0281
64/72 [=========================>....] - ETA: 0s - loss: 0.0211 - mae: 0.1076 - mse: 0.0211
72/72 [==============================] - 0s 7ms/step - loss: 0.0203 - mae: 0.1063 - mse: 0.0203 - val_loss: 0.0357 - val_mae: 0.1537 - val_mse: 0.0357
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0146 - mae: 0.1006 - mse: 0.0146
64/72 [=========================>....] - ETA: 0s - loss: 0.0121 - mae: 0.0891 - mse: 0.0121
72/72 [==============================] - 0s 6ms/step - loss: 0.0129 - mae: 0.0921 - mse: 0.0129 - val_loss: 0.0337 - val_mae: 0.1487 - val_mse: 0.0337
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0121 - mae: 0.0797 - mse: 0.0121
64/72 [=========================>....] - ETA: 0s - loss: 0.0157 - mae: 0.0921 - mse: 0.0157
72/72 [==============================] - 0s 7ms/step - loss: 0.0167 - mae: 0.0957 - mse: 0.0167 - val_loss: 0.0354 - val_mae: 0.1498 - val_mse: 0.0354
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         8.26443481 0.         0.        ]
average prediction= [4.333083]
baseline= 13.928571428571429
eachuser= [0. 0. 0. 8. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.0330543518066406
85 -:- nan
60 -:- nan
['train-weight-16.py', '1']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
65 5
65 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
65 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
65 16
65 17
2_155_65_16_csi_a16_21.dat
65 19
65 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
65 25
2_155_65_16_csi_a16_10.dat
65 27
65 28
2_155_65_16_csi_a16_19.dat
65 30
60 31
2_170_60_16_csi_a16_3.dat
60 33
60 34
60 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
60 38
60 39
60 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_16_csi_a16_19.dat
50 90
2_165_50_16_csi_a16_3.dat
50 92
50 93
2_165_50_16_csi_a16_27.dat
50 95
50 96
50 97
50 98
50 99
50 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
70 105
1_175_70_16_csi_a16_23.dat
70 107
70 108
70 109
1_175_70_16_csi_a16_14.dat
70 111
70 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
70 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
70 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
70 123
70 124
1_175_70_16_csi_a16_5.dat
70 126
1_175_70_16_csi_a16_18.dat
70 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
85 132
1_180_85_16_csi_a16_7.dat
85 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
85 139
85 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
85 148
1_180_85_16_csi_a16_17.dat
85 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
85 155
85 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
75 161
75 162
75 163
1_180_75_16_csi_a16_18.dat
75 165
75 166
1_180_75_16_csi_a16_16.dat
75 168
1_180_75_16_csi_a16_9.dat
75 170
75 171
1_180_75_16_csi_a16_17.dat
75 173
75 174
1_180_75_16_csi_a16_13.dat
75 176
1_180_75_16_csi_a16_10.dat
75 178
75 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
75 183
1_180_75_16_csi_a16_5.dat
75 185
75 186
75 187
75 188
75 189
75 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
85 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
85 197
85 198
85 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
85 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
85 205
85 206
85 207
85 208
85 209
85 210
1_173_85_16_csi_a16_29.dat
85 212
1_173_85_16_csi_a16_17.dat
85 214
85 215
1_173_85_16_csi_a16_22.dat
85 217
1_173_85_16_csi_a16_26.dat
85 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70
 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75
 75 75 75 75 75 75 75 75 75 75 75 75 75 85 85 85 85 85 85 85 85 85 85 85
 85 85 85 85 85]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.3093 - mae: 0.4590 - mse: 0.3093
64/72 [=========================>....] - ETA: 0s - loss: 0.3062 - mae: 0.4702 - mse: 0.3062
72/72 [==============================] - 1s 11ms/step - loss: 0.3053 - mae: 0.4730 - mse: 0.3053 - val_loss: 0.0858 - val_mae: 0.2791 - val_mse: 0.0858
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1469 - mae: 0.3058 - mse: 0.1469
64/72 [=========================>....] - ETA: 0s - loss: 0.1640 - mae: 0.3384 - mse: 0.1640
72/72 [==============================] - 0s 7ms/step - loss: 0.1792 - mae: 0.3598 - mse: 0.1792 - val_loss: 0.1041 - val_mae: 0.2482 - val_mse: 0.1041
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1312 - mae: 0.2965 - mse: 0.1312
64/72 [=========================>....] - ETA: 0s - loss: 0.1447 - mae: 0.3223 - mse: 0.1447
72/72 [==============================] - 0s 7ms/step - loss: 0.1324 - mae: 0.3052 - mse: 0.1324 - val_loss: 0.0381 - val_mae: 0.1723 - val_mse: 0.0381
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.0671 - mae: 0.2275 - mse: 0.0671
64/72 [=========================>....] - ETA: 0s - loss: 0.0651 - mae: 0.2209 - mse: 0.0651
72/72 [==============================] - 0s 7ms/step - loss: 0.0716 - mae: 0.2270 - mse: 0.0716 - val_loss: 0.0182 - val_mae: 0.1239 - val_mse: 0.0182
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.0420 - mae: 0.1705 - mse: 0.0420
64/72 [=========================>....] - ETA: 0s - loss: 0.0412 - mae: 0.1707 - mse: 0.0412
72/72 [==============================] - 0s 7ms/step - loss: 0.0402 - mae: 0.1694 - mse: 0.0402 - val_loss: 0.0163 - val_mae: 0.0909 - val_mse: 0.0163
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0492 - mae: 0.1922 - mse: 0.0492
64/72 [=========================>....] - ETA: 0s - loss: 0.0565 - mae: 0.1979 - mse: 0.0565
72/72 [==============================] - 0s 7ms/step - loss: 0.0550 - mae: 0.1942 - mse: 0.0550 - val_loss: 0.0175 - val_mae: 0.0991 - val_mse: 0.0175
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0310 - mae: 0.1385 - mse: 0.0310
64/72 [=========================>....] - ETA: 0s - loss: 0.0409 - mae: 0.1538 - mse: 0.0409
72/72 [==============================] - 0s 7ms/step - loss: 0.0406 - mae: 0.1562 - mse: 0.0406 - val_loss: 0.0185 - val_mae: 0.1015 - val_mse: 0.0185
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0340 - mae: 0.1434 - mse: 0.0340
64/72 [=========================>....] - ETA: 0s - loss: 0.0337 - mae: 0.1445 - mse: 0.0337
72/72 [==============================] - 0s 6ms/step - loss: 0.0333 - mae: 0.1420 - mse: 0.0333 - val_loss: 0.0164 - val_mae: 0.0935 - val_mse: 0.0164
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0363 - mae: 0.1340 - mse: 0.0363
64/72 [=========================>....] - ETA: 0s - loss: 0.0375 - mae: 0.1416 - mse: 0.0375
72/72 [==============================] - 0s 7ms/step - loss: 0.0381 - mae: 0.1453 - mse: 0.0381 - val_loss: 0.0121 - val_mae: 0.0812 - val_mse: 0.0121
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0205 - mae: 0.1151 - mse: 0.0205
64/72 [=========================>....] - ETA: 0s - loss: 0.0227 - mae: 0.1206 - mse: 0.0227
72/72 [==============================] - 0s 7ms/step - loss: 0.0231 - mae: 0.1227 - mse: 0.0231 - val_loss: 0.0093 - val_mae: 0.0746 - val_mse: 0.0093
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0374 - mae: 0.1615 - mse: 0.0374
64/72 [=========================>....] - ETA: 0s - loss: 0.0287 - mae: 0.1342 - mse: 0.0287
72/72 [==============================] - 0s 7ms/step - loss: 0.0284 - mae: 0.1315 - mse: 0.0284 - val_loss: 0.0066 - val_mae: 0.0680 - val_mse: 0.0066
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0250 - mae: 0.1305 - mse: 0.0250
64/72 [=========================>....] - ETA: 0s - loss: 0.0240 - mae: 0.1238 - mse: 0.0240
72/72 [==============================] - 0s 7ms/step - loss: 0.0262 - mae: 0.1273 - mse: 0.0262 - val_loss: 0.0040 - val_mae: 0.0537 - val_mse: 0.0040
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0248 - mae: 0.1276 - mse: 0.0248
64/72 [=========================>....] - ETA: 0s - loss: 0.0193 - mae: 0.1102 - mse: 0.0193
72/72 [==============================] - 0s 6ms/step - loss: 0.0180 - mae: 0.1065 - mse: 0.0180 - val_loss: 0.0044 - val_mae: 0.0474 - val_mse: 0.0044
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0259 - mae: 0.1280 - mse: 0.0259
64/72 [=========================>....] - ETA: 0s - loss: 0.0219 - mae: 0.1191 - mse: 0.0219
72/72 [==============================] - 0s 6ms/step - loss: 0.0210 - mae: 0.1167 - mse: 0.0210 - val_loss: 0.0032 - val_mae: 0.0494 - val_mse: 0.0032
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0206 - mae: 0.1144 - mse: 0.0206
64/72 [=========================>....] - ETA: 0s - loss: 0.0212 - mae: 0.1120 - mse: 0.0212
72/72 [==============================] - 0s 6ms/step - loss: 0.0208 - mae: 0.1117 - mse: 0.0208 - val_loss: 0.0057 - val_mae: 0.0684 - val_mse: 0.0057
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0281 - mae: 0.1288 - mse: 0.0281
64/72 [=========================>....] - ETA: 0s - loss: 0.0230 - mae: 0.1189 - mse: 0.0230
72/72 [==============================] - 0s 7ms/step - loss: 0.0222 - mae: 0.1145 - mse: 0.0222 - val_loss: 0.0057 - val_mae: 0.0719 - val_mse: 0.0057
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0183 - mae: 0.1059 - mse: 0.0183
64/72 [=========================>....] - ETA: 0s - loss: 0.0171 - mae: 0.1036 - mse: 0.0171
72/72 [==============================] - 0s 6ms/step - loss: 0.0190 - mae: 0.1077 - mse: 0.0190 - val_loss: 0.0070 - val_mae: 0.0732 - val_mse: 0.0070
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0245 - mae: 0.1260 - mse: 0.0245
64/72 [=========================>....] - ETA: 0s - loss: 0.0229 - mae: 0.1229 - mse: 0.0229
72/72 [==============================] - 0s 6ms/step - loss: 0.0237 - mae: 0.1259 - mse: 0.0237 - val_loss: 0.0123 - val_mae: 0.0955 - val_mse: 0.0123
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0344 - mae: 0.1571 - mse: 0.0344
64/72 [=========================>....] - ETA: 0s - loss: 0.0241 - mae: 0.1252 - mse: 0.0241
72/72 [==============================] - 0s 7ms/step - loss: 0.0231 - mae: 0.1220 - mse: 0.0231 - val_loss: 0.0075 - val_mae: 0.0761 - val_mse: 0.0075
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0215 - mae: 0.1190 - mse: 0.0215
64/72 [=========================>....] - ETA: 0s - loss: 0.0179 - mae: 0.1030 - mse: 0.0179
72/72 [==============================] - 0s 6ms/step - loss: 0.0183 - mae: 0.1036 - mse: 0.0183 - val_loss: 0.0046 - val_mae: 0.0542 - val_mse: 0.0046
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0213 - mae: 0.1160 - mse: 0.0213
64/72 [=========================>....] - ETA: 0s - loss: 0.0175 - mae: 0.1052 - mse: 0.0175
72/72 [==============================] - 0s 7ms/step - loss: 0.0161 - mae: 0.0992 - mse: 0.0161 - val_loss: 0.0044 - val_mae: 0.0540 - val_mse: 0.0044
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0118 - mae: 0.0761 - mse: 0.0118
64/72 [=========================>....] - ETA: 0s - loss: 0.0097 - mae: 0.0734 - mse: 0.0097
72/72 [==============================] - 0s 6ms/step - loss: 0.0099 - mae: 0.0740 - mse: 0.0099 - val_loss: 0.0043 - val_mae: 0.0503 - val_mse: 0.0043
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0085 - mae: 0.0766 - mse: 0.0085
64/72 [=========================>....] - ETA: 0s - loss: 0.0129 - mae: 0.0876 - mse: 0.0129
72/72 [==============================] - 0s 7ms/step - loss: 0.0124 - mae: 0.0868 - mse: 0.0124 - val_loss: 0.0050 - val_mae: 0.0530 - val_mse: 0.0050
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0216 - mae: 0.1064 - mse: 0.0216
64/72 [=========================>....] - ETA: 0s - loss: 0.0171 - mae: 0.0977 - mse: 0.0171
72/72 [==============================] - 0s 7ms/step - loss: 0.0164 - mae: 0.0973 - mse: 0.0164 - val_loss: 0.0058 - val_mae: 0.0644 - val_mse: 0.0058
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0088 - mae: 0.0692 - mse: 0.0088
64/72 [=========================>....] - ETA: 0s - loss: 0.0113 - mae: 0.0802 - mse: 0.0113
72/72 [==============================] - 0s 7ms/step - loss: 0.0110 - mae: 0.0801 - mse: 0.0110 - val_loss: 0.0069 - val_mae: 0.0779 - val_mse: 0.0069
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0177 - mae: 0.1138 - mse: 0.0177
64/72 [=========================>....] - ETA: 0s - loss: 0.0140 - mae: 0.0963 - mse: 0.0140
72/72 [==============================] - 0s 7ms/step - loss: 0.0150 - mae: 0.0981 - mse: 0.0150 - val_loss: 0.0046 - val_mae: 0.0537 - val_mse: 0.0046
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0079 - mae: 0.0722 - mse: 0.0079
64/72 [=========================>....] - ETA: 0s - loss: 0.0128 - mae: 0.0894 - mse: 0.0128
72/72 [==============================] - 0s 6ms/step - loss: 0.0126 - mae: 0.0882 - mse: 0.0126 - val_loss: 0.0046 - val_mae: 0.0514 - val_mse: 0.0046
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0123 - mae: 0.0873 - mse: 0.0123
64/72 [=========================>....] - ETA: 0s - loss: 0.0129 - mae: 0.0889 - mse: 0.0129
72/72 [==============================] - 0s 7ms/step - loss: 0.0132 - mae: 0.0900 - mse: 0.0132 - val_loss: 0.0071 - val_mae: 0.0525 - val_mse: 0.0071
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0163 - mae: 0.0943 - mse: 0.0163
64/72 [=========================>....] - ETA: 0s - loss: 0.0153 - mae: 0.0955 - mse: 0.0153
72/72 [==============================] - 0s 7ms/step - loss: 0.0153 - mae: 0.0955 - mse: 0.0153 - val_loss: 0.0065 - val_mae: 0.0537 - val_mse: 0.0065
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0133 - mae: 0.0889 - mse: 0.0133
64/72 [=========================>....] - ETA: 0s - loss: 0.0127 - mae: 0.0904 - mse: 0.0127
72/72 [==============================] - 0s 7ms/step - loss: 0.0135 - mae: 0.0914 - mse: 0.0135 - val_loss: 0.0085 - val_mae: 0.0740 - val_mse: 0.0085
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         9.21375656 0.         0.        ]
average prediction= [2.812222]
baseline= 13.214285714285714
eachuser= [0. 0. 0. 7. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.316250937325614
85 -:- nan
60 -:- nan
['train-weight-16.py', '1']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
65 5
65 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
65 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
65 16
65 17
2_155_65_16_csi_a16_21.dat
65 19
65 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
65 25
2_155_65_16_csi_a16_10.dat
65 27
65 28
2_155_65_16_csi_a16_19.dat
65 30
60 31
2_170_60_16_csi_a16_3.dat
60 33
60 34
60 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
60 38
60 39
60 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_16_csi_a16_19.dat
50 90
2_165_50_16_csi_a16_3.dat
50 92
50 93
2_165_50_16_csi_a16_27.dat
50 95
50 96
50 97
50 98
50 99
50 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
70 105
1_175_70_16_csi_a16_23.dat
70 107
70 108
70 109
1_175_70_16_csi_a16_14.dat
70 111
70 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
70 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
70 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
70 123
70 124
1_175_70_16_csi_a16_5.dat
70 126
1_175_70_16_csi_a16_18.dat
70 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
85 132
1_180_85_16_csi_a16_7.dat
85 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
85 139
85 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
85 148
1_180_85_16_csi_a16_17.dat
85 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
85 155
85 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
75 161
75 162
75 163
1_180_75_16_csi_a16_18.dat
75 165
75 166
1_180_75_16_csi_a16_16.dat
75 168
1_180_75_16_csi_a16_9.dat
75 170
75 171
1_180_75_16_csi_a16_17.dat
75 173
75 174
1_180_75_16_csi_a16_13.dat
75 176
1_180_75_16_csi_a16_10.dat
75 178
75 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
75 183
1_180_75_16_csi_a16_5.dat
75 185
75 186
75 187
75 188
75 189
75 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
85 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
85 197
85 198
85 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
85 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
85 205
85 206
85 207
85 208
85 209
85 210
1_173_85_16_csi_a16_29.dat
85 212
1_173_85_16_csi_a16_17.dat
85 214
85 215
1_173_85_16_csi_a16_22.dat
85 217
1_173_85_16_csi_a16_26.dat
85 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70
 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75
 75 75 75 75 75 75 75 75 75 75 75 75 75 85 85 85 85 85 85 85 85 85 85 85
 85 85 85 85 85]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.4627 - mae: 0.5742 - mse: 0.4627
64/72 [=========================>....] - ETA: 0s - loss: 0.3625 - mae: 0.4902 - mse: 0.3625
72/72 [==============================] - 1s 11ms/step - loss: 0.3477 - mae: 0.4821 - mse: 0.3477 - val_loss: 0.0546 - val_mae: 0.1931 - val_mse: 0.0546
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1516 - mae: 0.3282 - mse: 0.1516
64/72 [=========================>....] - ETA: 0s - loss: 0.1467 - mae: 0.3388 - mse: 0.1467
72/72 [==============================] - 0s 6ms/step - loss: 0.1437 - mae: 0.3335 - mse: 0.1437 - val_loss: 0.0824 - val_mae: 0.2534 - val_mse: 0.0824
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1042 - mae: 0.2624 - mse: 0.1042
64/72 [=========================>....] - ETA: 0s - loss: 0.1077 - mae: 0.2774 - mse: 0.1077
72/72 [==============================] - 0s 7ms/step - loss: 0.1010 - mae: 0.2666 - mse: 0.1010 - val_loss: 0.0450 - val_mae: 0.1836 - val_mse: 0.0450
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.0730 - mae: 0.2260 - mse: 0.0730
64/72 [=========================>....] - ETA: 0s - loss: 0.0659 - mae: 0.2224 - mse: 0.0659
72/72 [==============================] - 0s 6ms/step - loss: 0.0666 - mae: 0.2247 - mse: 0.0666 - val_loss: 0.0279 - val_mae: 0.1406 - val_mse: 0.0279
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.0451 - mae: 0.1867 - mse: 0.0451
64/72 [=========================>....] - ETA: 0s - loss: 0.0610 - mae: 0.2226 - mse: 0.0610
72/72 [==============================] - 0s 7ms/step - loss: 0.0637 - mae: 0.2268 - mse: 0.0637 - val_loss: 0.0304 - val_mae: 0.1388 - val_mse: 0.0304
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0888 - mae: 0.2523 - mse: 0.0888
64/72 [=========================>....] - ETA: 0s - loss: 0.0797 - mae: 0.2380 - mse: 0.0797
72/72 [==============================] - 0s 7ms/step - loss: 0.0728 - mae: 0.2234 - mse: 0.0728 - val_loss: 0.0279 - val_mae: 0.1311 - val_mse: 0.0279
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0324 - mae: 0.1468 - mse: 0.0324
64/72 [=========================>....] - ETA: 0s - loss: 0.0377 - mae: 0.1584 - mse: 0.0377
72/72 [==============================] - 0s 6ms/step - loss: 0.0405 - mae: 0.1628 - mse: 0.0405 - val_loss: 0.0292 - val_mae: 0.1246 - val_mse: 0.0292
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0324 - mae: 0.1451 - mse: 0.0324
64/72 [=========================>....] - ETA: 0s - loss: 0.0494 - mae: 0.1748 - mse: 0.0494
72/72 [==============================] - 0s 7ms/step - loss: 0.0479 - mae: 0.1742 - mse: 0.0479 - val_loss: 0.0255 - val_mae: 0.1259 - val_mse: 0.0255
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0399 - mae: 0.1683 - mse: 0.0399
64/72 [=========================>....] - ETA: 0s - loss: 0.0414 - mae: 0.1681 - mse: 0.0414
72/72 [==============================] - 0s 7ms/step - loss: 0.0473 - mae: 0.1761 - mse: 0.0473 - val_loss: 0.0226 - val_mae: 0.1281 - val_mse: 0.0226
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0239 - mae: 0.1286 - mse: 0.0239
64/72 [=========================>....] - ETA: 0s - loss: 0.0262 - mae: 0.1321 - mse: 0.0262
72/72 [==============================] - 0s 6ms/step - loss: 0.0281 - mae: 0.1373 - mse: 0.0281 - val_loss: 0.0192 - val_mae: 0.1201 - val_mse: 0.0192
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0472 - mae: 0.1785 - mse: 0.0472
64/72 [=========================>....] - ETA: 0s - loss: 0.0451 - mae: 0.1744 - mse: 0.0451
72/72 [==============================] - 0s 7ms/step - loss: 0.0453 - mae: 0.1747 - mse: 0.0453 - val_loss: 0.0154 - val_mae: 0.1046 - val_mse: 0.0154
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0313 - mae: 0.1476 - mse: 0.0313
64/72 [=========================>....] - ETA: 0s - loss: 0.0286 - mae: 0.1398 - mse: 0.0286
72/72 [==============================] - 0s 7ms/step - loss: 0.0280 - mae: 0.1401 - mse: 0.0280 - val_loss: 0.0136 - val_mae: 0.0993 - val_mse: 0.0136
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0323 - mae: 0.1438 - mse: 0.0323
64/72 [=========================>....] - ETA: 0s - loss: 0.0338 - mae: 0.1435 - mse: 0.0338
72/72 [==============================] - 0s 6ms/step - loss: 0.0354 - mae: 0.1485 - mse: 0.0354 - val_loss: 0.0120 - val_mae: 0.0914 - val_mse: 0.0120
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0226 - mae: 0.1237 - mse: 0.0226
64/72 [=========================>....] - ETA: 0s - loss: 0.0264 - mae: 0.1322 - mse: 0.0264
72/72 [==============================] - 0s 7ms/step - loss: 0.0256 - mae: 0.1296 - mse: 0.0256 - val_loss: 0.0099 - val_mae: 0.0831 - val_mse: 0.0099
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0283 - mae: 0.1280 - mse: 0.0283
64/72 [=========================>....] - ETA: 0s - loss: 0.0306 - mae: 0.1307 - mse: 0.0306
72/72 [==============================] - 0s 7ms/step - loss: 0.0311 - mae: 0.1333 - mse: 0.0311 - val_loss: 0.0096 - val_mae: 0.0844 - val_mse: 0.0096
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0235 - mae: 0.1216 - mse: 0.0235
64/72 [=========================>....] - ETA: 0s - loss: 0.0203 - mae: 0.1124 - mse: 0.0203
72/72 [==============================] - 0s 7ms/step - loss: 0.0194 - mae: 0.1096 - mse: 0.0194 - val_loss: 0.0107 - val_mae: 0.0880 - val_mse: 0.0107
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0226 - mae: 0.1207 - mse: 0.0226
64/72 [=========================>....] - ETA: 0s - loss: 0.0232 - mae: 0.1161 - mse: 0.0232
72/72 [==============================] - 0s 7ms/step - loss: 0.0237 - mae: 0.1173 - mse: 0.0237 - val_loss: 0.0136 - val_mae: 0.0988 - val_mse: 0.0136
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0302 - mae: 0.1355 - mse: 0.0302
64/72 [=========================>....] - ETA: 0s - loss: 0.0247 - mae: 0.1219 - mse: 0.0247
72/72 [==============================] - 0s 6ms/step - loss: 0.0223 - mae: 0.1134 - mse: 0.0223 - val_loss: 0.0174 - val_mae: 0.1137 - val_mse: 0.0174
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0194 - mae: 0.0891 - mse: 0.0194
64/72 [=========================>....] - ETA: 0s - loss: 0.0214 - mae: 0.1012 - mse: 0.0214
72/72 [==============================] - 0s 7ms/step - loss: 0.0201 - mae: 0.0994 - mse: 0.0201 - val_loss: 0.0206 - val_mae: 0.1214 - val_mse: 0.0206
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0245 - mae: 0.1121 - mse: 0.0245
64/72 [=========================>....] - ETA: 0s - loss: 0.0240 - mae: 0.1135 - mse: 0.0240
72/72 [==============================] - 0s 7ms/step - loss: 0.0249 - mae: 0.1171 - mse: 0.0249 - val_loss: 0.0145 - val_mae: 0.1072 - val_mse: 0.0145
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0170 - mae: 0.0837 - mse: 0.0170
64/72 [=========================>....] - ETA: 0s - loss: 0.0149 - mae: 0.0848 - mse: 0.0149
72/72 [==============================] - 0s 7ms/step - loss: 0.0159 - mae: 0.0878 - mse: 0.0159 - val_loss: 0.0115 - val_mae: 0.0857 - val_mse: 0.0115
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0310 - mae: 0.1226 - mse: 0.0310
64/72 [=========================>....] - ETA: 0s - loss: 0.0321 - mae: 0.1321 - mse: 0.0321
72/72 [==============================] - 0s 6ms/step - loss: 0.0303 - mae: 0.1281 - mse: 0.0303 - val_loss: 0.0158 - val_mae: 0.1094 - val_mse: 0.0158
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0176 - mae: 0.1002 - mse: 0.0176
64/72 [=========================>....] - ETA: 0s - loss: 0.0148 - mae: 0.0920 - mse: 0.0148
72/72 [==============================] - 0s 6ms/step - loss: 0.0146 - mae: 0.0931 - mse: 0.0146 - val_loss: 0.0193 - val_mae: 0.1166 - val_mse: 0.0193
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0217 - mae: 0.1061 - mse: 0.0217
64/72 [=========================>....] - ETA: 0s - loss: 0.0176 - mae: 0.0961 - mse: 0.0176
72/72 [==============================] - 0s 7ms/step - loss: 0.0175 - mae: 0.0973 - mse: 0.0175 - val_loss: 0.0125 - val_mae: 0.0952 - val_mse: 0.0125
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0180 - mae: 0.0967 - mse: 0.0180
64/72 [=========================>....] - ETA: 0s - loss: 0.0232 - mae: 0.1090 - mse: 0.0232
72/72 [==============================] - 0s 7ms/step - loss: 0.0208 - mae: 0.0999 - mse: 0.0208 - val_loss: 0.0084 - val_mae: 0.0858 - val_mse: 0.0084
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0152 - mae: 0.0892 - mse: 0.0152
64/72 [=========================>....] - ETA: 0s - loss: 0.0196 - mae: 0.0995 - mse: 0.0196
72/72 [==============================] - 0s 7ms/step - loss: 0.0195 - mae: 0.1021 - mse: 0.0195 - val_loss: 0.0087 - val_mae: 0.0814 - val_mse: 0.0087
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0122 - mae: 0.0869 - mse: 0.0122
64/72 [=========================>....] - ETA: 0s - loss: 0.0204 - mae: 0.1082 - mse: 0.0204
72/72 [==============================] - 0s 6ms/step - loss: 0.0204 - mae: 0.1104 - mse: 0.0204 - val_loss: 0.0108 - val_mae: 0.0901 - val_mse: 0.0108
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0095 - mae: 0.0741 - mse: 0.0095
64/72 [=========================>....] - ETA: 0s - loss: 0.0134 - mae: 0.0894 - mse: 0.0134
72/72 [==============================] - 0s 7ms/step - loss: 0.0122 - mae: 0.0841 - mse: 0.0122 - val_loss: 0.0126 - val_mae: 0.1004 - val_mse: 0.0126
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0132 - mae: 0.0797 - mse: 0.0132
64/72 [=========================>....] - ETA: 0s - loss: 0.0127 - mae: 0.0854 - mse: 0.0127
72/72 [==============================] - 0s 6ms/step - loss: 0.0138 - mae: 0.0873 - mse: 0.0138 - val_loss: 0.0113 - val_mae: 0.0960 - val_mse: 0.0113
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0130 - mae: 0.0839 - mse: 0.0130
64/72 [=========================>....] - ETA: 0s - loss: 0.0141 - mae: 0.0876 - mse: 0.0141
72/72 [==============================] - 0s 6ms/step - loss: 0.0136 - mae: 0.0862 - mse: 0.0136 - val_loss: 0.0105 - val_mae: 0.0897 - val_mse: 0.0105
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         3.66833878 0.         0.        ]
average prediction= [3.5612094]
baseline= 10.119047619047619
eachuser= [0. 0. 0. 3. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.2227795918782551
85 -:- nan
60 -:- nan
['train-weight-16.py', '1']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
65 5
65 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
65 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
65 16
65 17
2_155_65_16_csi_a16_21.dat
65 19
65 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
65 25
2_155_65_16_csi_a16_10.dat
65 27
65 28
2_155_65_16_csi_a16_19.dat
65 30
60 31
2_170_60_16_csi_a16_3.dat
60 33
60 34
60 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
60 38
60 39
60 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_16_csi_a16_19.dat
50 90
2_165_50_16_csi_a16_3.dat
50 92
50 93
2_165_50_16_csi_a16_27.dat
50 95
50 96
50 97
50 98
50 99
50 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
70 105
1_175_70_16_csi_a16_23.dat
70 107
70 108
70 109
1_175_70_16_csi_a16_14.dat
70 111
70 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
70 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
70 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
70 123
70 124
1_175_70_16_csi_a16_5.dat
70 126
1_175_70_16_csi_a16_18.dat
70 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
85 132
1_180_85_16_csi_a16_7.dat
85 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
85 139
85 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
85 148
1_180_85_16_csi_a16_17.dat
85 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
85 155
85 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
75 161
75 162
75 163
1_180_75_16_csi_a16_18.dat
75 165
75 166
1_180_75_16_csi_a16_16.dat
75 168
1_180_75_16_csi_a16_9.dat
75 170
75 171
1_180_75_16_csi_a16_17.dat
75 173
75 174
1_180_75_16_csi_a16_13.dat
75 176
1_180_75_16_csi_a16_10.dat
75 178
75 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
75 183
1_180_75_16_csi_a16_5.dat
75 185
75 186
75 187
75 188
75 189
75 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
85 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
85 197
85 198
85 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
85 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
85 205
85 206
85 207
85 208
85 209
85 210
1_173_85_16_csi_a16_29.dat
85 212
1_173_85_16_csi_a16_17.dat
85 214
85 215
1_173_85_16_csi_a16_22.dat
85 217
1_173_85_16_csi_a16_26.dat
85 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70
 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75
 75 75 75 75 75 75 75 75 75 75 75 75 75 85 85 85 85 85 85 85 85 85 85 85
 85 85 85 85 85]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.5385 - mae: 0.5948 - mse: 0.5385
64/72 [=========================>....] - ETA: 0s - loss: 0.4303 - mae: 0.5362 - mse: 0.4303
72/72 [==============================] - 1s 11ms/step - loss: 0.4085 - mae: 0.5195 - mse: 0.4085 - val_loss: 0.3511 - val_mae: 0.5564 - val_mse: 0.3511
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1510 - mae: 0.3381 - mse: 0.1510
64/72 [=========================>....] - ETA: 0s - loss: 0.1203 - mae: 0.3048 - mse: 0.1203
72/72 [==============================] - 0s 7ms/step - loss: 0.1195 - mae: 0.2981 - mse: 0.1195 - val_loss: 0.0680 - val_mae: 0.2246 - val_mse: 0.0680
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1278 - mae: 0.3134 - mse: 0.1278
64/72 [=========================>....] - ETA: 0s - loss: 0.1447 - mae: 0.3264 - mse: 0.1447
72/72 [==============================] - 0s 7ms/step - loss: 0.1424 - mae: 0.3244 - mse: 0.1424 - val_loss: 0.0933 - val_mae: 0.2623 - val_mse: 0.0933
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.1053 - mae: 0.2650 - mse: 0.1053
64/72 [=========================>....] - ETA: 0s - loss: 0.0996 - mae: 0.2633 - mse: 0.0996
72/72 [==============================] - 0s 7ms/step - loss: 0.0950 - mae: 0.2567 - mse: 0.0950 - val_loss: 0.1901 - val_mae: 0.3890 - val_mse: 0.1901
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.0777 - mae: 0.2482 - mse: 0.0777
64/72 [=========================>....] - ETA: 0s - loss: 0.0709 - mae: 0.2292 - mse: 0.0709
72/72 [==============================] - 0s 7ms/step - loss: 0.0734 - mae: 0.2336 - mse: 0.0734 - val_loss: 0.2167 - val_mae: 0.4222 - val_mse: 0.2167
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0598 - mae: 0.2031 - mse: 0.0598
64/72 [=========================>....] - ETA: 0s - loss: 0.0614 - mae: 0.2109 - mse: 0.0614
72/72 [==============================] - 0s 7ms/step - loss: 0.0648 - mae: 0.2169 - mse: 0.0648 - val_loss: 0.1539 - val_mae: 0.3417 - val_mse: 0.1539
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0447 - mae: 0.1864 - mse: 0.0447
64/72 [=========================>....] - ETA: 0s - loss: 0.0490 - mae: 0.1931 - mse: 0.0490
72/72 [==============================] - 0s 6ms/step - loss: 0.0496 - mae: 0.1953 - mse: 0.0496 - val_loss: 0.0795 - val_mae: 0.2352 - val_mse: 0.0795
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0387 - mae: 0.1728 - mse: 0.0387
64/72 [=========================>....] - ETA: 0s - loss: 0.0378 - mae: 0.1707 - mse: 0.0378
72/72 [==============================] - 0s 7ms/step - loss: 0.0374 - mae: 0.1693 - mse: 0.0374 - val_loss: 0.0547 - val_mae: 0.2040 - val_mse: 0.0547
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0634 - mae: 0.2223 - mse: 0.0634
64/72 [=========================>....] - ETA: 0s - loss: 0.0559 - mae: 0.2054 - mse: 0.0559
72/72 [==============================] - 0s 6ms/step - loss: 0.0511 - mae: 0.1943 - mse: 0.0511 - val_loss: 0.0795 - val_mae: 0.2319 - val_mse: 0.0795
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0447 - mae: 0.1770 - mse: 0.0447
64/72 [=========================>....] - ETA: 0s - loss: 0.0425 - mae: 0.1728 - mse: 0.0425
72/72 [==============================] - 0s 7ms/step - loss: 0.0423 - mae: 0.1726 - mse: 0.0423 - val_loss: 0.0987 - val_mae: 0.2611 - val_mse: 0.0987
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0522 - mae: 0.1941 - mse: 0.0522
64/72 [=========================>....] - ETA: 0s - loss: 0.0427 - mae: 0.1669 - mse: 0.0427
72/72 [==============================] - 0s 6ms/step - loss: 0.0393 - mae: 0.1591 - mse: 0.0393 - val_loss: 0.0869 - val_mae: 0.2420 - val_mse: 0.0869
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0288 - mae: 0.1378 - mse: 0.0288
64/72 [=========================>....] - ETA: 0s - loss: 0.0405 - mae: 0.1582 - mse: 0.0405
72/72 [==============================] - 0s 7ms/step - loss: 0.0394 - mae: 0.1542 - mse: 0.0394 - val_loss: 0.0681 - val_mae: 0.2119 - val_mse: 0.0681
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0479 - mae: 0.1725 - mse: 0.0479
64/72 [=========================>....] - ETA: 0s - loss: 0.0414 - mae: 0.1575 - mse: 0.0414
72/72 [==============================] - 0s 6ms/step - loss: 0.0457 - mae: 0.1626 - mse: 0.0457 - val_loss: 0.0606 - val_mae: 0.1989 - val_mse: 0.0606
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0197 - mae: 0.1128 - mse: 0.0197
64/72 [=========================>....] - ETA: 0s - loss: 0.0267 - mae: 0.1333 - mse: 0.0267
72/72 [==============================] - 0s 7ms/step - loss: 0.0252 - mae: 0.1284 - mse: 0.0252 - val_loss: 0.0679 - val_mae: 0.2113 - val_mse: 0.0679
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0249 - mae: 0.1217 - mse: 0.0249
64/72 [=========================>....] - ETA: 0s - loss: 0.0269 - mae: 0.1295 - mse: 0.0269
72/72 [==============================] - 0s 6ms/step - loss: 0.0251 - mae: 0.1249 - mse: 0.0251 - val_loss: 0.0648 - val_mae: 0.2079 - val_mse: 0.0648
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0165 - mae: 0.0928 - mse: 0.0165
64/72 [=========================>....] - ETA: 0s - loss: 0.0212 - mae: 0.1129 - mse: 0.0212
72/72 [==============================] - 0s 6ms/step - loss: 0.0228 - mae: 0.1179 - mse: 0.0228 - val_loss: 0.0615 - val_mae: 0.2036 - val_mse: 0.0615
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0306 - mae: 0.1343 - mse: 0.0306
64/72 [=========================>....] - ETA: 0s - loss: 0.0265 - mae: 0.1240 - mse: 0.0265
72/72 [==============================] - 0s 7ms/step - loss: 0.0259 - mae: 0.1216 - mse: 0.0259 - val_loss: 0.0649 - val_mae: 0.2111 - val_mse: 0.0649
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0258 - mae: 0.1327 - mse: 0.0258
64/72 [=========================>....] - ETA: 0s - loss: 0.0221 - mae: 0.1141 - mse: 0.0221
72/72 [==============================] - 0s 7ms/step - loss: 0.0207 - mae: 0.1101 - mse: 0.0207 - val_loss: 0.0496 - val_mae: 0.1819 - val_mse: 0.0496
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0171 - mae: 0.1017 - mse: 0.0171
64/72 [=========================>....] - ETA: 0s - loss: 0.0174 - mae: 0.1016 - mse: 0.0174
72/72 [==============================] - 0s 7ms/step - loss: 0.0188 - mae: 0.1065 - mse: 0.0188 - val_loss: 0.0474 - val_mae: 0.1772 - val_mse: 0.0474
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0161 - mae: 0.1040 - mse: 0.0161
64/72 [=========================>....] - ETA: 0s - loss: 0.0209 - mae: 0.1108 - mse: 0.0209
72/72 [==============================] - 0s 6ms/step - loss: 0.0197 - mae: 0.1084 - mse: 0.0197 - val_loss: 0.0803 - val_mae: 0.2475 - val_mse: 0.0803
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0195 - mae: 0.0976 - mse: 0.0195
64/72 [=========================>....] - ETA: 0s - loss: 0.0170 - mae: 0.0925 - mse: 0.0170
72/72 [==============================] - 0s 6ms/step - loss: 0.0162 - mae: 0.0907 - mse: 0.0162 - val_loss: 0.0654 - val_mae: 0.2135 - val_mse: 0.0654
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0177 - mae: 0.0967 - mse: 0.0177
64/72 [=========================>....] - ETA: 0s - loss: 0.0206 - mae: 0.1025 - mse: 0.0206
72/72 [==============================] - 0s 6ms/step - loss: 0.0216 - mae: 0.1043 - mse: 0.0216 - val_loss: 0.0452 - val_mae: 0.1716 - val_mse: 0.0452
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0136 - mae: 0.0772 - mse: 0.0136
64/72 [=========================>....] - ETA: 0s - loss: 0.0146 - mae: 0.0889 - mse: 0.0146
72/72 [==============================] - 0s 6ms/step - loss: 0.0150 - mae: 0.0899 - mse: 0.0150 - val_loss: 0.0617 - val_mae: 0.2053 - val_mse: 0.0617
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0268 - mae: 0.1079 - mse: 0.0268
64/72 [=========================>....] - ETA: 0s - loss: 0.0252 - mae: 0.1076 - mse: 0.0252
72/72 [==============================] - 0s 6ms/step - loss: 0.0237 - mae: 0.1048 - mse: 0.0237 - val_loss: 0.0841 - val_mae: 0.2554 - val_mse: 0.0841
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0195 - mae: 0.1072 - mse: 0.0195
64/72 [=========================>....] - ETA: 0s - loss: 0.0169 - mae: 0.0905 - mse: 0.0169
72/72 [==============================] - 0s 7ms/step - loss: 0.0156 - mae: 0.0861 - mse: 0.0156 - val_loss: 0.0611 - val_mae: 0.2039 - val_mse: 0.0611
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0171 - mae: 0.0910 - mse: 0.0171
64/72 [=========================>....] - ETA: 0s - loss: 0.0141 - mae: 0.0825 - mse: 0.0141
72/72 [==============================] - 1s 7ms/step - loss: 0.0177 - mae: 0.0936 - mse: 0.0177 - val_loss: 0.0537 - val_mae: 0.1848 - val_mse: 0.0537
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0156 - mae: 0.0823 - mse: 0.0156
64/72 [=========================>....] - ETA: 0s - loss: 0.0192 - mae: 0.0972 - mse: 0.0192
72/72 [==============================] - 0s 6ms/step - loss: 0.0178 - mae: 0.0938 - mse: 0.0178 - val_loss: 0.0620 - val_mae: 0.2071 - val_mse: 0.0620
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0135 - mae: 0.0854 - mse: 0.0135
64/72 [=========================>....] - ETA: 0s - loss: 0.0156 - mae: 0.0923 - mse: 0.0156
72/72 [==============================] - 0s 6ms/step - loss: 0.0162 - mae: 0.0928 - mse: 0.0162 - val_loss: 0.0425 - val_mae: 0.1599 - val_mse: 0.0425
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0139 - mae: 0.0822 - mse: 0.0139
64/72 [=========================>....] - ETA: 0s - loss: 0.0177 - mae: 0.0987 - mse: 0.0177
72/72 [==============================] - 0s 6ms/step - loss: 0.0162 - mae: 0.0936 - mse: 0.0162 - val_loss: 0.0311 - val_mae: 0.1408 - val_mse: 0.0311
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0365 - mae: 0.1368 - mse: 0.0365
64/72 [=========================>....] - ETA: 0s - loss: 0.0254 - mae: 0.1112 - mse: 0.0254
72/72 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1089 - mse: 0.0241 - val_loss: 0.0550 - val_mae: 0.1932 - val_mse: 0.0550
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         4.04075241 0.         0.        ]
average prediction= [4.2707543]
baseline= 10.833333333333334
eachuser= [0. 0. 0. 5. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.8081504821777343
85 -:- nan
60 -:- nan
['train-weight-16.py', '1']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
65 5
65 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
65 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
65 16
65 17
2_155_65_16_csi_a16_21.dat
65 19
65 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
65 25
2_155_65_16_csi_a16_10.dat
65 27
65 28
2_155_65_16_csi_a16_19.dat
65 30
60 31
2_170_60_16_csi_a16_3.dat
60 33
60 34
60 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
60 38
60 39
60 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_16_csi_a16_19.dat
50 90
2_165_50_16_csi_a16_3.dat
50 92
50 93
2_165_50_16_csi_a16_27.dat
50 95
50 96
50 97
50 98
50 99
50 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
70 105
1_175_70_16_csi_a16_23.dat
70 107
70 108
70 109
1_175_70_16_csi_a16_14.dat
70 111
70 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
70 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
70 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
70 123
70 124
1_175_70_16_csi_a16_5.dat
70 126
1_175_70_16_csi_a16_18.dat
70 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
85 132
1_180_85_16_csi_a16_7.dat
85 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
85 139
85 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
85 148
1_180_85_16_csi_a16_17.dat
85 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
85 155
85 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
75 161
75 162
75 163
1_180_75_16_csi_a16_18.dat
75 165
75 166
1_180_75_16_csi_a16_16.dat
75 168
1_180_75_16_csi_a16_9.dat
75 170
75 171
1_180_75_16_csi_a16_17.dat
75 173
75 174
1_180_75_16_csi_a16_13.dat
75 176
1_180_75_16_csi_a16_10.dat
75 178
75 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
75 183
1_180_75_16_csi_a16_5.dat
75 185
75 186
75 187
75 188
75 189
75 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
85 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
85 197
85 198
85 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
85 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
85 205
85 206
85 207
85 208
85 209
85 210
1_173_85_16_csi_a16_29.dat
85 212
1_173_85_16_csi_a16_17.dat
85 214
85 215
1_173_85_16_csi_a16_22.dat
85 217
1_173_85_16_csi_a16_26.dat
85 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70
 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75
 75 75 75 75 75 75 75 75 75 75 75 75 75 85 85 85 85 85 85 85 85 85 85 85
 85 85 85 85 85]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.5494 - mae: 0.6765 - mse: 0.5494
64/72 [=========================>....] - ETA: 0s - loss: 0.4045 - mae: 0.5519 - mse: 0.4045
72/72 [==============================] - 1s 11ms/step - loss: 0.3809 - mae: 0.5364 - mse: 0.3809 - val_loss: 0.2220 - val_mae: 0.4442 - val_mse: 0.2220
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1950 - mae: 0.3922 - mse: 0.1950
64/72 [=========================>....] - ETA: 0s - loss: 0.1801 - mae: 0.3677 - mse: 0.1801
72/72 [==============================] - 0s 7ms/step - loss: 0.1705 - mae: 0.3572 - mse: 0.1705 - val_loss: 0.1364 - val_mae: 0.3390 - val_mse: 0.1364
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1380 - mae: 0.3082 - mse: 0.1380
64/72 [=========================>....] - ETA: 0s - loss: 0.1301 - mae: 0.2972 - mse: 0.1301
72/72 [==============================] - 0s 7ms/step - loss: 0.1322 - mae: 0.2990 - mse: 0.1322 - val_loss: 0.0947 - val_mae: 0.2814 - val_mse: 0.0947
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.1528 - mae: 0.3600 - mse: 0.1528
64/72 [=========================>....] - ETA: 0s - loss: 0.1157 - mae: 0.2988 - mse: 0.1157
72/72 [==============================] - 0s 6ms/step - loss: 0.1100 - mae: 0.2857 - mse: 0.1100 - val_loss: 0.0808 - val_mae: 0.2626 - val_mse: 0.0808
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.0743 - mae: 0.2464 - mse: 0.0743
64/72 [=========================>....] - ETA: 0s - loss: 0.0654 - mae: 0.2294 - mse: 0.0654
72/72 [==============================] - 0s 7ms/step - loss: 0.0615 - mae: 0.2212 - mse: 0.0615 - val_loss: 0.0644 - val_mae: 0.2249 - val_mse: 0.0644
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0597 - mae: 0.2109 - mse: 0.0597
64/72 [=========================>....] - ETA: 0s - loss: 0.0534 - mae: 0.2003 - mse: 0.0534
72/72 [==============================] - 0s 7ms/step - loss: 0.0538 - mae: 0.1973 - mse: 0.0538 - val_loss: 0.0471 - val_mae: 0.1738 - val_mse: 0.0471
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0556 - mae: 0.2081 - mse: 0.0556
64/72 [=========================>....] - ETA: 0s - loss: 0.0627 - mae: 0.2152 - mse: 0.0627
72/72 [==============================] - 0s 6ms/step - loss: 0.0609 - mae: 0.2131 - mse: 0.0609 - val_loss: 0.0501 - val_mae: 0.1835 - val_mse: 0.0501
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0495 - mae: 0.1926 - mse: 0.0495
64/72 [=========================>....] - ETA: 0s - loss: 0.0503 - mae: 0.1902 - mse: 0.0503
72/72 [==============================] - 0s 7ms/step - loss: 0.0518 - mae: 0.1927 - mse: 0.0518 - val_loss: 0.0530 - val_mae: 0.1865 - val_mse: 0.0530
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0445 - mae: 0.1798 - mse: 0.0445
64/72 [=========================>....] - ETA: 0s - loss: 0.0528 - mae: 0.1978 - mse: 0.0528
72/72 [==============================] - 0s 7ms/step - loss: 0.0553 - mae: 0.2001 - mse: 0.0553 - val_loss: 0.0512 - val_mae: 0.1814 - val_mse: 0.0512
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0620 - mae: 0.2233 - mse: 0.0620
64/72 [=========================>....] - ETA: 0s - loss: 0.0510 - mae: 0.1925 - mse: 0.0510
72/72 [==============================] - 0s 6ms/step - loss: 0.0478 - mae: 0.1846 - mse: 0.0478 - val_loss: 0.0291 - val_mae: 0.1337 - val_mse: 0.0291
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0601 - mae: 0.1995 - mse: 0.0601
64/72 [=========================>....] - ETA: 0s - loss: 0.0475 - mae: 0.1831 - mse: 0.0475
72/72 [==============================] - 0s 6ms/step - loss: 0.0449 - mae: 0.1772 - mse: 0.0449 - val_loss: 0.0256 - val_mae: 0.1239 - val_mse: 0.0256
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0306 - mae: 0.1496 - mse: 0.0306
64/72 [=========================>....] - ETA: 0s - loss: 0.0352 - mae: 0.1580 - mse: 0.0352
72/72 [==============================] - 0s 7ms/step - loss: 0.0352 - mae: 0.1578 - mse: 0.0352 - val_loss: 0.0305 - val_mae: 0.1327 - val_mse: 0.0305
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0314 - mae: 0.1417 - mse: 0.0314
64/72 [=========================>....] - ETA: 0s - loss: 0.0263 - mae: 0.1308 - mse: 0.0263
72/72 [==============================] - 0s 7ms/step - loss: 0.0276 - mae: 0.1351 - mse: 0.0276 - val_loss: 0.0318 - val_mae: 0.1318 - val_mse: 0.0318
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0275 - mae: 0.1256 - mse: 0.0275
64/72 [=========================>....] - ETA: 0s - loss: 0.0243 - mae: 0.1222 - mse: 0.0243
72/72 [==============================] - 0s 7ms/step - loss: 0.0269 - mae: 0.1275 - mse: 0.0269 - val_loss: 0.0210 - val_mae: 0.1037 - val_mse: 0.0210
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0201 - mae: 0.1078 - mse: 0.0201
64/72 [=========================>....] - ETA: 0s - loss: 0.0203 - mae: 0.1073 - mse: 0.0203
72/72 [==============================] - 0s 6ms/step - loss: 0.0223 - mae: 0.1137 - mse: 0.0223 - val_loss: 0.0226 - val_mae: 0.1077 - val_mse: 0.0226
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0169 - mae: 0.0927 - mse: 0.0169
64/72 [=========================>....] - ETA: 0s - loss: 0.0262 - mae: 0.1157 - mse: 0.0262
72/72 [==============================] - 0s 6ms/step - loss: 0.0241 - mae: 0.1103 - mse: 0.0241 - val_loss: 0.0497 - val_mae: 0.1788 - val_mse: 0.0497
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0322 - mae: 0.1375 - mse: 0.0322
64/72 [=========================>....] - ETA: 0s - loss: 0.0277 - mae: 0.1240 - mse: 0.0277
72/72 [==============================] - 0s 6ms/step - loss: 0.0256 - mae: 0.1189 - mse: 0.0256 - val_loss: 0.0304 - val_mae: 0.1377 - val_mse: 0.0304
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0098 - mae: 0.0762 - mse: 0.0098
64/72 [=========================>....] - ETA: 0s - loss: 0.0124 - mae: 0.0853 - mse: 0.0124
72/72 [==============================] - 0s 7ms/step - loss: 0.0129 - mae: 0.0864 - mse: 0.0129 - val_loss: 0.0097 - val_mae: 0.0784 - val_mse: 0.0097
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0215 - mae: 0.1153 - mse: 0.0215
64/72 [=========================>....] - ETA: 0s - loss: 0.0248 - mae: 0.1217 - mse: 0.0248
72/72 [==============================] - 0s 7ms/step - loss: 0.0249 - mae: 0.1209 - mse: 0.0249 - val_loss: 0.0140 - val_mae: 0.0981 - val_mse: 0.0140
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0215 - mae: 0.1108 - mse: 0.0215
64/72 [=========================>....] - ETA: 0s - loss: 0.0230 - mae: 0.1176 - mse: 0.0230
72/72 [==============================] - 0s 6ms/step - loss: 0.0232 - mae: 0.1200 - mse: 0.0232 - val_loss: 0.0583 - val_mae: 0.1995 - val_mse: 0.0583
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0129 - mae: 0.0951 - mse: 0.0129
64/72 [=========================>....] - ETA: 0s - loss: 0.0328 - mae: 0.1375 - mse: 0.0328
72/72 [==============================] - 0s 7ms/step - loss: 0.0301 - mae: 0.1303 - mse: 0.0301 - val_loss: 0.0466 - val_mae: 0.1660 - val_mse: 0.0466
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0268 - mae: 0.1171 - mse: 0.0268
64/72 [=========================>....] - ETA: 0s - loss: 0.0255 - mae: 0.1152 - mse: 0.0255
72/72 [==============================] - 0s 7ms/step - loss: 0.0251 - mae: 0.1143 - mse: 0.0251 - val_loss: 0.0143 - val_mae: 0.0849 - val_mse: 0.0143
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0204 - mae: 0.1071 - mse: 0.0204
64/72 [=========================>....] - ETA: 0s - loss: 0.0227 - mae: 0.1142 - mse: 0.0227
72/72 [==============================] - 0s 6ms/step - loss: 0.0273 - mae: 0.1239 - mse: 0.0273 - val_loss: 0.0231 - val_mae: 0.1056 - val_mse: 0.0231
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0123 - mae: 0.0889 - mse: 0.0123
64/72 [=========================>....] - ETA: 0s - loss: 0.0111 - mae: 0.0820 - mse: 0.0111
72/72 [==============================] - 0s 7ms/step - loss: 0.0138 - mae: 0.0913 - mse: 0.0138 - val_loss: 0.0496 - val_mae: 0.1670 - val_mse: 0.0496
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0275 - mae: 0.1209 - mse: 0.0275
64/72 [=========================>....] - ETA: 0s - loss: 0.0318 - mae: 0.1349 - mse: 0.0318
72/72 [==============================] - 0s 7ms/step - loss: 0.0323 - mae: 0.1371 - mse: 0.0323 - val_loss: 0.0263 - val_mae: 0.1233 - val_mse: 0.0263
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0184 - mae: 0.0990 - mse: 0.0184
64/72 [=========================>....] - ETA: 0s - loss: 0.0145 - mae: 0.0903 - mse: 0.0145
72/72 [==============================] - 0s 6ms/step - loss: 0.0141 - mae: 0.0904 - mse: 0.0141 - val_loss: 0.0075 - val_mae: 0.0710 - val_mse: 0.0075
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0221 - mae: 0.1114 - mse: 0.0221
64/72 [=========================>....] - ETA: 0s - loss: 0.0227 - mae: 0.1147 - mse: 0.0227
72/72 [==============================] - 0s 7ms/step - loss: 0.0240 - mae: 0.1154 - mse: 0.0240 - val_loss: 0.0113 - val_mae: 0.0813 - val_mse: 0.0113
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0111 - mae: 0.0891 - mse: 0.0111
64/72 [=========================>....] - ETA: 0s - loss: 0.0137 - mae: 0.0944 - mse: 0.0137
72/72 [==============================] - 0s 7ms/step - loss: 0.0156 - mae: 0.0991 - mse: 0.0156 - val_loss: 0.0353 - val_mae: 0.1427 - val_mse: 0.0353
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0214 - mae: 0.1107 - mse: 0.0214
64/72 [=========================>....] - ETA: 0s - loss: 0.0233 - mae: 0.1123 - mse: 0.0233
72/72 [==============================] - 0s 7ms/step - loss: 0.0213 - mae: 0.1060 - mse: 0.0213 - val_loss: 0.0282 - val_mae: 0.1252 - val_mse: 0.0282
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0206 - mae: 0.1183 - mse: 0.0206
64/72 [=========================>....] - ETA: 0s - loss: 0.0155 - mae: 0.0982 - mse: 0.0155
72/72 [==============================] - 0s 7ms/step - loss: 0.0151 - mae: 0.0979 - mse: 0.0151 - val_loss: 0.0144 - val_mae: 0.0803 - val_mse: 0.0144
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         2.22399139 0.         0.        ]
average prediction= [3.6078088]
baseline= 13.452380952380953
eachuser= [0. 0. 0. 6. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.3706652323404948
85 -:- nan
60 -:- nan
['train-weight-16.py', '1']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
65 5
65 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
65 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
65 16
65 17
2_155_65_16_csi_a16_21.dat
65 19
65 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
65 25
2_155_65_16_csi_a16_10.dat
65 27
65 28
2_155_65_16_csi_a16_19.dat
65 30
60 31
2_170_60_16_csi_a16_3.dat
60 33
60 34
60 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
60 38
60 39
60 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_16_csi_a16_19.dat
50 90
2_165_50_16_csi_a16_3.dat
50 92
50 93
2_165_50_16_csi_a16_27.dat
50 95
50 96
50 97
50 98
50 99
50 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
70 105
1_175_70_16_csi_a16_23.dat
70 107
70 108
70 109
1_175_70_16_csi_a16_14.dat
70 111
70 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
70 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
70 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
70 123
70 124
1_175_70_16_csi_a16_5.dat
70 126
1_175_70_16_csi_a16_18.dat
70 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
85 132
1_180_85_16_csi_a16_7.dat
85 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
85 139
85 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
85 148
1_180_85_16_csi_a16_17.dat
85 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
85 155
85 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
75 161
75 162
75 163
1_180_75_16_csi_a16_18.dat
75 165
75 166
1_180_75_16_csi_a16_16.dat
75 168
1_180_75_16_csi_a16_9.dat
75 170
75 171
1_180_75_16_csi_a16_17.dat
75 173
75 174
1_180_75_16_csi_a16_13.dat
75 176
1_180_75_16_csi_a16_10.dat
75 178
75 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
75 183
1_180_75_16_csi_a16_5.dat
75 185
75 186
75 187
75 188
75 189
75 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
85 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
85 197
85 198
85 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
85 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
85 205
85 206
85 207
85 208
85 209
85 210
1_173_85_16_csi_a16_29.dat
85 212
1_173_85_16_csi_a16_17.dat
85 214
85 215
1_173_85_16_csi_a16_22.dat
85 217
1_173_85_16_csi_a16_26.dat
85 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70
 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75
 75 75 75 75 75 75 75 75 75 75 75 75 75 85 85 85 85 85 85 85 85 85 85 85
 85 85 85 85 85]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.5254 - mae: 0.6314 - mse: 0.5254
64/72 [=========================>....] - ETA: 0s - loss: 0.4343 - mae: 0.5628 - mse: 0.4343
72/72 [==============================] - 1s 11ms/step - loss: 0.4174 - mae: 0.5511 - mse: 0.4174 - val_loss: 0.0807 - val_mae: 0.2815 - val_mse: 0.0807
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.3070 - mae: 0.4914 - mse: 0.3070
64/72 [=========================>....] - ETA: 0s - loss: 0.2720 - mae: 0.4698 - mse: 0.2720
72/72 [==============================] - 1s 7ms/step - loss: 0.2614 - mae: 0.4561 - mse: 0.2614 - val_loss: 0.1237 - val_mae: 0.2725 - val_mse: 0.1237
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.2374 - mae: 0.4297 - mse: 0.2374
64/72 [=========================>....] - ETA: 0s - loss: 0.2090 - mae: 0.3908 - mse: 0.2090
72/72 [==============================] - 0s 7ms/step - loss: 0.2018 - mae: 0.3828 - mse: 0.2018 - val_loss: 0.1112 - val_mae: 0.2498 - val_mse: 0.1112
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.1444 - mae: 0.3235 - mse: 0.1444
64/72 [=========================>....] - ETA: 0s - loss: 0.1419 - mae: 0.3145 - mse: 0.1419
72/72 [==============================] - 0s 7ms/step - loss: 0.1421 - mae: 0.3175 - mse: 0.1421 - val_loss: 0.0757 - val_mae: 0.2184 - val_mse: 0.0757
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.1081 - mae: 0.2737 - mse: 0.1081
64/72 [=========================>....] - ETA: 0s - loss: 0.1149 - mae: 0.2930 - mse: 0.1149
72/72 [==============================] - 0s 6ms/step - loss: 0.1090 - mae: 0.2822 - mse: 0.1090 - val_loss: 0.0383 - val_mae: 0.1613 - val_mse: 0.0383
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0895 - mae: 0.2466 - mse: 0.0895
64/72 [=========================>....] - ETA: 0s - loss: 0.0814 - mae: 0.2421 - mse: 0.0814
72/72 [==============================] - 0s 7ms/step - loss: 0.0795 - mae: 0.2414 - mse: 0.0795 - val_loss: 0.0127 - val_mae: 0.0987 - val_mse: 0.0127
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0690 - mae: 0.2297 - mse: 0.0690
64/72 [=========================>....] - ETA: 0s - loss: 0.0585 - mae: 0.1996 - mse: 0.0585
72/72 [==============================] - 0s 7ms/step - loss: 0.0601 - mae: 0.2002 - mse: 0.0601 - val_loss: 0.0080 - val_mae: 0.0808 - val_mse: 0.0080
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0330 - mae: 0.1342 - mse: 0.0330
64/72 [=========================>....] - ETA: 0s - loss: 0.0340 - mae: 0.1425 - mse: 0.0340
72/72 [==============================] - 0s 6ms/step - loss: 0.0336 - mae: 0.1401 - mse: 0.0336 - val_loss: 0.0261 - val_mae: 0.1519 - val_mse: 0.0261
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0514 - mae: 0.1710 - mse: 0.0514
64/72 [=========================>....] - ETA: 0s - loss: 0.0467 - mae: 0.1708 - mse: 0.0467
72/72 [==============================] - 0s 6ms/step - loss: 0.0463 - mae: 0.1695 - mse: 0.0463 - val_loss: 0.0341 - val_mae: 0.1759 - val_mse: 0.0341
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0390 - mae: 0.1627 - mse: 0.0390
64/72 [=========================>....] - ETA: 0s - loss: 0.0354 - mae: 0.1563 - mse: 0.0354
72/72 [==============================] - 0s 7ms/step - loss: 0.0333 - mae: 0.1500 - mse: 0.0333 - val_loss: 0.0259 - val_mae: 0.1410 - val_mse: 0.0259
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0310 - mae: 0.1387 - mse: 0.0310
64/72 [=========================>....] - ETA: 0s - loss: 0.0323 - mae: 0.1398 - mse: 0.0323
72/72 [==============================] - 0s 7ms/step - loss: 0.0344 - mae: 0.1466 - mse: 0.0344 - val_loss: 0.0181 - val_mae: 0.1011 - val_mse: 0.0181
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0236 - mae: 0.1145 - mse: 0.0236
64/72 [=========================>....] - ETA: 0s - loss: 0.0236 - mae: 0.1198 - mse: 0.0236
72/72 [==============================] - 0s 7ms/step - loss: 0.0244 - mae: 0.1239 - mse: 0.0244 - val_loss: 0.0170 - val_mae: 0.0927 - val_mse: 0.0170
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0300 - mae: 0.1268 - mse: 0.0300
64/72 [=========================>....] - ETA: 0s - loss: 0.0314 - mae: 0.1336 - mse: 0.0314
72/72 [==============================] - 0s 6ms/step - loss: 0.0306 - mae: 0.1330 - mse: 0.0306 - val_loss: 0.0123 - val_mae: 0.0775 - val_mse: 0.0123
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0281 - mae: 0.1373 - mse: 0.0281
64/72 [=========================>....] - ETA: 0s - loss: 0.0287 - mae: 0.1428 - mse: 0.0287
72/72 [==============================] - 0s 7ms/step - loss: 0.0281 - mae: 0.1404 - mse: 0.0281 - val_loss: 0.0104 - val_mae: 0.0649 - val_mse: 0.0104
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0218 - mae: 0.1189 - mse: 0.0218
64/72 [=========================>....] - ETA: 0s - loss: 0.0258 - mae: 0.1306 - mse: 0.0258
72/72 [==============================] - 0s 7ms/step - loss: 0.0261 - mae: 0.1328 - mse: 0.0261 - val_loss: 0.0104 - val_mae: 0.0661 - val_mse: 0.0104
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0112 - mae: 0.0876 - mse: 0.0112
64/72 [=========================>....] - ETA: 0s - loss: 0.0140 - mae: 0.0984 - mse: 0.0140
72/72 [==============================] - 0s 7ms/step - loss: 0.0139 - mae: 0.0981 - mse: 0.0139 - val_loss: 0.0143 - val_mae: 0.0862 - val_mse: 0.0143
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0222 - mae: 0.1036 - mse: 0.0222
64/72 [=========================>....] - ETA: 0s - loss: 0.0283 - mae: 0.1251 - mse: 0.0283
72/72 [==============================] - 1s 7ms/step - loss: 0.0277 - mae: 0.1232 - mse: 0.0277 - val_loss: 0.0088 - val_mae: 0.0709 - val_mse: 0.0088
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0155 - mae: 0.0934 - mse: 0.0155
64/72 [=========================>....] - ETA: 0s - loss: 0.0186 - mae: 0.1009 - mse: 0.0186
72/72 [==============================] - 0s 7ms/step - loss: 0.0189 - mae: 0.1017 - mse: 0.0189 - val_loss: 0.0076 - val_mae: 0.0721 - val_mse: 0.0076
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0319 - mae: 0.1367 - mse: 0.0319
64/72 [=========================>....] - ETA: 0s - loss: 0.0239 - mae: 0.1180 - mse: 0.0239
72/72 [==============================] - 0s 7ms/step - loss: 0.0230 - mae: 0.1160 - mse: 0.0230 - val_loss: 0.0068 - val_mae: 0.0665 - val_mse: 0.0068
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0118 - mae: 0.0852 - mse: 0.0118
64/72 [=========================>....] - ETA: 0s - loss: 0.0144 - mae: 0.0928 - mse: 0.0144
72/72 [==============================] - 0s 7ms/step - loss: 0.0147 - mae: 0.0952 - mse: 0.0147 - val_loss: 0.0089 - val_mae: 0.0707 - val_mse: 0.0089
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0187 - mae: 0.1027 - mse: 0.0187
64/72 [=========================>....] - ETA: 0s - loss: 0.0206 - mae: 0.1071 - mse: 0.0206
72/72 [==============================] - 0s 7ms/step - loss: 0.0196 - mae: 0.1046 - mse: 0.0196 - val_loss: 0.0053 - val_mae: 0.0548 - val_mse: 0.0053
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0310 - mae: 0.1149 - mse: 0.0310
64/72 [=========================>....] - ETA: 0s - loss: 0.0256 - mae: 0.1107 - mse: 0.0256
72/72 [==============================] - 0s 7ms/step - loss: 0.0258 - mae: 0.1122 - mse: 0.0258 - val_loss: 0.0037 - val_mae: 0.0509 - val_mse: 0.0037
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0245 - mae: 0.1209 - mse: 0.0245
64/72 [=========================>....] - ETA: 0s - loss: 0.0216 - mae: 0.1147 - mse: 0.0216
72/72 [==============================] - 0s 7ms/step - loss: 0.0218 - mae: 0.1148 - mse: 0.0218 - val_loss: 0.0040 - val_mae: 0.0462 - val_mse: 0.0040
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0124 - mae: 0.0880 - mse: 0.0124
64/72 [=========================>....] - ETA: 0s - loss: 0.0173 - mae: 0.0996 - mse: 0.0173
72/72 [==============================] - 0s 6ms/step - loss: 0.0172 - mae: 0.0998 - mse: 0.0172 - val_loss: 0.0064 - val_mae: 0.0505 - val_mse: 0.0064
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0103 - mae: 0.0827 - mse: 0.0103
64/72 [=========================>....] - ETA: 0s - loss: 0.0113 - mae: 0.0805 - mse: 0.0113
72/72 [==============================] - 0s 7ms/step - loss: 0.0117 - mae: 0.0826 - mse: 0.0117 - val_loss: 0.0040 - val_mae: 0.0526 - val_mse: 0.0040
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0120 - mae: 0.0884 - mse: 0.0120
64/72 [=========================>....] - ETA: 0s - loss: 0.0126 - mae: 0.0885 - mse: 0.0126
72/72 [==============================] - 0s 6ms/step - loss: 0.0122 - mae: 0.0878 - mse: 0.0122 - val_loss: 0.0048 - val_mae: 0.0621 - val_mse: 0.0048
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0125 - mae: 0.0763 - mse: 0.0125
64/72 [=========================>....] - ETA: 0s - loss: 0.0129 - mae: 0.0851 - mse: 0.0129
72/72 [==============================] - 0s 6ms/step - loss: 0.0123 - mae: 0.0836 - mse: 0.0123 - val_loss: 0.0049 - val_mae: 0.0610 - val_mse: 0.0049
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0131 - mae: 0.0898 - mse: 0.0131
64/72 [=========================>....] - ETA: 0s - loss: 0.0138 - mae: 0.0912 - mse: 0.0138
72/72 [==============================] - 0s 7ms/step - loss: 0.0128 - mae: 0.0868 - mse: 0.0128 - val_loss: 0.0064 - val_mae: 0.0566 - val_mse: 0.0064
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0115 - mae: 0.0806 - mse: 0.0115
64/72 [=========================>....] - ETA: 0s - loss: 0.0175 - mae: 0.0994 - mse: 0.0175
72/72 [==============================] - 0s 6ms/step - loss: 0.0168 - mae: 0.0981 - mse: 0.0168 - val_loss: 0.0052 - val_mae: 0.0528 - val_mse: 0.0052
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0209 - mae: 0.1051 - mse: 0.0209
64/72 [=========================>....] - ETA: 0s - loss: 0.0150 - mae: 0.0916 - mse: 0.0150
72/72 [==============================] - 0s 6ms/step - loss: 0.0148 - mae: 0.0907 - mse: 0.0148 - val_loss: 0.0051 - val_mae: 0.0520 - val_mse: 0.0051
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         4.76315689 0.         0.        ]
average prediction= [2.9761343]
baseline= 10.595238095238095
eachuser= [0. 0. 0. 3. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.5877189636230469
85 -:- nan
60 -:- nan
['train-weight-16.py', '1']
2_155_65_16_csi_a16_18.dat
2_155_65_16_csi_a16_5.dat
2_155_65_16_csi_a16_23.dat
2_155_65_16_csi_a16_9.dat
65 5
65 6
2_155_65_16_csi_a16_25.dat
2_155_65_16_csi_a16_4.dat
2_155_65_16_csi_a16_29.dat
2_155_65_16_csi_a16_14.dat
65 11
2_155_65_16_csi_a16_3.dat
2_155_65_16_csi_a16_28.dat
2_155_65_16_csi_a16_13.dat
2_155_65_16_csi_a16_30.dat
65 16
65 17
2_155_65_16_csi_a16_21.dat
65 19
65 20
2_155_65_16_csi_a16_24.dat
2_155_65_16_csi_a16_17.dat
2_155_65_16_csi_a16_15.dat
2_155_65_16_csi_a16_6.dat
65 25
2_155_65_16_csi_a16_10.dat
65 27
65 28
2_155_65_16_csi_a16_19.dat
65 30
60 31
2_170_60_16_csi_a16_3.dat
60 33
60 34
60 35
2_170_60_16_csi_a16_9.dat
2_170_60_16_csi_a16_6.dat
60 38
60 39
60 40
1_165_65_16_csi_a16_30.dat
1_165_65_16_csi_a16_16.dat
1_165_65_16_csi_a16_14.dat
1_165_65_16_csi_a16_7.dat
1_165_65_16_csi_a16_20.dat
1_165_65_16_csi_a16_17.dat
1_165_65_16_csi_a16_6.dat
1_165_65_16_csi_a16_4.dat
1_165_65_16_csi_a16_29.dat
1_165_65_16_csi_a16_24.dat
1_165_65_16_csi_a16_9.dat
1_165_65_16_csi_a16_19.dat
1_165_65_16_csi_a16_2.dat
1_165_65_16_csi_a16_1.dat
1_165_65_16_csi_a16_28.dat
1_165_65_16_csi_a16_23.dat
1_165_65_16_csi_a16_15.dat
1_165_65_16_csi_a16_3.dat
1_165_65_16_csi_a16_25.dat
1_165_65_16_csi_a16_12.dat
1_165_65_16_csi_a16_5.dat
1_165_65_16_csi_a16_21.dat
1_165_65_16_csi_a16_10.dat
1_165_65_16_csi_a16_11.dat
1_165_65_16_csi_a16_13.dat
1_165_65_16_csi_a16_22.dat
1_165_65_16_csi_a16_27.dat
1_165_65_16_csi_a16_18.dat
1_165_65_16_csi_a16_26.dat
1_165_65_16_csi_a16_8.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_16_csi_a16_19.dat
50 90
2_165_50_16_csi_a16_3.dat
50 92
50 93
2_165_50_16_csi_a16_27.dat
50 95
50 96
50 97
50 98
50 99
50 100
1_175_70_16_csi_a16_8.dat
1_175_70_16_csi_a16_20.dat
1_175_70_16_csi_a16_1.dat
1_175_70_16_csi_a16_2.dat
70 105
1_175_70_16_csi_a16_23.dat
70 107
70 108
70 109
1_175_70_16_csi_a16_14.dat
70 111
70 112
1_175_70_16_csi_a16_16.dat
1_175_70_16_csi_a16_25.dat
70 115
1_175_70_16_csi_a16_6.dat
1_175_70_16_csi_a16_10.dat
70 118
1_175_70_16_csi_a16_3.dat
1_175_70_16_csi_a16_15.dat
1_175_70_16_csi_a16_9.dat
1_175_70_16_csi_a16_19.dat
70 123
70 124
1_175_70_16_csi_a16_5.dat
70 126
1_175_70_16_csi_a16_18.dat
70 128
1_175_70_16_csi_a16_28.dat
1_175_70_16_csi_a16_7.dat
1_180_85_16_csi_a16_30.dat
85 132
1_180_85_16_csi_a16_7.dat
85 134
1_180_85_16_csi_a16_15.dat
1_180_85_16_csi_a16_11.dat
1_180_85_16_csi_a16_9.dat
1_180_85_16_csi_a16_4.dat
85 139
85 140
1_180_85_16_csi_a16_13.dat
1_180_85_16_csi_a16_27.dat
1_180_85_16_csi_a16_16.dat
1_180_85_16_csi_a16_18.dat
1_180_85_16_csi_a16_2.dat
1_180_85_16_csi_a16_5.dat
1_180_85_16_csi_a16_3.dat
85 148
1_180_85_16_csi_a16_17.dat
85 150
1_180_85_16_csi_a16_23.dat
1_180_85_16_csi_a16_25.dat
1_180_85_16_csi_a16_12.dat
1_180_85_16_csi_a16_6.dat
85 155
85 156
1_180_85_16_csi_a16_29.dat
1_180_85_16_csi_a16_26.dat
1_180_85_16_csi_a16_20.dat
1_180_85_16_csi_a16_22.dat
75 161
75 162
75 163
1_180_75_16_csi_a16_18.dat
75 165
75 166
1_180_75_16_csi_a16_16.dat
75 168
1_180_75_16_csi_a16_9.dat
75 170
75 171
1_180_75_16_csi_a16_17.dat
75 173
75 174
1_180_75_16_csi_a16_13.dat
75 176
1_180_75_16_csi_a16_10.dat
75 178
75 179
1_180_75_16_csi_a16_14.dat
1_180_75_16_csi_a16_4.dat
1_180_75_16_csi_a16_21.dat
75 183
1_180_75_16_csi_a16_5.dat
75 185
75 186
75 187
75 188
75 189
75 190
1_173_85_16_csi_a16_16.dat
1_173_85_16_csi_a16_25.dat
1_173_85_16_csi_a16_12.dat
85 194
1_173_85_16_csi_a16_20.dat
1_173_85_16_csi_a16_21.dat
85 197
85 198
85 199
1_173_85_16_csi_a16_23.dat
1_173_85_16_csi_a16_27.dat
85 202
1_173_85_16_csi_a16_19.dat
1_173_85_16_csi_a16_10.dat
85 205
85 206
85 207
85 208
85 209
85 210
1_173_85_16_csi_a16_29.dat
85 212
1_173_85_16_csi_a16_17.dat
85 214
85 215
1_173_85_16_csi_a16_22.dat
85 217
1_173_85_16_csi_a16_26.dat
85 219
1_173_85_16_csi_a16_24.dat
(101, 30, 3)
(101, 462, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70
 70 70 70 70 70 70 70 70 70 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75
 75 75 75 75 75 75 75 75 75 75 75 75 75 85 85 85 85 85 85 85 85 85 85 85
 85 85 85 85 85]
(101, 462, 30, 3, 1)

Loaded dataset of 101 samples, each sized (462, 30, 3, 1)


Train on 80 samples
Test on 21 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 462, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 462, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 462, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 462, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 462, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 462, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 462, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 72 samples, validate on 8 samples
Epoch 1/30

32/72 [============>.................] - ETA: 0s - loss: 0.4433 - mae: 0.5646 - mse: 0.4433
64/72 [=========================>....] - ETA: 0s - loss: 0.3636 - mae: 0.5141 - mse: 0.3636
72/72 [==============================] - 1s 11ms/step - loss: 0.3390 - mae: 0.4930 - mse: 0.3390 - val_loss: 0.1367 - val_mae: 0.3183 - val_mse: 0.1367
Epoch 2/30

32/72 [============>.................] - ETA: 0s - loss: 0.1088 - mae: 0.2949 - mse: 0.1088
64/72 [=========================>....] - ETA: 0s - loss: 0.1238 - mae: 0.3066 - mse: 0.1238
72/72 [==============================] - 0s 7ms/step - loss: 0.1324 - mae: 0.3133 - mse: 0.1324 - val_loss: 0.0902 - val_mae: 0.2738 - val_mse: 0.0902
Epoch 3/30

32/72 [============>.................] - ETA: 0s - loss: 0.1170 - mae: 0.2648 - mse: 0.1170
64/72 [=========================>....] - ETA: 0s - loss: 0.1428 - mae: 0.3095 - mse: 0.1428
72/72 [==============================] - 0s 7ms/step - loss: 0.1429 - mae: 0.3092 - mse: 0.1429 - val_loss: 0.0655 - val_mae: 0.2509 - val_mse: 0.0655
Epoch 4/30

32/72 [============>.................] - ETA: 0s - loss: 0.0837 - mae: 0.2474 - mse: 0.0837
64/72 [=========================>....] - ETA: 0s - loss: 0.0833 - mae: 0.2498 - mse: 0.0833
72/72 [==============================] - 0s 7ms/step - loss: 0.0828 - mae: 0.2520 - mse: 0.0828 - val_loss: 0.0809 - val_mae: 0.2317 - val_mse: 0.0809
Epoch 5/30

32/72 [============>.................] - ETA: 0s - loss: 0.1084 - mae: 0.3043 - mse: 0.1084
64/72 [=========================>....] - ETA: 0s - loss: 0.0935 - mae: 0.2758 - mse: 0.0935
72/72 [==============================] - 0s 6ms/step - loss: 0.0856 - mae: 0.2604 - mse: 0.0856 - val_loss: 0.0758 - val_mae: 0.2164 - val_mse: 0.0758
Epoch 6/30

32/72 [============>.................] - ETA: 0s - loss: 0.0676 - mae: 0.2328 - mse: 0.0676
64/72 [=========================>....] - ETA: 0s - loss: 0.0663 - mae: 0.2279 - mse: 0.0663
72/72 [==============================] - 0s 7ms/step - loss: 0.0627 - mae: 0.2203 - mse: 0.0627 - val_loss: 0.0499 - val_mae: 0.1750 - val_mse: 0.0499
Epoch 7/30

32/72 [============>.................] - ETA: 0s - loss: 0.0627 - mae: 0.2233 - mse: 0.0627
64/72 [=========================>....] - ETA: 0s - loss: 0.0499 - mae: 0.1965 - mse: 0.0499
72/72 [==============================] - 0s 7ms/step - loss: 0.0545 - mae: 0.2000 - mse: 0.0545 - val_loss: 0.0353 - val_mae: 0.1470 - val_mse: 0.0353
Epoch 8/30

32/72 [============>.................] - ETA: 0s - loss: 0.0502 - mae: 0.1839 - mse: 0.0502
64/72 [=========================>....] - ETA: 0s - loss: 0.0481 - mae: 0.1793 - mse: 0.0481
72/72 [==============================] - 0s 7ms/step - loss: 0.0461 - mae: 0.1771 - mse: 0.0461 - val_loss: 0.0333 - val_mae: 0.1433 - val_mse: 0.0333
Epoch 9/30

32/72 [============>.................] - ETA: 0s - loss: 0.0366 - mae: 0.1611 - mse: 0.0366
64/72 [=========================>....] - ETA: 0s - loss: 0.0434 - mae: 0.1707 - mse: 0.0434
72/72 [==============================] - 0s 7ms/step - loss: 0.0407 - mae: 0.1646 - mse: 0.0407 - val_loss: 0.0309 - val_mae: 0.1386 - val_mse: 0.0309
Epoch 10/30

32/72 [============>.................] - ETA: 0s - loss: 0.0371 - mae: 0.1470 - mse: 0.0371
64/72 [=========================>....] - ETA: 0s - loss: 0.0409 - mae: 0.1582 - mse: 0.0409
72/72 [==============================] - 0s 7ms/step - loss: 0.0404 - mae: 0.1568 - mse: 0.0404 - val_loss: 0.0186 - val_mae: 0.1074 - val_mse: 0.0186
Epoch 11/30

32/72 [============>.................] - ETA: 0s - loss: 0.0250 - mae: 0.1332 - mse: 0.0250
64/72 [=========================>....] - ETA: 0s - loss: 0.0218 - mae: 0.1217 - mse: 0.0218
72/72 [==============================] - 0s 7ms/step - loss: 0.0225 - mae: 0.1214 - mse: 0.0225 - val_loss: 0.0082 - val_mae: 0.0742 - val_mse: 0.0082
Epoch 12/30

32/72 [============>.................] - ETA: 0s - loss: 0.0340 - mae: 0.1335 - mse: 0.0340
64/72 [=========================>....] - ETA: 0s - loss: 0.0353 - mae: 0.1401 - mse: 0.0353
72/72 [==============================] - 0s 6ms/step - loss: 0.0334 - mae: 0.1340 - mse: 0.0334 - val_loss: 0.0185 - val_mae: 0.1173 - val_mse: 0.0185
Epoch 13/30

32/72 [============>.................] - ETA: 0s - loss: 0.0234 - mae: 0.1094 - mse: 0.0234
64/72 [=========================>....] - ETA: 0s - loss: 0.0230 - mae: 0.1141 - mse: 0.0230
72/72 [==============================] - 0s 7ms/step - loss: 0.0252 - mae: 0.1218 - mse: 0.0252 - val_loss: 0.0228 - val_mae: 0.1301 - val_mse: 0.0228
Epoch 14/30

32/72 [============>.................] - ETA: 0s - loss: 0.0236 - mae: 0.1173 - mse: 0.0236
64/72 [=========================>....] - ETA: 0s - loss: 0.0273 - mae: 0.1250 - mse: 0.0273
72/72 [==============================] - 1s 7ms/step - loss: 0.0271 - mae: 0.1265 - mse: 0.0271 - val_loss: 0.0210 - val_mae: 0.1240 - val_mse: 0.0210
Epoch 15/30

32/72 [============>.................] - ETA: 0s - loss: 0.0248 - mae: 0.1247 - mse: 0.0248
64/72 [=========================>....] - ETA: 0s - loss: 0.0206 - mae: 0.1104 - mse: 0.0206
72/72 [==============================] - 0s 7ms/step - loss: 0.0201 - mae: 0.1095 - mse: 0.0201 - val_loss: 0.0088 - val_mae: 0.0837 - val_mse: 0.0088
Epoch 16/30

32/72 [============>.................] - ETA: 0s - loss: 0.0231 - mae: 0.1180 - mse: 0.0231
64/72 [=========================>....] - ETA: 0s - loss: 0.0277 - mae: 0.1284 - mse: 0.0277
72/72 [==============================] - 0s 6ms/step - loss: 0.0264 - mae: 0.1251 - mse: 0.0264 - val_loss: 0.0093 - val_mae: 0.0859 - val_mse: 0.0093
Epoch 17/30

32/72 [============>.................] - ETA: 0s - loss: 0.0298 - mae: 0.1335 - mse: 0.0298
64/72 [=========================>....] - ETA: 0s - loss: 0.0295 - mae: 0.1299 - mse: 0.0295
72/72 [==============================] - 0s 6ms/step - loss: 0.0278 - mae: 0.1260 - mse: 0.0278 - val_loss: 0.0299 - val_mae: 0.1468 - val_mse: 0.0299
Epoch 18/30

32/72 [============>.................] - ETA: 0s - loss: 0.0240 - mae: 0.1148 - mse: 0.0240
64/72 [=========================>....] - ETA: 0s - loss: 0.0241 - mae: 0.1198 - mse: 0.0241
72/72 [==============================] - 0s 7ms/step - loss: 0.0219 - mae: 0.1123 - mse: 0.0219 - val_loss: 0.0279 - val_mae: 0.1493 - val_mse: 0.0279
Epoch 19/30

32/72 [============>.................] - ETA: 0s - loss: 0.0288 - mae: 0.1342 - mse: 0.0288
64/72 [=========================>....] - ETA: 0s - loss: 0.0250 - mae: 0.1246 - mse: 0.0250
72/72 [==============================] - 0s 7ms/step - loss: 0.0224 - mae: 0.1144 - mse: 0.0224 - val_loss: 0.0170 - val_mae: 0.1153 - val_mse: 0.0170
Epoch 20/30

32/72 [============>.................] - ETA: 0s - loss: 0.0178 - mae: 0.0986 - mse: 0.0178
64/72 [=========================>....] - ETA: 0s - loss: 0.0190 - mae: 0.1016 - mse: 0.0190
72/72 [==============================] - 0s 7ms/step - loss: 0.0199 - mae: 0.1052 - mse: 0.0199 - val_loss: 0.0164 - val_mae: 0.1126 - val_mse: 0.0164
Epoch 21/30

32/72 [============>.................] - ETA: 0s - loss: 0.0148 - mae: 0.0871 - mse: 0.0148
64/72 [=========================>....] - ETA: 0s - loss: 0.0195 - mae: 0.1030 - mse: 0.0195
72/72 [==============================] - 0s 6ms/step - loss: 0.0182 - mae: 0.0992 - mse: 0.0182 - val_loss: 0.0175 - val_mae: 0.1155 - val_mse: 0.0175
Epoch 22/30

32/72 [============>.................] - ETA: 0s - loss: 0.0142 - mae: 0.0875 - mse: 0.0142
64/72 [=========================>....] - ETA: 0s - loss: 0.0152 - mae: 0.0906 - mse: 0.0152
72/72 [==============================] - 0s 6ms/step - loss: 0.0162 - mae: 0.0928 - mse: 0.0162 - val_loss: 0.0136 - val_mae: 0.0971 - val_mse: 0.0136
Epoch 23/30

32/72 [============>.................] - ETA: 0s - loss: 0.0212 - mae: 0.1093 - mse: 0.0212
64/72 [=========================>....] - ETA: 0s - loss: 0.0174 - mae: 0.0993 - mse: 0.0174
72/72 [==============================] - 0s 7ms/step - loss: 0.0160 - mae: 0.0950 - mse: 0.0160 - val_loss: 0.0194 - val_mae: 0.1225 - val_mse: 0.0194
Epoch 24/30

32/72 [============>.................] - ETA: 0s - loss: 0.0210 - mae: 0.1124 - mse: 0.0210
64/72 [=========================>....] - ETA: 0s - loss: 0.0219 - mae: 0.1128 - mse: 0.0219
72/72 [==============================] - 0s 7ms/step - loss: 0.0207 - mae: 0.1090 - mse: 0.0207 - val_loss: 0.0267 - val_mae: 0.1485 - val_mse: 0.0267
Epoch 25/30

32/72 [============>.................] - ETA: 0s - loss: 0.0165 - mae: 0.0847 - mse: 0.0165
64/72 [=========================>....] - ETA: 0s - loss: 0.0191 - mae: 0.0975 - mse: 0.0191
72/72 [==============================] - 0s 7ms/step - loss: 0.0182 - mae: 0.0957 - mse: 0.0182 - val_loss: 0.0225 - val_mae: 0.1363 - val_mse: 0.0225
Epoch 26/30

32/72 [============>.................] - ETA: 0s - loss: 0.0193 - mae: 0.1004 - mse: 0.0193
64/72 [=========================>....] - ETA: 0s - loss: 0.0226 - mae: 0.1110 - mse: 0.0226
72/72 [==============================] - 0s 6ms/step - loss: 0.0217 - mae: 0.1092 - mse: 0.0217 - val_loss: 0.0148 - val_mae: 0.1052 - val_mse: 0.0148
Epoch 27/30

32/72 [============>.................] - ETA: 0s - loss: 0.0155 - mae: 0.0851 - mse: 0.0155
64/72 [=========================>....] - ETA: 0s - loss: 0.0159 - mae: 0.0882 - mse: 0.0159
72/72 [==============================] - 0s 6ms/step - loss: 0.0152 - mae: 0.0856 - mse: 0.0152 - val_loss: 0.0122 - val_mae: 0.0937 - val_mse: 0.0122
Epoch 28/30

32/72 [============>.................] - ETA: 0s - loss: 0.0206 - mae: 0.1020 - mse: 0.0206
64/72 [=========================>....] - ETA: 0s - loss: 0.0201 - mae: 0.1026 - mse: 0.0201
72/72 [==============================] - 0s 7ms/step - loss: 0.0199 - mae: 0.1013 - mse: 0.0199 - val_loss: 0.0175 - val_mae: 0.1170 - val_mse: 0.0175
Epoch 29/30

32/72 [============>.................] - ETA: 0s - loss: 0.0166 - mae: 0.0942 - mse: 0.0166
64/72 [=========================>....] - ETA: 0s - loss: 0.0130 - mae: 0.0837 - mse: 0.0130
72/72 [==============================] - 0s 7ms/step - loss: 0.0128 - mae: 0.0829 - mse: 0.0128 - val_loss: 0.0189 - val_mae: 0.1183 - val_mse: 0.0189
Epoch 30/30

32/72 [============>.................] - ETA: 0s - loss: 0.0167 - mae: 0.0978 - mse: 0.0167
64/72 [=========================>....] - ETA: 0s - loss: 0.0153 - mae: 0.0948 - mse: 0.0153
72/72 [==============================] - 0s 6ms/step - loss: 0.0156 - mae: 0.0957 - mse: 0.0156 - val_loss: 0.0117 - val_mae: 0.0879 - val_mse: 0.0117
Saving trained model...
119
Testing...
heightdiff= [0.         0.         0.         6.69046021 0.         0.        ]
average prediction= [2.6211562]
baseline= 12.261904761904763
eachuser= [0. 0. 0. 6. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.1150767008463542
85 -:- nan
60 -:- nan
