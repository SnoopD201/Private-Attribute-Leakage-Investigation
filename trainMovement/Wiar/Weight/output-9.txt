['train-weight-9.py', '1']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
65 7
2_155_65_9_csi_a9_19.dat
65 9
65 10
65 11
65 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
65 15
2_155_65_9_csi_a9_7.dat
65 17
65 18
65 19
2_155_65_9_csi_a9_21.dat
65 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
65 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_9_csi_a9_29.dat
65 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
65 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
65 52
1_165_65_9_csi_a9_16.dat
65 54
65 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
65 58
65 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
65 62
65 63
1_165_65_9_csi_a9_5.dat
65 65
65 66
1_165_65_9_csi_a9_18.dat
65 68
65 69
1_165_65_9_csi_a9_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_9_csi_a9_22.dat
50 90
50 91
50 92
50 93
50 94
2_165_50_9_csi_a9_26.dat
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
1_180_85_9_csi_a9_7.dat
85 132
85 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
85 140
85 141
85 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
85 146
85 147
85 148
85 149
85 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
85 153
85 154
85 155
1_180_85_9_csi_a9_18.dat
85 157
85 158
85 159
85 160
75 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
75 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
75 171
1_180_75_9_csi_a9_25.dat
75 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
75 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
75 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
75 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
85 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
85 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
85 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
85 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
85 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 60 60 60 65 65 65
 65 65 65 65 65 65 65 65 65 65 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75 85 85 85 85
 85]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.3873 - mae: 0.5333 - mse: 0.3873
64/86 [=====================>........] - ETA: 0s - loss: 0.3224 - mae: 0.4890 - mse: 0.3224
86/86 [==============================] - 1s 9ms/step - loss: 0.2801 - mae: 0.4512 - mse: 0.2801 - val_loss: 0.1224 - val_mae: 0.2918 - val_mse: 0.1224
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1132 - mae: 0.2724 - mse: 0.1132
64/86 [=====================>........] - ETA: 0s - loss: 0.1510 - mae: 0.3176 - mse: 0.1510
86/86 [==============================] - 0s 5ms/step - loss: 0.1523 - mae: 0.3084 - mse: 0.1523 - val_loss: 0.1304 - val_mae: 0.3105 - val_mse: 0.1304
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.2202 - mae: 0.3574 - mse: 0.2202
64/86 [=====================>........] - ETA: 0s - loss: 0.1697 - mae: 0.3187 - mse: 0.1697
86/86 [==============================] - 0s 6ms/step - loss: 0.1559 - mae: 0.3037 - mse: 0.1559 - val_loss: 0.0791 - val_mae: 0.2395 - val_mse: 0.0791
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0853 - mae: 0.2573 - mse: 0.0853
64/86 [=====================>........] - ETA: 0s - loss: 0.0735 - mae: 0.2268 - mse: 0.0735
86/86 [==============================] - 0s 5ms/step - loss: 0.0823 - mae: 0.2411 - mse: 0.0823 - val_loss: 0.0704 - val_mae: 0.2124 - val_mse: 0.0704
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0706 - mae: 0.2323 - mse: 0.0706
64/86 [=====================>........] - ETA: 0s - loss: 0.0703 - mae: 0.2305 - mse: 0.0703
86/86 [==============================] - 0s 5ms/step - loss: 0.0749 - mae: 0.2362 - mse: 0.0749 - val_loss: 0.0558 - val_mae: 0.1860 - val_mse: 0.0558
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0777 - mae: 0.2547 - mse: 0.0777
64/86 [=====================>........] - ETA: 0s - loss: 0.0715 - mae: 0.2400 - mse: 0.0715
86/86 [==============================] - 0s 6ms/step - loss: 0.0723 - mae: 0.2366 - mse: 0.0723 - val_loss: 0.0337 - val_mae: 0.1463 - val_mse: 0.0337
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0985 - mae: 0.2695 - mse: 0.0985
64/86 [=====================>........] - ETA: 0s - loss: 0.0735 - mae: 0.2293 - mse: 0.0735
86/86 [==============================] - 0s 5ms/step - loss: 0.0804 - mae: 0.2404 - mse: 0.0804 - val_loss: 0.0249 - val_mae: 0.1198 - val_mse: 0.0249
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0695 - mae: 0.2129 - mse: 0.0695
64/86 [=====================>........] - ETA: 0s - loss: 0.0737 - mae: 0.2305 - mse: 0.0737
86/86 [==============================] - 1s 6ms/step - loss: 0.0678 - mae: 0.2190 - mse: 0.0678 - val_loss: 0.0295 - val_mae: 0.1332 - val_mse: 0.0295
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0501 - mae: 0.1891 - mse: 0.0501
64/86 [=====================>........] - ETA: 0s - loss: 0.0572 - mae: 0.2024 - mse: 0.0572
86/86 [==============================] - 0s 5ms/step - loss: 0.0535 - mae: 0.1956 - mse: 0.0535 - val_loss: 0.0354 - val_mae: 0.1410 - val_mse: 0.0354
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0500 - mae: 0.1926 - mse: 0.0500
64/86 [=====================>........] - ETA: 0s - loss: 0.0429 - mae: 0.1735 - mse: 0.0429
86/86 [==============================] - 0s 5ms/step - loss: 0.0437 - mae: 0.1752 - mse: 0.0437 - val_loss: 0.0403 - val_mae: 0.1495 - val_mse: 0.0403
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0453 - mae: 0.1685 - mse: 0.0453
64/86 [=====================>........] - ETA: 0s - loss: 0.0372 - mae: 0.1613 - mse: 0.0372
86/86 [==============================] - 0s 6ms/step - loss: 0.0399 - mae: 0.1639 - mse: 0.0399 - val_loss: 0.0319 - val_mae: 0.1386 - val_mse: 0.0319
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0229 - mae: 0.1322 - mse: 0.0229
64/86 [=====================>........] - ETA: 0s - loss: 0.0283 - mae: 0.1414 - mse: 0.0283
86/86 [==============================] - 0s 5ms/step - loss: 0.0376 - mae: 0.1549 - mse: 0.0376 - val_loss: 0.0241 - val_mae: 0.1273 - val_mse: 0.0241
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0349 - mae: 0.1595 - mse: 0.0349
64/86 [=====================>........] - ETA: 0s - loss: 0.0324 - mae: 0.1524 - mse: 0.0324
86/86 [==============================] - 0s 5ms/step - loss: 0.0282 - mae: 0.1398 - mse: 0.0282 - val_loss: 0.0343 - val_mae: 0.1507 - val_mse: 0.0343
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0287 - mae: 0.1342 - mse: 0.0287
64/86 [=====================>........] - ETA: 0s - loss: 0.0373 - mae: 0.1544 - mse: 0.0373
86/86 [==============================] - 0s 5ms/step - loss: 0.0360 - mae: 0.1546 - mse: 0.0360 - val_loss: 0.0387 - val_mae: 0.1569 - val_mse: 0.0387
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0379 - mae: 0.1534 - mse: 0.0379
64/86 [=====================>........] - ETA: 0s - loss: 0.0351 - mae: 0.1499 - mse: 0.0351
86/86 [==============================] - 0s 6ms/step - loss: 0.0362 - mae: 0.1533 - mse: 0.0362 - val_loss: 0.0274 - val_mae: 0.1364 - val_mse: 0.0274
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0371 - mae: 0.1294 - mse: 0.0371
64/86 [=====================>........] - ETA: 0s - loss: 0.0326 - mae: 0.1294 - mse: 0.0326
86/86 [==============================] - 0s 5ms/step - loss: 0.0276 - mae: 0.1181 - mse: 0.0276 - val_loss: 0.0245 - val_mae: 0.1265 - val_mse: 0.0245
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0390 - mae: 0.1551 - mse: 0.0390
64/86 [=====================>........] - ETA: 0s - loss: 0.0292 - mae: 0.1292 - mse: 0.0292
86/86 [==============================] - 0s 5ms/step - loss: 0.0277 - mae: 0.1266 - mse: 0.0277 - val_loss: 0.0228 - val_mae: 0.1236 - val_mse: 0.0228
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0231 - mae: 0.1188 - mse: 0.0231
64/86 [=====================>........] - ETA: 0s - loss: 0.0168 - mae: 0.1003 - mse: 0.0168
86/86 [==============================] - 0s 5ms/step - loss: 0.0184 - mae: 0.1078 - mse: 0.0184 - val_loss: 0.0234 - val_mae: 0.1293 - val_mse: 0.0234
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0298 - mae: 0.1316 - mse: 0.0298
64/86 [=====================>........] - ETA: 0s - loss: 0.0234 - mae: 0.1149 - mse: 0.0234
86/86 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.1172 - mse: 0.0242 - val_loss: 0.0294 - val_mae: 0.1431 - val_mse: 0.0294
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0266 - mae: 0.1217 - mse: 0.0266
64/86 [=====================>........] - ETA: 0s - loss: 0.0249 - mae: 0.1186 - mse: 0.0249
86/86 [==============================] - 0s 4ms/step - loss: 0.0257 - mae: 0.1189 - mse: 0.0257 - val_loss: 0.0309 - val_mae: 0.1481 - val_mse: 0.0309
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0258 - mae: 0.1083 - mse: 0.0258
64/86 [=====================>........] - ETA: 0s - loss: 0.0230 - mae: 0.1078 - mse: 0.0230
86/86 [==============================] - 0s 6ms/step - loss: 0.0242 - mae: 0.1144 - mse: 0.0242 - val_loss: 0.0194 - val_mae: 0.1184 - val_mse: 0.0194
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0231 - mae: 0.1108 - mse: 0.0231
64/86 [=====================>........] - ETA: 0s - loss: 0.0254 - mae: 0.1192 - mse: 0.0254
86/86 [==============================] - 0s 5ms/step - loss: 0.0234 - mae: 0.1149 - mse: 0.0234 - val_loss: 0.0330 - val_mae: 0.1529 - val_mse: 0.0330
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0224 - mae: 0.1171 - mse: 0.0224
64/86 [=====================>........] - ETA: 0s - loss: 0.0216 - mae: 0.1155 - mse: 0.0216
86/86 [==============================] - 0s 6ms/step - loss: 0.0232 - mae: 0.1140 - mse: 0.0232 - val_loss: 0.0252 - val_mae: 0.1301 - val_mse: 0.0252
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0128 - mae: 0.0878 - mse: 0.0128
64/86 [=====================>........] - ETA: 0s - loss: 0.0138 - mae: 0.0932 - mse: 0.0138
86/86 [==============================] - 1s 6ms/step - loss: 0.0146 - mae: 0.0981 - mse: 0.0146 - val_loss: 0.0105 - val_mae: 0.0815 - val_mse: 0.0105
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0178 - mae: 0.1040 - mse: 0.0178
64/86 [=====================>........] - ETA: 0s - loss: 0.0159 - mae: 0.1001 - mse: 0.0159
86/86 [==============================] - 0s 5ms/step - loss: 0.0167 - mae: 0.1019 - mse: 0.0167 - val_loss: 0.0212 - val_mae: 0.1195 - val_mse: 0.0212
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0264 - mae: 0.1140 - mse: 0.0264
64/86 [=====================>........] - ETA: 0s - loss: 0.0200 - mae: 0.1001 - mse: 0.0200
86/86 [==============================] - 0s 5ms/step - loss: 0.0207 - mae: 0.1035 - mse: 0.0207 - val_loss: 0.0294 - val_mae: 0.1417 - val_mse: 0.0294
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0137 - mae: 0.0903 - mse: 0.0137
64/86 [=====================>........] - ETA: 0s - loss: 0.0200 - mae: 0.1038 - mse: 0.0200
86/86 [==============================] - 0s 5ms/step - loss: 0.0176 - mae: 0.0990 - mse: 0.0176 - val_loss: 0.0120 - val_mae: 0.0885 - val_mse: 0.0120
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0110 - mae: 0.0768 - mse: 0.0110
64/86 [=====================>........] - ETA: 0s - loss: 0.0154 - mae: 0.0921 - mse: 0.0154
86/86 [==============================] - 0s 6ms/step - loss: 0.0189 - mae: 0.1024 - mse: 0.0189 - val_loss: 0.0111 - val_mae: 0.0876 - val_mse: 0.0111
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0141 - mae: 0.0827 - mse: 0.0141
64/86 [=====================>........] - ETA: 0s - loss: 0.0139 - mae: 0.0869 - mse: 0.0139
86/86 [==============================] - 0s 5ms/step - loss: 0.0165 - mae: 0.0928 - mse: 0.0165 - val_loss: 0.0225 - val_mae: 0.1251 - val_mse: 0.0225
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0119 - mae: 0.0769 - mse: 0.0119
64/86 [=====================>........] - ETA: 0s - loss: 0.0147 - mae: 0.0848 - mse: 0.0147
86/86 [==============================] - 0s 5ms/step - loss: 0.0139 - mae: 0.0840 - mse: 0.0139 - val_loss: 0.0176 - val_mae: 0.1101 - val_mse: 0.0176
Saving trained model...
99
Testing...
heightdiff= [0.         0.         0.         7.01729202 0.         0.        ]
average prediction= [2.6912234]
baseline= 10.5
eachuser= [0. 0. 0. 8. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.8771615028381348
85 -:- nan
60 -:- nan
['train-weight-9.py', '1']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
65 7
2_155_65_9_csi_a9_19.dat
65 9
65 10
65 11
65 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
65 15
2_155_65_9_csi_a9_7.dat
65 17
65 18
65 19
2_155_65_9_csi_a9_21.dat
65 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
65 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_9_csi_a9_29.dat
65 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
65 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
65 52
1_165_65_9_csi_a9_16.dat
65 54
65 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
65 58
65 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
65 62
65 63
1_165_65_9_csi_a9_5.dat
65 65
65 66
1_165_65_9_csi_a9_18.dat
65 68
65 69
1_165_65_9_csi_a9_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_9_csi_a9_22.dat
50 90
50 91
50 92
50 93
50 94
2_165_50_9_csi_a9_26.dat
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
1_180_85_9_csi_a9_7.dat
85 132
85 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
85 140
85 141
85 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
85 146
85 147
85 148
85 149
85 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
85 153
85 154
85 155
1_180_85_9_csi_a9_18.dat
85 157
85 158
85 159
85 160
75 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
75 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
75 171
1_180_75_9_csi_a9_25.dat
75 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
75 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
75 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
75 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
85 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
85 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
85 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
85 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
85 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 60 60 60 65 65 65
 65 65 65 65 65 65 65 65 65 65 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75 85 85 85 85
 85]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.3350 - mae: 0.4789 - mse: 0.3350
64/86 [=====================>........] - ETA: 0s - loss: 0.3157 - mae: 0.4730 - mse: 0.3157
86/86 [==============================] - 1s 9ms/step - loss: 0.2863 - mae: 0.4489 - mse: 0.2863 - val_loss: 0.1232 - val_mae: 0.2959 - val_mse: 0.1232
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1604 - mae: 0.3530 - mse: 0.1604
64/86 [=====================>........] - ETA: 0s - loss: 0.1230 - mae: 0.2887 - mse: 0.1230
86/86 [==============================] - 0s 6ms/step - loss: 0.1428 - mae: 0.3107 - mse: 0.1428 - val_loss: 0.1367 - val_mae: 0.3302 - val_mse: 0.1367
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1654 - mae: 0.3445 - mse: 0.1654
64/86 [=====================>........] - ETA: 0s - loss: 0.1594 - mae: 0.3302 - mse: 0.1594
86/86 [==============================] - 1s 6ms/step - loss: 0.1460 - mae: 0.3121 - mse: 0.1460 - val_loss: 0.0821 - val_mae: 0.2530 - val_mse: 0.0821
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0951 - mae: 0.2512 - mse: 0.0951
64/86 [=====================>........] - ETA: 0s - loss: 0.0994 - mae: 0.2666 - mse: 0.0994
86/86 [==============================] - 1s 6ms/step - loss: 0.0975 - mae: 0.2641 - mse: 0.0975 - val_loss: 0.0714 - val_mae: 0.2158 - val_mse: 0.0714
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1078 - mae: 0.2901 - mse: 0.1078
64/86 [=====================>........] - ETA: 0s - loss: 0.1016 - mae: 0.2837 - mse: 0.1016
86/86 [==============================] - 1s 6ms/step - loss: 0.0903 - mae: 0.2655 - mse: 0.0903 - val_loss: 0.0603 - val_mae: 0.2042 - val_mse: 0.0603
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0565 - mae: 0.1997 - mse: 0.0565
64/86 [=====================>........] - ETA: 0s - loss: 0.0694 - mae: 0.2297 - mse: 0.0694
86/86 [==============================] - 1s 6ms/step - loss: 0.0723 - mae: 0.2337 - mse: 0.0723 - val_loss: 0.0486 - val_mae: 0.1907 - val_mse: 0.0486
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0627 - mae: 0.2125 - mse: 0.0627
64/86 [=====================>........] - ETA: 0s - loss: 0.0650 - mae: 0.2170 - mse: 0.0650
86/86 [==============================] - 1s 6ms/step - loss: 0.0610 - mae: 0.2068 - mse: 0.0610 - val_loss: 0.0514 - val_mae: 0.2024 - val_mse: 0.0514
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0624 - mae: 0.2049 - mse: 0.0624
64/86 [=====================>........] - ETA: 0s - loss: 0.0712 - mae: 0.2150 - mse: 0.0712
86/86 [==============================] - 0s 6ms/step - loss: 0.0607 - mae: 0.1933 - mse: 0.0607 - val_loss: 0.0445 - val_mae: 0.1828 - val_mse: 0.0445
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0459 - mae: 0.1857 - mse: 0.0459
64/86 [=====================>........] - ETA: 0s - loss: 0.0528 - mae: 0.1923 - mse: 0.0528
86/86 [==============================] - 0s 6ms/step - loss: 0.0527 - mae: 0.1939 - mse: 0.0527 - val_loss: 0.0391 - val_mae: 0.1681 - val_mse: 0.0391
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0652 - mae: 0.2214 - mse: 0.0652
64/86 [=====================>........] - ETA: 0s - loss: 0.0511 - mae: 0.1916 - mse: 0.0511
86/86 [==============================] - 0s 5ms/step - loss: 0.0477 - mae: 0.1830 - mse: 0.0477 - val_loss: 0.0371 - val_mae: 0.1618 - val_mse: 0.0371
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0588 - mae: 0.2036 - mse: 0.0588
64/86 [=====================>........] - ETA: 0s - loss: 0.0520 - mae: 0.1881 - mse: 0.0520
86/86 [==============================] - 0s 5ms/step - loss: 0.0482 - mae: 0.1794 - mse: 0.0482 - val_loss: 0.0351 - val_mae: 0.1545 - val_mse: 0.0351
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0388 - mae: 0.1729 - mse: 0.0388
64/86 [=====================>........] - ETA: 0s - loss: 0.0357 - mae: 0.1599 - mse: 0.0357
86/86 [==============================] - 0s 5ms/step - loss: 0.0357 - mae: 0.1580 - mse: 0.0357 - val_loss: 0.0319 - val_mae: 0.1445 - val_mse: 0.0319
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0472 - mae: 0.1819 - mse: 0.0472
64/86 [=====================>........] - ETA: 0s - loss: 0.0465 - mae: 0.1665 - mse: 0.0465
86/86 [==============================] - 0s 5ms/step - loss: 0.0448 - mae: 0.1650 - mse: 0.0448 - val_loss: 0.0287 - val_mae: 0.1334 - val_mse: 0.0287
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0417 - mae: 0.1622 - mse: 0.0417
64/86 [=====================>........] - ETA: 0s - loss: 0.0392 - mae: 0.1594 - mse: 0.0392
86/86 [==============================] - 0s 5ms/step - loss: 0.0359 - mae: 0.1518 - mse: 0.0359 - val_loss: 0.0276 - val_mae: 0.1229 - val_mse: 0.0276
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0395 - mae: 0.1585 - mse: 0.0395
64/86 [=====================>........] - ETA: 0s - loss: 0.0383 - mae: 0.1489 - mse: 0.0383
86/86 [==============================] - 0s 5ms/step - loss: 0.0378 - mae: 0.1465 - mse: 0.0378 - val_loss: 0.0252 - val_mae: 0.1101 - val_mse: 0.0252
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0452 - mae: 0.1729 - mse: 0.0452
64/86 [=====================>........] - ETA: 0s - loss: 0.0342 - mae: 0.1415 - mse: 0.0342
86/86 [==============================] - 0s 5ms/step - loss: 0.0323 - mae: 0.1405 - mse: 0.0323 - val_loss: 0.0218 - val_mae: 0.1102 - val_mse: 0.0218
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0404 - mae: 0.1504 - mse: 0.0404
64/86 [=====================>........] - ETA: 0s - loss: 0.0328 - mae: 0.1307 - mse: 0.0328
86/86 [==============================] - 0s 5ms/step - loss: 0.0306 - mae: 0.1269 - mse: 0.0306 - val_loss: 0.0206 - val_mae: 0.1051 - val_mse: 0.0206
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0371 - mae: 0.1415 - mse: 0.0371
64/86 [=====================>........] - ETA: 0s - loss: 0.0359 - mae: 0.1382 - mse: 0.0359
86/86 [==============================] - 0s 5ms/step - loss: 0.0318 - mae: 0.1332 - mse: 0.0318 - val_loss: 0.0194 - val_mae: 0.1068 - val_mse: 0.0194
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0364 - mae: 0.1447 - mse: 0.0364
64/86 [=====================>........] - ETA: 0s - loss: 0.0265 - mae: 0.1202 - mse: 0.0265
86/86 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.1126 - mse: 0.0241 - val_loss: 0.0178 - val_mae: 0.0957 - val_mse: 0.0178
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0185 - mae: 0.0950 - mse: 0.0185
64/86 [=====================>........] - ETA: 0s - loss: 0.0148 - mae: 0.0877 - mse: 0.0148
86/86 [==============================] - 0s 6ms/step - loss: 0.0178 - mae: 0.0959 - mse: 0.0178 - val_loss: 0.0171 - val_mae: 0.1033 - val_mse: 0.0171
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0249 - mae: 0.1109 - mse: 0.0249
64/86 [=====================>........] - ETA: 0s - loss: 0.0362 - mae: 0.1340 - mse: 0.0362
86/86 [==============================] - 0s 5ms/step - loss: 0.0370 - mae: 0.1396 - mse: 0.0370 - val_loss: 0.0182 - val_mae: 0.1051 - val_mse: 0.0182
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0244 - mae: 0.1172 - mse: 0.0244
64/86 [=====================>........] - ETA: 0s - loss: 0.0242 - mae: 0.1135 - mse: 0.0242
86/86 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.1145 - mse: 0.0243 - val_loss: 0.0123 - val_mae: 0.0795 - val_mse: 0.0123
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0128 - mae: 0.0895 - mse: 0.0128
64/86 [=====================>........] - ETA: 0s - loss: 0.0177 - mae: 0.1013 - mse: 0.0177
86/86 [==============================] - 0s 5ms/step - loss: 0.0184 - mae: 0.1010 - mse: 0.0184 - val_loss: 0.0113 - val_mae: 0.0750 - val_mse: 0.0113
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0173 - mae: 0.0994 - mse: 0.0173
64/86 [=====================>........] - ETA: 0s - loss: 0.0227 - mae: 0.1089 - mse: 0.0227
86/86 [==============================] - 0s 5ms/step - loss: 0.0214 - mae: 0.1074 - mse: 0.0214 - val_loss: 0.0121 - val_mae: 0.0883 - val_mse: 0.0121
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0165 - mae: 0.0980 - mse: 0.0165
64/86 [=====================>........] - ETA: 0s - loss: 0.0149 - mae: 0.0931 - mse: 0.0149
86/86 [==============================] - 0s 5ms/step - loss: 0.0158 - mae: 0.0961 - mse: 0.0158 - val_loss: 0.0120 - val_mae: 0.0892 - val_mse: 0.0120
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0083 - mae: 0.0709 - mse: 0.0083
64/86 [=====================>........] - ETA: 0s - loss: 0.0171 - mae: 0.0913 - mse: 0.0171
86/86 [==============================] - 0s 5ms/step - loss: 0.0223 - mae: 0.1035 - mse: 0.0223 - val_loss: 0.0112 - val_mae: 0.0838 - val_mse: 0.0112
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0223 - mae: 0.1031 - mse: 0.0223
64/86 [=====================>........] - ETA: 0s - loss: 0.0203 - mae: 0.0981 - mse: 0.0203
86/86 [==============================] - 0s 5ms/step - loss: 0.0183 - mae: 0.0935 - mse: 0.0183 - val_loss: 0.0099 - val_mae: 0.0754 - val_mse: 0.0099
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0133 - mae: 0.0913 - mse: 0.0133
64/86 [=====================>........] - ETA: 0s - loss: 0.0169 - mae: 0.0945 - mse: 0.0169
86/86 [==============================] - 0s 5ms/step - loss: 0.0212 - mae: 0.1045 - mse: 0.0212 - val_loss: 0.0096 - val_mae: 0.0785 - val_mse: 0.0096
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0280 - mae: 0.1158 - mse: 0.0280
64/86 [=====================>........] - ETA: 0s - loss: 0.0225 - mae: 0.1046 - mse: 0.0225
86/86 [==============================] - 0s 6ms/step - loss: 0.0180 - mae: 0.0921 - mse: 0.0180 - val_loss: 0.0084 - val_mae: 0.0722 - val_mse: 0.0084
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0284 - mae: 0.1125 - mse: 0.0284
64/86 [=====================>........] - ETA: 0s - loss: 0.0235 - mae: 0.1023 - mse: 0.0235
86/86 [==============================] - 0s 5ms/step - loss: 0.0225 - mae: 0.1029 - mse: 0.0225 - val_loss: 0.0073 - val_mae: 0.0622 - val_mse: 0.0073
Saving trained model...
99
Testing...
heightdiff= [0.         0.         0.         3.25763702 0.         0.        ]
average prediction= [2.7742066]
baseline= 8.7
eachuser= [0. 0. 0. 5. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.6515274047851562
85 -:- nan
60 -:- nan
['train-weight-9.py', '1']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
65 7
2_155_65_9_csi_a9_19.dat
65 9
65 10
65 11
65 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
65 15
2_155_65_9_csi_a9_7.dat
65 17
65 18
65 19
2_155_65_9_csi_a9_21.dat
65 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
65 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_9_csi_a9_29.dat
65 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
65 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
65 52
1_165_65_9_csi_a9_16.dat
65 54
65 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
65 58
65 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
65 62
65 63
1_165_65_9_csi_a9_5.dat
65 65
65 66
1_165_65_9_csi_a9_18.dat
65 68
65 69
1_165_65_9_csi_a9_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_9_csi_a9_22.dat
50 90
50 91
50 92
50 93
50 94
2_165_50_9_csi_a9_26.dat
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
1_180_85_9_csi_a9_7.dat
85 132
85 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
85 140
85 141
85 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
85 146
85 147
85 148
85 149
85 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
85 153
85 154
85 155
1_180_85_9_csi_a9_18.dat
85 157
85 158
85 159
85 160
75 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
75 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
75 171
1_180_75_9_csi_a9_25.dat
75 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
75 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
75 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
75 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
85 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
85 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
85 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
85 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
85 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 60 60 60 65 65 65
 65 65 65 65 65 65 65 65 65 65 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75 85 85 85 85
 85]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.4721 - mae: 0.5802 - mse: 0.4721
64/86 [=====================>........] - ETA: 0s - loss: 0.3567 - mae: 0.4853 - mse: 0.3567
86/86 [==============================] - 1s 10ms/step - loss: 0.3187 - mae: 0.4514 - mse: 0.3187 - val_loss: 0.1039 - val_mae: 0.2412 - val_mse: 0.1039
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1427 - mae: 0.2930 - mse: 0.1427
64/86 [=====================>........] - ETA: 0s - loss: 0.1359 - mae: 0.2882 - mse: 0.1359
86/86 [==============================] - 1s 6ms/step - loss: 0.1283 - mae: 0.2860 - mse: 0.1283 - val_loss: 0.1106 - val_mae: 0.2743 - val_mse: 0.1106
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1550 - mae: 0.3531 - mse: 0.1550
64/86 [=====================>........] - ETA: 0s - loss: 0.1529 - mae: 0.3544 - mse: 0.1529
86/86 [==============================] - 1s 6ms/step - loss: 0.1448 - mae: 0.3407 - mse: 0.1448 - val_loss: 0.0776 - val_mae: 0.1915 - val_mse: 0.0776
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1001 - mae: 0.2759 - mse: 0.1001
64/86 [=====================>........] - ETA: 0s - loss: 0.0930 - mae: 0.2597 - mse: 0.0930
86/86 [==============================] - 1s 6ms/step - loss: 0.0914 - mae: 0.2550 - mse: 0.0914 - val_loss: 0.0727 - val_mae: 0.1800 - val_mse: 0.0727
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0986 - mae: 0.2368 - mse: 0.0986
64/86 [=====================>........] - ETA: 0s - loss: 0.0883 - mae: 0.2345 - mse: 0.0883
86/86 [==============================] - 1s 6ms/step - loss: 0.0811 - mae: 0.2188 - mse: 0.0811 - val_loss: 0.0781 - val_mae: 0.2089 - val_mse: 0.0781
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0554 - mae: 0.1988 - mse: 0.0554
64/86 [=====================>........] - ETA: 0s - loss: 0.0660 - mae: 0.2068 - mse: 0.0660
86/86 [==============================] - 0s 6ms/step - loss: 0.0822 - mae: 0.2310 - mse: 0.0822 - val_loss: 0.0702 - val_mae: 0.2058 - val_mse: 0.0702
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0929 - mae: 0.2498 - mse: 0.0929
64/86 [=====================>........] - ETA: 0s - loss: 0.0838 - mae: 0.2355 - mse: 0.0838
86/86 [==============================] - 1s 6ms/step - loss: 0.0765 - mae: 0.2276 - mse: 0.0765 - val_loss: 0.0524 - val_mae: 0.1764 - val_mse: 0.0524
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0857 - mae: 0.2442 - mse: 0.0857
64/86 [=====================>........] - ETA: 0s - loss: 0.0721 - mae: 0.2258 - mse: 0.0721
86/86 [==============================] - 1s 6ms/step - loss: 0.0692 - mae: 0.2204 - mse: 0.0692 - val_loss: 0.0463 - val_mae: 0.1725 - val_mse: 0.0463
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0562 - mae: 0.2029 - mse: 0.0562
64/86 [=====================>........] - ETA: 0s - loss: 0.0544 - mae: 0.1929 - mse: 0.0544
86/86 [==============================] - 1s 6ms/step - loss: 0.0599 - mae: 0.1979 - mse: 0.0599 - val_loss: 0.0465 - val_mae: 0.1778 - val_mse: 0.0465
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0587 - mae: 0.2086 - mse: 0.0587
64/86 [=====================>........] - ETA: 0s - loss: 0.0501 - mae: 0.1861 - mse: 0.0501
86/86 [==============================] - 1s 6ms/step - loss: 0.0478 - mae: 0.1820 - mse: 0.0478 - val_loss: 0.0542 - val_mae: 0.2050 - val_mse: 0.0542
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0420 - mae: 0.1713 - mse: 0.0420
64/86 [=====================>........] - ETA: 0s - loss: 0.0452 - mae: 0.1810 - mse: 0.0452
86/86 [==============================] - 1s 6ms/step - loss: 0.0423 - mae: 0.1752 - mse: 0.0423 - val_loss: 0.0523 - val_mae: 0.2056 - val_mse: 0.0523
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0593 - mae: 0.1814 - mse: 0.0593
64/86 [=====================>........] - ETA: 0s - loss: 0.0524 - mae: 0.1771 - mse: 0.0524
86/86 [==============================] - 1s 6ms/step - loss: 0.0475 - mae: 0.1666 - mse: 0.0475 - val_loss: 0.0452 - val_mae: 0.1898 - val_mse: 0.0452
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0451 - mae: 0.1682 - mse: 0.0451
64/86 [=====================>........] - ETA: 0s - loss: 0.0448 - mae: 0.1595 - mse: 0.0448
86/86 [==============================] - 1s 6ms/step - loss: 0.0393 - mae: 0.1490 - mse: 0.0393 - val_loss: 0.0425 - val_mae: 0.1842 - val_mse: 0.0425
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0258 - mae: 0.1301 - mse: 0.0258
64/86 [=====================>........] - ETA: 0s - loss: 0.0310 - mae: 0.1375 - mse: 0.0310
86/86 [==============================] - 1s 6ms/step - loss: 0.0458 - mae: 0.1615 - mse: 0.0458 - val_loss: 0.0423 - val_mae: 0.1871 - val_mse: 0.0423
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0314 - mae: 0.1459 - mse: 0.0314
64/86 [=====================>........] - ETA: 0s - loss: 0.0396 - mae: 0.1491 - mse: 0.0396
86/86 [==============================] - 1s 6ms/step - loss: 0.0416 - mae: 0.1540 - mse: 0.0416 - val_loss: 0.0458 - val_mae: 0.1957 - val_mse: 0.0458
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0406 - mae: 0.1559 - mse: 0.0406
64/86 [=====================>........] - ETA: 0s - loss: 0.0391 - mae: 0.1532 - mse: 0.0391
86/86 [==============================] - 0s 6ms/step - loss: 0.0373 - mae: 0.1482 - mse: 0.0373 - val_loss: 0.0422 - val_mae: 0.1871 - val_mse: 0.0422
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0286 - mae: 0.1332 - mse: 0.0286
64/86 [=====================>........] - ETA: 0s - loss: 0.0341 - mae: 0.1375 - mse: 0.0341
86/86 [==============================] - 1s 6ms/step - loss: 0.0331 - mae: 0.1366 - mse: 0.0331 - val_loss: 0.0315 - val_mae: 0.1634 - val_mse: 0.0315
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0343 - mae: 0.1485 - mse: 0.0343
64/86 [=====================>........] - ETA: 0s - loss: 0.0345 - mae: 0.1492 - mse: 0.0345
86/86 [==============================] - 1s 6ms/step - loss: 0.0289 - mae: 0.1348 - mse: 0.0289 - val_loss: 0.0214 - val_mae: 0.1335 - val_mse: 0.0214
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0388 - mae: 0.1418 - mse: 0.0388
64/86 [=====================>........] - ETA: 0s - loss: 0.0317 - mae: 0.1320 - mse: 0.0317
86/86 [==============================] - 1s 6ms/step - loss: 0.0301 - mae: 0.1278 - mse: 0.0301 - val_loss: 0.0205 - val_mae: 0.1296 - val_mse: 0.0205
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0265 - mae: 0.1173 - mse: 0.0265
64/86 [=====================>........] - ETA: 0s - loss: 0.0233 - mae: 0.1073 - mse: 0.0233
86/86 [==============================] - 1s 6ms/step - loss: 0.0221 - mae: 0.1074 - mse: 0.0221 - val_loss: 0.0265 - val_mae: 0.1429 - val_mse: 0.0265
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0257 - mae: 0.1225 - mse: 0.0257
64/86 [=====================>........] - ETA: 0s - loss: 0.0222 - mae: 0.1094 - mse: 0.0222
86/86 [==============================] - 1s 6ms/step - loss: 0.0209 - mae: 0.1081 - mse: 0.0209 - val_loss: 0.0223 - val_mae: 0.1327 - val_mse: 0.0223
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0257 - mae: 0.1140 - mse: 0.0257
64/86 [=====================>........] - ETA: 0s - loss: 0.0189 - mae: 0.0969 - mse: 0.0189
86/86 [==============================] - 0s 6ms/step - loss: 0.0194 - mae: 0.1005 - mse: 0.0194 - val_loss: 0.0153 - val_mae: 0.1091 - val_mse: 0.0153
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0234 - mae: 0.1014 - mse: 0.0234
64/86 [=====================>........] - ETA: 0s - loss: 0.0166 - mae: 0.0867 - mse: 0.0166
86/86 [==============================] - 1s 6ms/step - loss: 0.0179 - mae: 0.0930 - mse: 0.0179 - val_loss: 0.0185 - val_mae: 0.1165 - val_mse: 0.0185
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0143 - mae: 0.0880 - mse: 0.0143
64/86 [=====================>........] - ETA: 0s - loss: 0.0181 - mae: 0.0980 - mse: 0.0181
86/86 [==============================] - 0s 6ms/step - loss: 0.0185 - mae: 0.0993 - mse: 0.0185 - val_loss: 0.0208 - val_mae: 0.1222 - val_mse: 0.0208
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0149 - mae: 0.0941 - mse: 0.0149
64/86 [=====================>........] - ETA: 0s - loss: 0.0177 - mae: 0.0928 - mse: 0.0177
86/86 [==============================] - 0s 6ms/step - loss: 0.0190 - mae: 0.0966 - mse: 0.0190 - val_loss: 0.0092 - val_mae: 0.0794 - val_mse: 0.0092
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0190 - mae: 0.1170 - mse: 0.0190
64/86 [=====================>........] - ETA: 0s - loss: 0.0165 - mae: 0.1039 - mse: 0.0165
86/86 [==============================] - 1s 6ms/step - loss: 0.0172 - mae: 0.1041 - mse: 0.0172 - val_loss: 0.0081 - val_mae: 0.0743 - val_mse: 0.0081
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0162 - mae: 0.1010 - mse: 0.0162
64/86 [=====================>........] - ETA: 0s - loss: 0.0266 - mae: 0.1185 - mse: 0.0266
86/86 [==============================] - 1s 6ms/step - loss: 0.0245 - mae: 0.1144 - mse: 0.0245 - val_loss: 0.0243 - val_mae: 0.1306 - val_mse: 0.0243
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0164 - mae: 0.1016 - mse: 0.0164
64/86 [=====================>........] - ETA: 0s - loss: 0.0208 - mae: 0.1082 - mse: 0.0208
86/86 [==============================] - 1s 6ms/step - loss: 0.0215 - mae: 0.1089 - mse: 0.0215 - val_loss: 0.0311 - val_mae: 0.1456 - val_mse: 0.0311
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0162 - mae: 0.0990 - mse: 0.0162
64/86 [=====================>........] - ETA: 0s - loss: 0.0162 - mae: 0.0984 - mse: 0.0162
86/86 [==============================] - 1s 6ms/step - loss: 0.0159 - mae: 0.0983 - mse: 0.0159 - val_loss: 0.0127 - val_mae: 0.0965 - val_mse: 0.0127
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0252 - mae: 0.1041 - mse: 0.0252
64/86 [=====================>........] - ETA: 0s - loss: 0.0199 - mae: 0.0991 - mse: 0.0199
86/86 [==============================] - 1s 6ms/step - loss: 0.0257 - mae: 0.1102 - mse: 0.0257 - val_loss: 0.0095 - val_mae: 0.0803 - val_mse: 0.0095
Saving trained model...
99
Testing...
heightdiff= [0.         0.         0.         2.16975403 0.         0.        ]
average prediction= [3.8469648]
baseline= 8.9
eachuser= [0. 0. 0. 5. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.4339508056640625
85 -:- nan
60 -:- nan
['train-weight-9.py', '1']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
65 7
2_155_65_9_csi_a9_19.dat
65 9
65 10
65 11
65 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
65 15
2_155_65_9_csi_a9_7.dat
65 17
65 18
65 19
2_155_65_9_csi_a9_21.dat
65 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
65 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_9_csi_a9_29.dat
65 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
65 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
65 52
1_165_65_9_csi_a9_16.dat
65 54
65 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
65 58
65 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
65 62
65 63
1_165_65_9_csi_a9_5.dat
65 65
65 66
1_165_65_9_csi_a9_18.dat
65 68
65 69
1_165_65_9_csi_a9_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_9_csi_a9_22.dat
50 90
50 91
50 92
50 93
50 94
2_165_50_9_csi_a9_26.dat
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
1_180_85_9_csi_a9_7.dat
85 132
85 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
85 140
85 141
85 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
85 146
85 147
85 148
85 149
85 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
85 153
85 154
85 155
1_180_85_9_csi_a9_18.dat
85 157
85 158
85 159
85 160
75 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
75 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
75 171
1_180_75_9_csi_a9_25.dat
75 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
75 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
75 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
75 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
85 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
85 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
85 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
85 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
85 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 60 60 60 65 65 65
 65 65 65 65 65 65 65 65 65 65 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75 85 85 85 85
 85]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.2368 - mae: 0.3936 - mse: 0.2368
64/86 [=====================>........] - ETA: 0s - loss: 0.2192 - mae: 0.3892 - mse: 0.2192
86/86 [==============================] - 1s 9ms/step - loss: 0.1989 - mae: 0.3721 - mse: 0.1989 - val_loss: 0.1937 - val_mae: 0.3819 - val_mse: 0.1937
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1396 - mae: 0.3151 - mse: 0.1396
64/86 [=====================>........] - ETA: 0s - loss: 0.1640 - mae: 0.3340 - mse: 0.1640
86/86 [==============================] - 0s 6ms/step - loss: 0.1577 - mae: 0.3255 - mse: 0.1577 - val_loss: 0.1374 - val_mae: 0.3184 - val_mse: 0.1374
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0966 - mae: 0.2483 - mse: 0.0966
64/86 [=====================>........] - ETA: 0s - loss: 0.0871 - mae: 0.2380 - mse: 0.0871
86/86 [==============================] - 0s 6ms/step - loss: 0.1008 - mae: 0.2606 - mse: 0.1008 - val_loss: 0.1101 - val_mae: 0.2731 - val_mse: 0.1101
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0846 - mae: 0.2348 - mse: 0.0846
64/86 [=====================>........] - ETA: 0s - loss: 0.0810 - mae: 0.2292 - mse: 0.0810
86/86 [==============================] - 1s 6ms/step - loss: 0.0769 - mae: 0.2280 - mse: 0.0769 - val_loss: 0.0844 - val_mae: 0.2451 - val_mse: 0.0844
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0792 - mae: 0.2497 - mse: 0.0792
64/86 [=====================>........] - ETA: 0s - loss: 0.0742 - mae: 0.2363 - mse: 0.0742
86/86 [==============================] - 0s 6ms/step - loss: 0.0697 - mae: 0.2260 - mse: 0.0697 - val_loss: 0.0655 - val_mae: 0.2273 - val_mse: 0.0655
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0583 - mae: 0.2082 - mse: 0.0583
64/86 [=====================>........] - ETA: 0s - loss: 0.0573 - mae: 0.2038 - mse: 0.0573
86/86 [==============================] - 1s 6ms/step - loss: 0.0584 - mae: 0.2061 - mse: 0.0584 - val_loss: 0.0569 - val_mae: 0.2097 - val_mse: 0.0569
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0657 - mae: 0.2162 - mse: 0.0657
64/86 [=====================>........] - ETA: 0s - loss: 0.0709 - mae: 0.2225 - mse: 0.0709
86/86 [==============================] - 0s 6ms/step - loss: 0.0630 - mae: 0.2079 - mse: 0.0630 - val_loss: 0.0577 - val_mae: 0.1835 - val_mse: 0.0577
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0674 - mae: 0.2206 - mse: 0.0674
64/86 [=====================>........] - ETA: 0s - loss: 0.0613 - mae: 0.2096 - mse: 0.0613
86/86 [==============================] - 0s 6ms/step - loss: 0.0548 - mae: 0.1989 - mse: 0.0548 - val_loss: 0.0581 - val_mae: 0.1712 - val_mse: 0.0581
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0488 - mae: 0.1819 - mse: 0.0488
64/86 [=====================>........] - ETA: 0s - loss: 0.0512 - mae: 0.1821 - mse: 0.0512
86/86 [==============================] - 1s 6ms/step - loss: 0.0505 - mae: 0.1841 - mse: 0.0505 - val_loss: 0.0501 - val_mae: 0.1700 - val_mse: 0.0501
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0529 - mae: 0.1793 - mse: 0.0529
64/86 [=====================>........] - ETA: 0s - loss: 0.0509 - mae: 0.1765 - mse: 0.0509
86/86 [==============================] - 0s 5ms/step - loss: 0.0550 - mae: 0.1854 - mse: 0.0550 - val_loss: 0.0466 - val_mae: 0.1656 - val_mse: 0.0466
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0798 - mae: 0.2135 - mse: 0.0798
64/86 [=====================>........] - ETA: 0s - loss: 0.0558 - mae: 0.1741 - mse: 0.0558
86/86 [==============================] - 0s 6ms/step - loss: 0.0531 - mae: 0.1703 - mse: 0.0531 - val_loss: 0.0533 - val_mae: 0.1674 - val_mse: 0.0533
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0480 - mae: 0.1728 - mse: 0.0480
64/86 [=====================>........] - ETA: 0s - loss: 0.0434 - mae: 0.1638 - mse: 0.0434
86/86 [==============================] - 1s 6ms/step - loss: 0.0422 - mae: 0.1650 - mse: 0.0422 - val_loss: 0.0435 - val_mae: 0.1569 - val_mse: 0.0435
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0625 - mae: 0.1978 - mse: 0.0625
64/86 [=====================>........] - ETA: 0s - loss: 0.0502 - mae: 0.1805 - mse: 0.0502
86/86 [==============================] - 0s 6ms/step - loss: 0.0453 - mae: 0.1702 - mse: 0.0453 - val_loss: 0.0370 - val_mae: 0.1521 - val_mse: 0.0370
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0417 - mae: 0.1678 - mse: 0.0417
64/86 [=====================>........] - ETA: 0s - loss: 0.0389 - mae: 0.1625 - mse: 0.0389
86/86 [==============================] - 1s 6ms/step - loss: 0.0427 - mae: 0.1683 - mse: 0.0427 - val_loss: 0.0337 - val_mae: 0.1442 - val_mse: 0.0337
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0402 - mae: 0.1539 - mse: 0.0402
64/86 [=====================>........] - ETA: 0s - loss: 0.0442 - mae: 0.1679 - mse: 0.0442
86/86 [==============================] - 0s 6ms/step - loss: 0.0450 - mae: 0.1713 - mse: 0.0450 - val_loss: 0.0354 - val_mae: 0.1461 - val_mse: 0.0354
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0223 - mae: 0.1338 - mse: 0.0223
64/86 [=====================>........] - ETA: 0s - loss: 0.0356 - mae: 0.1570 - mse: 0.0356
86/86 [==============================] - 1s 6ms/step - loss: 0.0333 - mae: 0.1498 - mse: 0.0333 - val_loss: 0.0287 - val_mae: 0.1303 - val_mse: 0.0287
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0342 - mae: 0.1346 - mse: 0.0342
64/86 [=====================>........] - ETA: 0s - loss: 0.0320 - mae: 0.1309 - mse: 0.0320
86/86 [==============================] - 1s 6ms/step - loss: 0.0321 - mae: 0.1357 - mse: 0.0321 - val_loss: 0.0273 - val_mae: 0.1269 - val_mse: 0.0273
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0460 - mae: 0.1651 - mse: 0.0460
64/86 [=====================>........] - ETA: 0s - loss: 0.0306 - mae: 0.1333 - mse: 0.0306
86/86 [==============================] - 1s 6ms/step - loss: 0.0308 - mae: 0.1344 - mse: 0.0308 - val_loss: 0.0301 - val_mae: 0.1284 - val_mse: 0.0301
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0366 - mae: 0.1530 - mse: 0.0366
64/86 [=====================>........] - ETA: 0s - loss: 0.0293 - mae: 0.1393 - mse: 0.0293
86/86 [==============================] - 0s 6ms/step - loss: 0.0261 - mae: 0.1294 - mse: 0.0261 - val_loss: 0.0280 - val_mae: 0.1236 - val_mse: 0.0280
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0208 - mae: 0.1227 - mse: 0.0208
64/86 [=====================>........] - ETA: 0s - loss: 0.0246 - mae: 0.1258 - mse: 0.0246
86/86 [==============================] - 1s 6ms/step - loss: 0.0277 - mae: 0.1285 - mse: 0.0277 - val_loss: 0.0253 - val_mae: 0.1165 - val_mse: 0.0253
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0305 - mae: 0.1447 - mse: 0.0305
64/86 [=====================>........] - ETA: 0s - loss: 0.0234 - mae: 0.1244 - mse: 0.0234
86/86 [==============================] - 0s 6ms/step - loss: 0.0214 - mae: 0.1162 - mse: 0.0214 - val_loss: 0.0234 - val_mae: 0.1108 - val_mse: 0.0234
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0216 - mae: 0.1211 - mse: 0.0216
64/86 [=====================>........] - ETA: 0s - loss: 0.0193 - mae: 0.1111 - mse: 0.0193
86/86 [==============================] - 0s 6ms/step - loss: 0.0245 - mae: 0.1233 - mse: 0.0245 - val_loss: 0.0255 - val_mae: 0.1170 - val_mse: 0.0255
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0267 - mae: 0.1233 - mse: 0.0267
64/86 [=====================>........] - ETA: 0s - loss: 0.0217 - mae: 0.1133 - mse: 0.0217
86/86 [==============================] - 1s 6ms/step - loss: 0.0202 - mae: 0.1072 - mse: 0.0202 - val_loss: 0.0251 - val_mae: 0.1229 - val_mse: 0.0251
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0344 - mae: 0.1599 - mse: 0.0344
64/86 [=====================>........] - ETA: 0s - loss: 0.0263 - mae: 0.1280 - mse: 0.0263
86/86 [==============================] - 1s 6ms/step - loss: 0.0257 - mae: 0.1270 - mse: 0.0257 - val_loss: 0.0251 - val_mae: 0.1217 - val_mse: 0.0251
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0216 - mae: 0.1036 - mse: 0.0216
64/86 [=====================>........] - ETA: 0s - loss: 0.0194 - mae: 0.1015 - mse: 0.0194
86/86 [==============================] - 0s 6ms/step - loss: 0.0168 - mae: 0.0966 - mse: 0.0168 - val_loss: 0.0299 - val_mae: 0.1163 - val_mse: 0.0299
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0254 - mae: 0.1274 - mse: 0.0254
64/86 [=====================>........] - ETA: 0s - loss: 0.0266 - mae: 0.1238 - mse: 0.0266
86/86 [==============================] - 1s 6ms/step - loss: 0.0226 - mae: 0.1155 - mse: 0.0226 - val_loss: 0.0294 - val_mae: 0.1190 - val_mse: 0.0294
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0177 - mae: 0.0905 - mse: 0.0177
64/86 [=====================>........] - ETA: 0s - loss: 0.0197 - mae: 0.1069 - mse: 0.0197
86/86 [==============================] - 0s 6ms/step - loss: 0.0212 - mae: 0.1123 - mse: 0.0212 - val_loss: 0.0279 - val_mae: 0.1200 - val_mse: 0.0279
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0368 - mae: 0.1448 - mse: 0.0368
64/86 [=====================>........] - ETA: 0s - loss: 0.0264 - mae: 0.1216 - mse: 0.0264
86/86 [==============================] - 0s 6ms/step - loss: 0.0237 - mae: 0.1134 - mse: 0.0237 - val_loss: 0.0308 - val_mae: 0.1349 - val_mse: 0.0308
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0264 - mae: 0.1214 - mse: 0.0264
64/86 [=====================>........] - ETA: 0s - loss: 0.0197 - mae: 0.1089 - mse: 0.0197
86/86 [==============================] - 1s 6ms/step - loss: 0.0209 - mae: 0.1093 - mse: 0.0209 - val_loss: 0.0301 - val_mae: 0.1226 - val_mse: 0.0301
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0153 - mae: 0.0914 - mse: 0.0153
64/86 [=====================>........] - ETA: 0s - loss: 0.0141 - mae: 0.0888 - mse: 0.0141
86/86 [==============================] - 1s 6ms/step - loss: 0.0161 - mae: 0.0947 - mse: 0.0161 - val_loss: 0.0253 - val_mae: 0.1142 - val_mse: 0.0253
Saving trained model...
99
Testing...
heightdiff= [ 0.          0.          0.         17.28348923  0.          0.        ]
average prediction= [3.377302]
baseline= 10.9
eachuser= [0. 0. 0. 7. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 2.4690698896135603
85 -:- nan
60 -:- nan
['train-weight-9.py', '1']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
65 7
2_155_65_9_csi_a9_19.dat
65 9
65 10
65 11
65 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
65 15
2_155_65_9_csi_a9_7.dat
65 17
65 18
65 19
2_155_65_9_csi_a9_21.dat
65 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
65 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_9_csi_a9_29.dat
65 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
65 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
65 52
1_165_65_9_csi_a9_16.dat
65 54
65 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
65 58
65 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
65 62
65 63
1_165_65_9_csi_a9_5.dat
65 65
65 66
1_165_65_9_csi_a9_18.dat
65 68
65 69
1_165_65_9_csi_a9_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_9_csi_a9_22.dat
50 90
50 91
50 92
50 93
50 94
2_165_50_9_csi_a9_26.dat
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
1_180_85_9_csi_a9_7.dat
85 132
85 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
85 140
85 141
85 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
85 146
85 147
85 148
85 149
85 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
85 153
85 154
85 155
1_180_85_9_csi_a9_18.dat
85 157
85 158
85 159
85 160
75 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
75 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
75 171
1_180_75_9_csi_a9_25.dat
75 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
75 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
75 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
75 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
85 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
85 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
85 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
85 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
85 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 60 60 60 65 65 65
 65 65 65 65 65 65 65 65 65 65 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75 85 85 85 85
 85]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.3481 - mae: 0.5164 - mse: 0.3481
64/86 [=====================>........] - ETA: 0s - loss: 0.3107 - mae: 0.4709 - mse: 0.3107
86/86 [==============================] - 1s 10ms/step - loss: 0.2750 - mae: 0.4417 - mse: 0.2750 - val_loss: 0.1240 - val_mae: 0.3061 - val_mse: 0.1240
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1546 - mae: 0.3337 - mse: 0.1546
64/86 [=====================>........] - ETA: 0s - loss: 0.1203 - mae: 0.2954 - mse: 0.1203
86/86 [==============================] - 1s 6ms/step - loss: 0.1125 - mae: 0.2828 - mse: 0.1125 - val_loss: 0.1303 - val_mae: 0.3006 - val_mse: 0.1303
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1309 - mae: 0.2867 - mse: 0.1309
64/86 [=====================>........] - ETA: 0s - loss: 0.1478 - mae: 0.3108 - mse: 0.1478
86/86 [==============================] - 0s 6ms/step - loss: 0.1249 - mae: 0.2796 - mse: 0.1249 - val_loss: 0.0893 - val_mae: 0.2647 - val_mse: 0.0893
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0898 - mae: 0.2294 - mse: 0.0898
64/86 [=====================>........] - ETA: 0s - loss: 0.0793 - mae: 0.2183 - mse: 0.0793
86/86 [==============================] - 0s 6ms/step - loss: 0.0816 - mae: 0.2316 - mse: 0.0816 - val_loss: 0.0739 - val_mae: 0.2429 - val_mse: 0.0739
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0852 - mae: 0.2509 - mse: 0.0852
64/86 [=====================>........] - ETA: 0s - loss: 0.0746 - mae: 0.2367 - mse: 0.0746
86/86 [==============================] - 1s 6ms/step - loss: 0.0719 - mae: 0.2319 - mse: 0.0719 - val_loss: 0.0845 - val_mae: 0.2247 - val_mse: 0.0845
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0682 - mae: 0.2291 - mse: 0.0682
64/86 [=====================>........] - ETA: 0s - loss: 0.0680 - mae: 0.2251 - mse: 0.0680
86/86 [==============================] - 1s 6ms/step - loss: 0.0734 - mae: 0.2337 - mse: 0.0734 - val_loss: 0.0721 - val_mae: 0.1982 - val_mse: 0.0721
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0713 - mae: 0.2433 - mse: 0.0713
64/86 [=====================>........] - ETA: 0s - loss: 0.0600 - mae: 0.2096 - mse: 0.0600
86/86 [==============================] - 1s 6ms/step - loss: 0.0588 - mae: 0.2071 - mse: 0.0588 - val_loss: 0.0423 - val_mae: 0.1592 - val_mse: 0.0423
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0440 - mae: 0.1617 - mse: 0.0440
64/86 [=====================>........] - ETA: 0s - loss: 0.0499 - mae: 0.1740 - mse: 0.0499
86/86 [==============================] - 0s 6ms/step - loss: 0.0487 - mae: 0.1695 - mse: 0.0487 - val_loss: 0.0245 - val_mae: 0.1163 - val_mse: 0.0245
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0665 - mae: 0.1756 - mse: 0.0665
64/86 [=====================>........] - ETA: 0s - loss: 0.0505 - mae: 0.1598 - mse: 0.0505
86/86 [==============================] - 0s 5ms/step - loss: 0.0548 - mae: 0.1674 - mse: 0.0548 - val_loss: 0.0294 - val_mae: 0.1188 - val_mse: 0.0294
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0509 - mae: 0.1560 - mse: 0.0509
64/86 [=====================>........] - ETA: 0s - loss: 0.0550 - mae: 0.1639 - mse: 0.0550
86/86 [==============================] - 1s 6ms/step - loss: 0.0491 - mae: 0.1583 - mse: 0.0491 - val_loss: 0.0544 - val_mae: 0.1725 - val_mse: 0.0544
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0270 - mae: 0.1376 - mse: 0.0270
64/86 [=====================>........] - ETA: 0s - loss: 0.0428 - mae: 0.1614 - mse: 0.0428
86/86 [==============================] - 1s 6ms/step - loss: 0.0494 - mae: 0.1691 - mse: 0.0494 - val_loss: 0.0551 - val_mae: 0.1747 - val_mse: 0.0551
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0511 - mae: 0.1742 - mse: 0.0511
64/86 [=====================>........] - ETA: 0s - loss: 0.0461 - mae: 0.1646 - mse: 0.0461
86/86 [==============================] - 0s 6ms/step - loss: 0.0473 - mae: 0.1673 - mse: 0.0473 - val_loss: 0.0283 - val_mae: 0.1121 - val_mse: 0.0283
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0443 - mae: 0.1693 - mse: 0.0443
64/86 [=====================>........] - ETA: 0s - loss: 0.0357 - mae: 0.1465 - mse: 0.0357
86/86 [==============================] - 0s 6ms/step - loss: 0.0403 - mae: 0.1537 - mse: 0.0403 - val_loss: 0.0155 - val_mae: 0.0888 - val_mse: 0.0155
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0384 - mae: 0.1425 - mse: 0.0384
64/86 [=====================>........] - ETA: 0s - loss: 0.0536 - mae: 0.1704 - mse: 0.0536
86/86 [==============================] - 0s 5ms/step - loss: 0.0506 - mae: 0.1682 - mse: 0.0506 - val_loss: 0.0149 - val_mae: 0.0819 - val_mse: 0.0149
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0709 - mae: 0.1977 - mse: 0.0709
64/86 [=====================>........] - ETA: 0s - loss: 0.0464 - mae: 0.1541 - mse: 0.0464
86/86 [==============================] - 0s 6ms/step - loss: 0.0417 - mae: 0.1475 - mse: 0.0417 - val_loss: 0.0308 - val_mae: 0.1169 - val_mse: 0.0308
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0351 - mae: 0.1420 - mse: 0.0351
64/86 [=====================>........] - ETA: 0s - loss: 0.0369 - mae: 0.1466 - mse: 0.0369
86/86 [==============================] - 1s 6ms/step - loss: 0.0407 - mae: 0.1579 - mse: 0.0407 - val_loss: 0.0330 - val_mae: 0.1270 - val_mse: 0.0330
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0296 - mae: 0.1360 - mse: 0.0296
64/86 [=====================>........] - ETA: 0s - loss: 0.0306 - mae: 0.1398 - mse: 0.0306
86/86 [==============================] - 0s 6ms/step - loss: 0.0294 - mae: 0.1319 - mse: 0.0294 - val_loss: 0.0184 - val_mae: 0.0915 - val_mse: 0.0184
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0293 - mae: 0.1316 - mse: 0.0293
64/86 [=====================>........] - ETA: 0s - loss: 0.0263 - mae: 0.1200 - mse: 0.0263
86/86 [==============================] - 0s 6ms/step - loss: 0.0293 - mae: 0.1247 - mse: 0.0293 - val_loss: 0.0112 - val_mae: 0.0744 - val_mse: 0.0112
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0408 - mae: 0.1612 - mse: 0.0408
64/86 [=====================>........] - ETA: 0s - loss: 0.0340 - mae: 0.1455 - mse: 0.0340
86/86 [==============================] - 1s 6ms/step - loss: 0.0294 - mae: 0.1300 - mse: 0.0294 - val_loss: 0.0113 - val_mae: 0.0749 - val_mse: 0.0113
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0274 - mae: 0.1179 - mse: 0.0274
64/86 [=====================>........] - ETA: 0s - loss: 0.0352 - mae: 0.1339 - mse: 0.0352
86/86 [==============================] - 0s 5ms/step - loss: 0.0310 - mae: 0.1284 - mse: 0.0310 - val_loss: 0.0138 - val_mae: 0.0859 - val_mse: 0.0138
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0290 - mae: 0.1362 - mse: 0.0290
64/86 [=====================>........] - ETA: 0s - loss: 0.0288 - mae: 0.1333 - mse: 0.0288
86/86 [==============================] - 1s 6ms/step - loss: 0.0272 - mae: 0.1260 - mse: 0.0272 - val_loss: 0.0160 - val_mae: 0.0947 - val_mse: 0.0160
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0346 - mae: 0.1427 - mse: 0.0346
64/86 [=====================>........] - ETA: 0s - loss: 0.0243 - mae: 0.1156 - mse: 0.0243
86/86 [==============================] - 0s 6ms/step - loss: 0.0248 - mae: 0.1166 - mse: 0.0248 - val_loss: 0.0139 - val_mae: 0.0905 - val_mse: 0.0139
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0177 - mae: 0.0960 - mse: 0.0177
64/86 [=====================>........] - ETA: 0s - loss: 0.0247 - mae: 0.1146 - mse: 0.0247
86/86 [==============================] - 0s 6ms/step - loss: 0.0212 - mae: 0.1028 - mse: 0.0212 - val_loss: 0.0094 - val_mae: 0.0768 - val_mse: 0.0094
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0396 - mae: 0.1563 - mse: 0.0396
64/86 [=====================>........] - ETA: 0s - loss: 0.0297 - mae: 0.1321 - mse: 0.0297
86/86 [==============================] - 0s 6ms/step - loss: 0.0305 - mae: 0.1352 - mse: 0.0305 - val_loss: 0.0079 - val_mae: 0.0712 - val_mse: 0.0079
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0190 - mae: 0.0953 - mse: 0.0190
64/86 [=====================>........] - ETA: 0s - loss: 0.0364 - mae: 0.1296 - mse: 0.0364
86/86 [==============================] - 1s 6ms/step - loss: 0.0307 - mae: 0.1174 - mse: 0.0307 - val_loss: 0.0151 - val_mae: 0.0930 - val_mse: 0.0151
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0182 - mae: 0.1061 - mse: 0.0182
64/86 [=====================>........] - ETA: 0s - loss: 0.0184 - mae: 0.1004 - mse: 0.0184
86/86 [==============================] - 1s 6ms/step - loss: 0.0237 - mae: 0.1165 - mse: 0.0237 - val_loss: 0.0159 - val_mae: 0.0934 - val_mse: 0.0159
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0322 - mae: 0.1216 - mse: 0.0322
64/86 [=====================>........] - ETA: 0s - loss: 0.0246 - mae: 0.1110 - mse: 0.0246
86/86 [==============================] - 0s 6ms/step - loss: 0.0225 - mae: 0.1050 - mse: 0.0225 - val_loss: 0.0087 - val_mae: 0.0694 - val_mse: 0.0087
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0196 - mae: 0.1085 - mse: 0.0196
64/86 [=====================>........] - ETA: 0s - loss: 0.0208 - mae: 0.1073 - mse: 0.0208
86/86 [==============================] - 1s 6ms/step - loss: 0.0199 - mae: 0.1067 - mse: 0.0199 - val_loss: 0.0053 - val_mae: 0.0554 - val_mse: 0.0053
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0190 - mae: 0.0979 - mse: 0.0190
64/86 [=====================>........] - ETA: 0s - loss: 0.0207 - mae: 0.1039 - mse: 0.0207
86/86 [==============================] - 1s 6ms/step - loss: 0.0215 - mae: 0.1101 - mse: 0.0215 - val_loss: 0.0063 - val_mae: 0.0627 - val_mse: 0.0063
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0268 - mae: 0.1056 - mse: 0.0268
64/86 [=====================>........] - ETA: 0s - loss: 0.0214 - mae: 0.0978 - mse: 0.0214
86/86 [==============================] - 1s 6ms/step - loss: 0.0245 - mae: 0.1052 - mse: 0.0245 - val_loss: 0.0166 - val_mae: 0.1041 - val_mse: 0.0166
Saving trained model...
99
Testing...
heightdiff= [ 0.         0.         0.        15.7310257  0.         0.       ]
average prediction= [3.534365]
baseline= 9.5
eachuser= [0. 0. 0. 5. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 3.1462051391601564
85 -:- nan
60 -:- nan
['train-weight-9.py', '1']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
65 7
2_155_65_9_csi_a9_19.dat
65 9
65 10
65 11
65 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
65 15
2_155_65_9_csi_a9_7.dat
65 17
65 18
65 19
2_155_65_9_csi_a9_21.dat
65 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
65 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_9_csi_a9_29.dat
65 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
65 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
65 52
1_165_65_9_csi_a9_16.dat
65 54
65 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
65 58
65 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
65 62
65 63
1_165_65_9_csi_a9_5.dat
65 65
65 66
1_165_65_9_csi_a9_18.dat
65 68
65 69
1_165_65_9_csi_a9_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_9_csi_a9_22.dat
50 90
50 91
50 92
50 93
50 94
2_165_50_9_csi_a9_26.dat
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
1_180_85_9_csi_a9_7.dat
85 132
85 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
85 140
85 141
85 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
85 146
85 147
85 148
85 149
85 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
85 153
85 154
85 155
1_180_85_9_csi_a9_18.dat
85 157
85 158
85 159
85 160
75 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
75 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
75 171
1_180_75_9_csi_a9_25.dat
75 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
75 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
75 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
75 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
85 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
85 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
85 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
85 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
85 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 60 60 60 65 65 65
 65 65 65 65 65 65 65 65 65 65 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75 85 85 85 85
 85]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.2066 - mae: 0.3633 - mse: 0.2066
64/86 [=====================>........] - ETA: 0s - loss: 0.1571 - mae: 0.3128 - mse: 0.1571
86/86 [==============================] - 1s 10ms/step - loss: 0.1628 - mae: 0.3210 - mse: 0.1628 - val_loss: 0.1379 - val_mae: 0.3062 - val_mse: 0.1379
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1647 - mae: 0.3318 - mse: 0.1647
64/86 [=====================>........] - ETA: 0s - loss: 0.1473 - mae: 0.3180 - mse: 0.1473
86/86 [==============================] - 0s 6ms/step - loss: 0.1401 - mae: 0.3127 - mse: 0.1401 - val_loss: 0.0966 - val_mae: 0.2594 - val_mse: 0.0966
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1049 - mae: 0.2703 - mse: 0.1049
64/86 [=====================>........] - ETA: 0s - loss: 0.0912 - mae: 0.2507 - mse: 0.0912
86/86 [==============================] - 0s 6ms/step - loss: 0.1019 - mae: 0.2592 - mse: 0.1019 - val_loss: 0.0805 - val_mae: 0.2304 - val_mse: 0.0805
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0896 - mae: 0.2483 - mse: 0.0896
64/86 [=====================>........] - ETA: 0s - loss: 0.0844 - mae: 0.2364 - mse: 0.0844
86/86 [==============================] - 1s 6ms/step - loss: 0.0840 - mae: 0.2390 - mse: 0.0840 - val_loss: 0.0535 - val_mae: 0.2026 - val_mse: 0.0535
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0520 - mae: 0.1772 - mse: 0.0520
64/86 [=====================>........] - ETA: 0s - loss: 0.0644 - mae: 0.2061 - mse: 0.0644
86/86 [==============================] - 0s 6ms/step - loss: 0.0668 - mae: 0.2098 - mse: 0.0668 - val_loss: 0.0403 - val_mae: 0.1758 - val_mse: 0.0403
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0602 - mae: 0.1869 - mse: 0.0602
64/86 [=====================>........] - ETA: 0s - loss: 0.0670 - mae: 0.2051 - mse: 0.0670
86/86 [==============================] - 1s 6ms/step - loss: 0.0602 - mae: 0.1957 - mse: 0.0602 - val_loss: 0.0444 - val_mae: 0.1703 - val_mse: 0.0444
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0462 - mae: 0.1755 - mse: 0.0462
64/86 [=====================>........] - ETA: 0s - loss: 0.0444 - mae: 0.1693 - mse: 0.0444
86/86 [==============================] - 0s 6ms/step - loss: 0.0451 - mae: 0.1756 - mse: 0.0451 - val_loss: 0.0461 - val_mae: 0.1705 - val_mse: 0.0461
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0663 - mae: 0.2073 - mse: 0.0663
64/86 [=====================>........] - ETA: 0s - loss: 0.0592 - mae: 0.1990 - mse: 0.0592
86/86 [==============================] - 1s 6ms/step - loss: 0.0526 - mae: 0.1822 - mse: 0.0526 - val_loss: 0.0291 - val_mae: 0.1359 - val_mse: 0.0291
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0604 - mae: 0.2004 - mse: 0.0604
64/86 [=====================>........] - ETA: 0s - loss: 0.0515 - mae: 0.1734 - mse: 0.0515
86/86 [==============================] - 0s 6ms/step - loss: 0.0530 - mae: 0.1735 - mse: 0.0530 - val_loss: 0.0257 - val_mae: 0.1224 - val_mse: 0.0257
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0297 - mae: 0.1321 - mse: 0.0297
64/86 [=====================>........] - ETA: 0s - loss: 0.0412 - mae: 0.1548 - mse: 0.0412
86/86 [==============================] - 1s 6ms/step - loss: 0.0367 - mae: 0.1472 - mse: 0.0367 - val_loss: 0.0256 - val_mae: 0.1220 - val_mse: 0.0256
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0293 - mae: 0.1279 - mse: 0.0293
64/86 [=====================>........] - ETA: 0s - loss: 0.0302 - mae: 0.1325 - mse: 0.0302
86/86 [==============================] - 0s 6ms/step - loss: 0.0361 - mae: 0.1466 - mse: 0.0361 - val_loss: 0.0273 - val_mae: 0.1274 - val_mse: 0.0273
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0639 - mae: 0.1906 - mse: 0.0639
64/86 [=====================>........] - ETA: 0s - loss: 0.0506 - mae: 0.1635 - mse: 0.0506
86/86 [==============================] - 1s 6ms/step - loss: 0.0464 - mae: 0.1578 - mse: 0.0464 - val_loss: 0.0265 - val_mae: 0.1271 - val_mse: 0.0265
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0257 - mae: 0.1313 - mse: 0.0257
64/86 [=====================>........] - ETA: 0s - loss: 0.0335 - mae: 0.1414 - mse: 0.0335
86/86 [==============================] - 0s 6ms/step - loss: 0.0321 - mae: 0.1388 - mse: 0.0321 - val_loss: 0.0255 - val_mae: 0.1258 - val_mse: 0.0255
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0397 - mae: 0.1427 - mse: 0.0397
64/86 [=====================>........] - ETA: 0s - loss: 0.0408 - mae: 0.1468 - mse: 0.0408
86/86 [==============================] - 0s 6ms/step - loss: 0.0428 - mae: 0.1506 - mse: 0.0428 - val_loss: 0.0172 - val_mae: 0.1042 - val_mse: 0.0172
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0580 - mae: 0.1729 - mse: 0.0580
64/86 [=====================>........] - ETA: 0s - loss: 0.0465 - mae: 0.1565 - mse: 0.0465
86/86 [==============================] - 0s 6ms/step - loss: 0.0397 - mae: 0.1447 - mse: 0.0397 - val_loss: 0.0189 - val_mae: 0.1100 - val_mse: 0.0189
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0384 - mae: 0.1520 - mse: 0.0384
64/86 [=====================>........] - ETA: 0s - loss: 0.0328 - mae: 0.1405 - mse: 0.0328
86/86 [==============================] - 1s 6ms/step - loss: 0.0283 - mae: 0.1302 - mse: 0.0283 - val_loss: 0.0182 - val_mae: 0.1080 - val_mse: 0.0182
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0148 - mae: 0.0926 - mse: 0.0148
64/86 [=====================>........] - ETA: 0s - loss: 0.0194 - mae: 0.1004 - mse: 0.0194
86/86 [==============================] - 0s 6ms/step - loss: 0.0191 - mae: 0.0994 - mse: 0.0191 - val_loss: 0.0127 - val_mae: 0.0903 - val_mse: 0.0127
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0228 - mae: 0.1014 - mse: 0.0228
64/86 [=====================>........] - ETA: 0s - loss: 0.0288 - mae: 0.1239 - mse: 0.0288
86/86 [==============================] - 0s 6ms/step - loss: 0.0256 - mae: 0.1139 - mse: 0.0256 - val_loss: 0.0201 - val_mae: 0.1074 - val_mse: 0.0201
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0274 - mae: 0.1210 - mse: 0.0274
64/86 [=====================>........] - ETA: 0s - loss: 0.0278 - mae: 0.1243 - mse: 0.0278
86/86 [==============================] - 0s 6ms/step - loss: 0.0253 - mae: 0.1182 - mse: 0.0253 - val_loss: 0.0162 - val_mae: 0.0955 - val_mse: 0.0162
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0254 - mae: 0.1116 - mse: 0.0254
64/86 [=====================>........] - ETA: 0s - loss: 0.0188 - mae: 0.0976 - mse: 0.0188
86/86 [==============================] - 1s 6ms/step - loss: 0.0224 - mae: 0.1135 - mse: 0.0224 - val_loss: 0.0063 - val_mae: 0.0595 - val_mse: 0.0063
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0295 - mae: 0.1338 - mse: 0.0295
64/86 [=====================>........] - ETA: 0s - loss: 0.0247 - mae: 0.1142 - mse: 0.0247
86/86 [==============================] - 1s 6ms/step - loss: 0.0232 - mae: 0.1103 - mse: 0.0232 - val_loss: 0.0300 - val_mae: 0.1373 - val_mse: 0.0300
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0457 - mae: 0.1446 - mse: 0.0457
64/86 [=====================>........] - ETA: 0s - loss: 0.0391 - mae: 0.1353 - mse: 0.0391
86/86 [==============================] - 1s 6ms/step - loss: 0.0361 - mae: 0.1343 - mse: 0.0361 - val_loss: 0.0282 - val_mae: 0.1321 - val_mse: 0.0282
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0285 - mae: 0.1247 - mse: 0.0285
64/86 [=====================>........] - ETA: 0s - loss: 0.0237 - mae: 0.1091 - mse: 0.0237
86/86 [==============================] - 0s 6ms/step - loss: 0.0227 - mae: 0.1052 - mse: 0.0227 - val_loss: 0.0059 - val_mae: 0.0491 - val_mse: 0.0059
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0154 - mae: 0.0913 - mse: 0.0154
64/86 [=====================>........] - ETA: 0s - loss: 0.0250 - mae: 0.1137 - mse: 0.0250
86/86 [==============================] - 0s 6ms/step - loss: 0.0288 - mae: 0.1231 - mse: 0.0288 - val_loss: 0.0173 - val_mae: 0.0990 - val_mse: 0.0173
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0202 - mae: 0.1091 - mse: 0.0202
64/86 [=====================>........] - ETA: 0s - loss: 0.0211 - mae: 0.1110 - mse: 0.0211
86/86 [==============================] - 1s 6ms/step - loss: 0.0200 - mae: 0.1088 - mse: 0.0200 - val_loss: 0.0290 - val_mae: 0.1304 - val_mse: 0.0290
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0165 - mae: 0.0949 - mse: 0.0165
64/86 [=====================>........] - ETA: 0s - loss: 0.0209 - mae: 0.1086 - mse: 0.0209
86/86 [==============================] - 1s 6ms/step - loss: 0.0229 - mae: 0.1112 - mse: 0.0229 - val_loss: 0.0101 - val_mae: 0.0708 - val_mse: 0.0101
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0255 - mae: 0.1126 - mse: 0.0255
64/86 [=====================>........] - ETA: 0s - loss: 0.0244 - mae: 0.1117 - mse: 0.0244
86/86 [==============================] - 0s 6ms/step - loss: 0.0232 - mae: 0.1079 - mse: 0.0232 - val_loss: 0.0066 - val_mae: 0.0551 - val_mse: 0.0066
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0277 - mae: 0.1082 - mse: 0.0277
64/86 [=====================>........] - ETA: 0s - loss: 0.0214 - mae: 0.1044 - mse: 0.0214
86/86 [==============================] - 0s 6ms/step - loss: 0.0206 - mae: 0.1022 - mse: 0.0206 - val_loss: 0.0218 - val_mae: 0.1117 - val_mse: 0.0218
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0129 - mae: 0.0828 - mse: 0.0129
64/86 [=====================>........] - ETA: 0s - loss: 0.0179 - mae: 0.0969 - mse: 0.0179
86/86 [==============================] - 0s 6ms/step - loss: 0.0182 - mae: 0.0953 - mse: 0.0182 - val_loss: 0.0267 - val_mae: 0.1287 - val_mse: 0.0267
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0197 - mae: 0.1097 - mse: 0.0197
64/86 [=====================>........] - ETA: 0s - loss: 0.0175 - mae: 0.0978 - mse: 0.0175
86/86 [==============================] - 0s 6ms/step - loss: 0.0182 - mae: 0.1023 - mse: 0.0182 - val_loss: 0.0123 - val_mae: 0.0851 - val_mse: 0.0123
Saving trained model...
99
Testing...
heightdiff= [ 0.          0.          0.         13.45148468  0.          0.        ]
average prediction= [3.2264678]
baseline= 10.1
eachuser= [0. 0. 0. 7. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.9216406685965401
85 -:- nan
60 -:- nan
['train-weight-9.py', '1']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
65 7
2_155_65_9_csi_a9_19.dat
65 9
65 10
65 11
65 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
65 15
2_155_65_9_csi_a9_7.dat
65 17
65 18
65 19
2_155_65_9_csi_a9_21.dat
65 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
65 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_9_csi_a9_29.dat
65 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
65 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
65 52
1_165_65_9_csi_a9_16.dat
65 54
65 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
65 58
65 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
65 62
65 63
1_165_65_9_csi_a9_5.dat
65 65
65 66
1_165_65_9_csi_a9_18.dat
65 68
65 69
1_165_65_9_csi_a9_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_9_csi_a9_22.dat
50 90
50 91
50 92
50 93
50 94
2_165_50_9_csi_a9_26.dat
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
1_180_85_9_csi_a9_7.dat
85 132
85 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
85 140
85 141
85 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
85 146
85 147
85 148
85 149
85 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
85 153
85 154
85 155
1_180_85_9_csi_a9_18.dat
85 157
85 158
85 159
85 160
75 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
75 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
75 171
1_180_75_9_csi_a9_25.dat
75 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
75 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
75 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
75 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
85 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
85 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
85 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
85 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
85 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 60 60 60 65 65 65
 65 65 65 65 65 65 65 65 65 65 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75 85 85 85 85
 85]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.3565 - mae: 0.5161 - mse: 0.3565
64/86 [=====================>........] - ETA: 0s - loss: 0.3079 - mae: 0.4827 - mse: 0.3079
86/86 [==============================] - 1s 10ms/step - loss: 0.2654 - mae: 0.4397 - mse: 0.2654 - val_loss: 0.1138 - val_mae: 0.2764 - val_mse: 0.1138
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1716 - mae: 0.3503 - mse: 0.1716
64/86 [=====================>........] - ETA: 0s - loss: 0.1608 - mae: 0.3216 - mse: 0.1608
86/86 [==============================] - 1s 6ms/step - loss: 0.1552 - mae: 0.3177 - mse: 0.1552 - val_loss: 0.0719 - val_mae: 0.2405 - val_mse: 0.0719
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1450 - mae: 0.3018 - mse: 0.1450
64/86 [=====================>........] - ETA: 0s - loss: 0.1085 - mae: 0.2599 - mse: 0.1085
86/86 [==============================] - 0s 6ms/step - loss: 0.1087 - mae: 0.2615 - mse: 0.1087 - val_loss: 0.0750 - val_mae: 0.2461 - val_mse: 0.0750
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0813 - mae: 0.2495 - mse: 0.0813
64/86 [=====================>........] - ETA: 0s - loss: 0.0876 - mae: 0.2626 - mse: 0.0876
86/86 [==============================] - 0s 6ms/step - loss: 0.0807 - mae: 0.2494 - mse: 0.0807 - val_loss: 0.0842 - val_mae: 0.2595 - val_mse: 0.0842
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0914 - mae: 0.2784 - mse: 0.0914
64/86 [=====================>........] - ETA: 0s - loss: 0.0825 - mae: 0.2563 - mse: 0.0825
86/86 [==============================] - 0s 6ms/step - loss: 0.0893 - mae: 0.2641 - mse: 0.0893 - val_loss: 0.0672 - val_mae: 0.2428 - val_mse: 0.0672
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0655 - mae: 0.2241 - mse: 0.0655
64/86 [=====================>........] - ETA: 0s - loss: 0.0618 - mae: 0.2175 - mse: 0.0618
86/86 [==============================] - 0s 6ms/step - loss: 0.0637 - mae: 0.2204 - mse: 0.0637 - val_loss: 0.0537 - val_mae: 0.2047 - val_mse: 0.0537
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0812 - mae: 0.2368 - mse: 0.0812
64/86 [=====================>........] - ETA: 0s - loss: 0.0653 - mae: 0.2053 - mse: 0.0653
86/86 [==============================] - 1s 6ms/step - loss: 0.0678 - mae: 0.2067 - mse: 0.0678 - val_loss: 0.0473 - val_mae: 0.1836 - val_mse: 0.0473
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0522 - mae: 0.1961 - mse: 0.0522
64/86 [=====================>........] - ETA: 0s - loss: 0.0477 - mae: 0.1831 - mse: 0.0477
86/86 [==============================] - 0s 6ms/step - loss: 0.0461 - mae: 0.1754 - mse: 0.0461 - val_loss: 0.0446 - val_mae: 0.1917 - val_mse: 0.0446
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0533 - mae: 0.1924 - mse: 0.0533
64/86 [=====================>........] - ETA: 0s - loss: 0.0504 - mae: 0.1885 - mse: 0.0504
86/86 [==============================] - 0s 6ms/step - loss: 0.0490 - mae: 0.1887 - mse: 0.0490 - val_loss: 0.0462 - val_mae: 0.1941 - val_mse: 0.0462
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0486 - mae: 0.1810 - mse: 0.0486
64/86 [=====================>........] - ETA: 0s - loss: 0.0530 - mae: 0.1848 - mse: 0.0530
86/86 [==============================] - 1s 6ms/step - loss: 0.0519 - mae: 0.1821 - mse: 0.0519 - val_loss: 0.0370 - val_mae: 0.1693 - val_mse: 0.0370
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0310 - mae: 0.1477 - mse: 0.0310
64/86 [=====================>........] - ETA: 0s - loss: 0.0414 - mae: 0.1649 - mse: 0.0414
86/86 [==============================] - 0s 6ms/step - loss: 0.0416 - mae: 0.1649 - mse: 0.0416 - val_loss: 0.0345 - val_mae: 0.1623 - val_mse: 0.0345
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0447 - mae: 0.1618 - mse: 0.0447
64/86 [=====================>........] - ETA: 0s - loss: 0.0331 - mae: 0.1382 - mse: 0.0331
86/86 [==============================] - 0s 6ms/step - loss: 0.0329 - mae: 0.1409 - mse: 0.0329 - val_loss: 0.0353 - val_mae: 0.1618 - val_mse: 0.0353
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0339 - mae: 0.1537 - mse: 0.0339
64/86 [=====================>........] - ETA: 0s - loss: 0.0269 - mae: 0.1354 - mse: 0.0269
86/86 [==============================] - 1s 6ms/step - loss: 0.0288 - mae: 0.1362 - mse: 0.0288 - val_loss: 0.0413 - val_mae: 0.1740 - val_mse: 0.0413
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0365 - mae: 0.1453 - mse: 0.0365
64/86 [=====================>........] - ETA: 0s - loss: 0.0329 - mae: 0.1342 - mse: 0.0329
86/86 [==============================] - 0s 6ms/step - loss: 0.0302 - mae: 0.1341 - mse: 0.0302 - val_loss: 0.0410 - val_mae: 0.1666 - val_mse: 0.0410
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0221 - mae: 0.1122 - mse: 0.0221
64/86 [=====================>........] - ETA: 0s - loss: 0.0217 - mae: 0.1118 - mse: 0.0217
86/86 [==============================] - 0s 6ms/step - loss: 0.0230 - mae: 0.1168 - mse: 0.0230 - val_loss: 0.0410 - val_mae: 0.1525 - val_mse: 0.0410
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0280 - mae: 0.1218 - mse: 0.0280
64/86 [=====================>........] - ETA: 0s - loss: 0.0305 - mae: 0.1345 - mse: 0.0305
86/86 [==============================] - 0s 6ms/step - loss: 0.0266 - mae: 0.1262 - mse: 0.0266 - val_loss: 0.0397 - val_mae: 0.1357 - val_mse: 0.0397
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0362 - mae: 0.1430 - mse: 0.0362
64/86 [=====================>........] - ETA: 0s - loss: 0.0273 - mae: 0.1267 - mse: 0.0273
86/86 [==============================] - 1s 6ms/step - loss: 0.0267 - mae: 0.1259 - mse: 0.0267 - val_loss: 0.0365 - val_mae: 0.1320 - val_mse: 0.0365
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0222 - mae: 0.1006 - mse: 0.0222
64/86 [=====================>........] - ETA: 0s - loss: 0.0241 - mae: 0.1138 - mse: 0.0241
86/86 [==============================] - 1s 6ms/step - loss: 0.0204 - mae: 0.1035 - mse: 0.0204 - val_loss: 0.0247 - val_mae: 0.1183 - val_mse: 0.0247
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0237 - mae: 0.1005 - mse: 0.0237
64/86 [=====================>........] - ETA: 0s - loss: 0.0226 - mae: 0.1037 - mse: 0.0226
86/86 [==============================] - 0s 6ms/step - loss: 0.0230 - mae: 0.1009 - mse: 0.0230 - val_loss: 0.0298 - val_mae: 0.1364 - val_mse: 0.0298
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0188 - mae: 0.0997 - mse: 0.0188
64/86 [=====================>........] - ETA: 0s - loss: 0.0188 - mae: 0.1037 - mse: 0.0188
86/86 [==============================] - 0s 6ms/step - loss: 0.0178 - mae: 0.0978 - mse: 0.0178 - val_loss: 0.0375 - val_mae: 0.1481 - val_mse: 0.0375
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0346 - mae: 0.1288 - mse: 0.0346
64/86 [=====================>........] - ETA: 0s - loss: 0.0238 - mae: 0.1053 - mse: 0.0238
86/86 [==============================] - 0s 6ms/step - loss: 0.0230 - mae: 0.1062 - mse: 0.0230 - val_loss: 0.0288 - val_mae: 0.1193 - val_mse: 0.0288
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0202 - mae: 0.1078 - mse: 0.0202
64/86 [=====================>........] - ETA: 0s - loss: 0.0191 - mae: 0.1049 - mse: 0.0191
86/86 [==============================] - 0s 6ms/step - loss: 0.0221 - mae: 0.1144 - mse: 0.0221 - val_loss: 0.0420 - val_mae: 0.1436 - val_mse: 0.0420
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0128 - mae: 0.0872 - mse: 0.0128
64/86 [=====================>........] - ETA: 0s - loss: 0.0170 - mae: 0.0956 - mse: 0.0170
86/86 [==============================] - 1s 6ms/step - loss: 0.0209 - mae: 0.1027 - mse: 0.0209 - val_loss: 0.0592 - val_mae: 0.1799 - val_mse: 0.0592
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0246 - mae: 0.1237 - mse: 0.0246
64/86 [=====================>........] - ETA: 0s - loss: 0.0216 - mae: 0.1139 - mse: 0.0216
86/86 [==============================] - 0s 5ms/step - loss: 0.0185 - mae: 0.1051 - mse: 0.0185 - val_loss: 0.0269 - val_mae: 0.1247 - val_mse: 0.0269
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0163 - mae: 0.0898 - mse: 0.0163
64/86 [=====================>........] - ETA: 0s - loss: 0.0202 - mae: 0.1013 - mse: 0.0202
86/86 [==============================] - 0s 6ms/step - loss: 0.0175 - mae: 0.0942 - mse: 0.0175 - val_loss: 0.0202 - val_mae: 0.1089 - val_mse: 0.0202
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0198 - mae: 0.1048 - mse: 0.0198
64/86 [=====================>........] - ETA: 0s - loss: 0.0164 - mae: 0.0944 - mse: 0.0164
86/86 [==============================] - 0s 6ms/step - loss: 0.0151 - mae: 0.0891 - mse: 0.0151 - val_loss: 0.0349 - val_mae: 0.1358 - val_mse: 0.0349
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0336 - mae: 0.1265 - mse: 0.0336
64/86 [=====================>........] - ETA: 0s - loss: 0.0282 - mae: 0.1198 - mse: 0.0282
86/86 [==============================] - 1s 6ms/step - loss: 0.0230 - mae: 0.1065 - mse: 0.0230 - val_loss: 0.0664 - val_mae: 0.1886 - val_mse: 0.0664
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0187 - mae: 0.0953 - mse: 0.0187
64/86 [=====================>........] - ETA: 0s - loss: 0.0139 - mae: 0.0800 - mse: 0.0139
86/86 [==============================] - 0s 5ms/step - loss: 0.0194 - mae: 0.0910 - mse: 0.0194 - val_loss: 0.0557 - val_mae: 0.1525 - val_mse: 0.0557
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0117 - mae: 0.0730 - mse: 0.0117
64/86 [=====================>........] - ETA: 0s - loss: 0.0117 - mae: 0.0739 - mse: 0.0117
86/86 [==============================] - 0s 6ms/step - loss: 0.0110 - mae: 0.0735 - mse: 0.0110 - val_loss: 0.0258 - val_mae: 0.1005 - val_mse: 0.0258
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0220 - mae: 0.1109 - mse: 0.0220
64/86 [=====================>........] - ETA: 0s - loss: 0.0195 - mae: 0.0999 - mse: 0.0195
86/86 [==============================] - 1s 6ms/step - loss: 0.0183 - mae: 0.0974 - mse: 0.0183 - val_loss: 0.0352 - val_mae: 0.1396 - val_mse: 0.0352
Saving trained model...
99
Testing...
heightdiff= [0.         0.         0.         4.66007614 0.         0.        ]
average prediction= [4.5207677]
baseline= 10.3
eachuser= [0. 0. 0. 8. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.5825095176696777
85 -:- nan
60 -:- nan
['train-weight-9.py', '1']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
65 7
2_155_65_9_csi_a9_19.dat
65 9
65 10
65 11
65 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
65 15
2_155_65_9_csi_a9_7.dat
65 17
65 18
65 19
2_155_65_9_csi_a9_21.dat
65 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
65 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_9_csi_a9_29.dat
65 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
65 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
65 52
1_165_65_9_csi_a9_16.dat
65 54
65 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
65 58
65 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
65 62
65 63
1_165_65_9_csi_a9_5.dat
65 65
65 66
1_165_65_9_csi_a9_18.dat
65 68
65 69
1_165_65_9_csi_a9_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_9_csi_a9_22.dat
50 90
50 91
50 92
50 93
50 94
2_165_50_9_csi_a9_26.dat
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
1_180_85_9_csi_a9_7.dat
85 132
85 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
85 140
85 141
85 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
85 146
85 147
85 148
85 149
85 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
85 153
85 154
85 155
1_180_85_9_csi_a9_18.dat
85 157
85 158
85 159
85 160
75 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
75 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
75 171
1_180_75_9_csi_a9_25.dat
75 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
75 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
75 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
75 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
85 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
85 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
85 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
85 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
85 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 60 60 60 65 65 65
 65 65 65 65 65 65 65 65 65 65 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75 85 85 85 85
 85]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.3483 - mae: 0.4752 - mse: 0.3483
64/86 [=====================>........] - ETA: 0s - loss: 0.3193 - mae: 0.4736 - mse: 0.3193
86/86 [==============================] - 1s 10ms/step - loss: 0.2920 - mae: 0.4495 - mse: 0.2920 - val_loss: 0.0618 - val_mae: 0.1790 - val_mse: 0.0618
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1180 - mae: 0.2917 - mse: 0.1180
64/86 [=====================>........] - ETA: 0s - loss: 0.1264 - mae: 0.2955 - mse: 0.1264
86/86 [==============================] - 1s 6ms/step - loss: 0.1219 - mae: 0.2920 - mse: 0.1219 - val_loss: 0.1194 - val_mae: 0.3420 - val_mse: 0.1194
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1171 - mae: 0.2905 - mse: 0.1171
64/86 [=====================>........] - ETA: 0s - loss: 0.1257 - mae: 0.3044 - mse: 0.1257
86/86 [==============================] - 1s 6ms/step - loss: 0.1187 - mae: 0.2924 - mse: 0.1187 - val_loss: 0.0827 - val_mae: 0.2836 - val_mse: 0.0827
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1068 - mae: 0.2623 - mse: 0.1068
64/86 [=====================>........] - ETA: 0s - loss: 0.1099 - mae: 0.2637 - mse: 0.1099
86/86 [==============================] - 1s 6ms/step - loss: 0.0945 - mae: 0.2453 - mse: 0.0945 - val_loss: 0.0397 - val_mae: 0.1724 - val_mse: 0.0397
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0500 - mae: 0.1843 - mse: 0.0500
64/86 [=====================>........] - ETA: 0s - loss: 0.0566 - mae: 0.1918 - mse: 0.0566
86/86 [==============================] - 0s 6ms/step - loss: 0.0701 - mae: 0.2128 - mse: 0.0701 - val_loss: 0.0374 - val_mae: 0.1683 - val_mse: 0.0374
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1031 - mae: 0.2659 - mse: 0.1031
64/86 [=====================>........] - ETA: 0s - loss: 0.0815 - mae: 0.2361 - mse: 0.0815
86/86 [==============================] - 0s 6ms/step - loss: 0.0753 - mae: 0.2268 - mse: 0.0753 - val_loss: 0.0565 - val_mae: 0.2019 - val_mse: 0.0565
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0509 - mae: 0.1807 - mse: 0.0509
64/86 [=====================>........] - ETA: 0s - loss: 0.0644 - mae: 0.1973 - mse: 0.0644
86/86 [==============================] - 1s 6ms/step - loss: 0.0698 - mae: 0.2015 - mse: 0.0698 - val_loss: 0.0846 - val_mae: 0.2294 - val_mse: 0.0846
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0667 - mae: 0.2123 - mse: 0.0667
64/86 [=====================>........] - ETA: 0s - loss: 0.0633 - mae: 0.1999 - mse: 0.0633
86/86 [==============================] - 0s 6ms/step - loss: 0.0634 - mae: 0.1972 - mse: 0.0634 - val_loss: 0.0698 - val_mae: 0.2109 - val_mse: 0.0698
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0651 - mae: 0.2133 - mse: 0.0651
64/86 [=====================>........] - ETA: 0s - loss: 0.0561 - mae: 0.1872 - mse: 0.0561
86/86 [==============================] - 1s 6ms/step - loss: 0.0529 - mae: 0.1878 - mse: 0.0529 - val_loss: 0.0574 - val_mae: 0.1906 - val_mse: 0.0574
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0762 - mae: 0.2048 - mse: 0.0762
64/86 [=====================>........] - ETA: 0s - loss: 0.0541 - mae: 0.1717 - mse: 0.0541
86/86 [==============================] - 0s 5ms/step - loss: 0.0472 - mae: 0.1608 - mse: 0.0472 - val_loss: 0.0502 - val_mae: 0.1732 - val_mse: 0.0502
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0493 - mae: 0.1703 - mse: 0.0493
64/86 [=====================>........] - ETA: 0s - loss: 0.0469 - mae: 0.1660 - mse: 0.0469
86/86 [==============================] - 0s 6ms/step - loss: 0.0485 - mae: 0.1675 - mse: 0.0485 - val_loss: 0.0529 - val_mae: 0.1706 - val_mse: 0.0529
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0450 - mae: 0.1689 - mse: 0.0450
64/86 [=====================>........] - ETA: 0s - loss: 0.0388 - mae: 0.1571 - mse: 0.0388
86/86 [==============================] - 0s 6ms/step - loss: 0.0347 - mae: 0.1454 - mse: 0.0347 - val_loss: 0.0523 - val_mae: 0.1689 - val_mse: 0.0523
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0228 - mae: 0.1196 - mse: 0.0228
64/86 [=====================>........] - ETA: 0s - loss: 0.0342 - mae: 0.1307 - mse: 0.0342
86/86 [==============================] - 0s 6ms/step - loss: 0.0334 - mae: 0.1277 - mse: 0.0334 - val_loss: 0.0490 - val_mae: 0.1514 - val_mse: 0.0490
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0221 - mae: 0.1102 - mse: 0.0221
64/86 [=====================>........] - ETA: 0s - loss: 0.0281 - mae: 0.1251 - mse: 0.0281
86/86 [==============================] - 0s 6ms/step - loss: 0.0311 - mae: 0.1336 - mse: 0.0311 - val_loss: 0.0343 - val_mae: 0.1240 - val_mse: 0.0343
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0199 - mae: 0.0988 - mse: 0.0199
64/86 [=====================>........] - ETA: 0s - loss: 0.0312 - mae: 0.1220 - mse: 0.0312
86/86 [==============================] - 0s 5ms/step - loss: 0.0297 - mae: 0.1260 - mse: 0.0297 - val_loss: 0.0350 - val_mae: 0.1304 - val_mse: 0.0350
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0166 - mae: 0.0970 - mse: 0.0166
64/86 [=====================>........] - ETA: 0s - loss: 0.0247 - mae: 0.1123 - mse: 0.0247
86/86 [==============================] - 1s 6ms/step - loss: 0.0300 - mae: 0.1260 - mse: 0.0300 - val_loss: 0.0331 - val_mae: 0.1346 - val_mse: 0.0331
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0292 - mae: 0.1353 - mse: 0.0292
64/86 [=====================>........] - ETA: 0s - loss: 0.0283 - mae: 0.1204 - mse: 0.0283
86/86 [==============================] - 1s 6ms/step - loss: 0.0299 - mae: 0.1247 - mse: 0.0299 - val_loss: 0.0244 - val_mae: 0.1137 - val_mse: 0.0244
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0282 - mae: 0.1202 - mse: 0.0282
64/86 [=====================>........] - ETA: 0s - loss: 0.0256 - mae: 0.1234 - mse: 0.0256
86/86 [==============================] - 1s 6ms/step - loss: 0.0283 - mae: 0.1295 - mse: 0.0283 - val_loss: 0.0256 - val_mae: 0.1144 - val_mse: 0.0256
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0375 - mae: 0.1421 - mse: 0.0375
64/86 [=====================>........] - ETA: 0s - loss: 0.0283 - mae: 0.1231 - mse: 0.0283
86/86 [==============================] - 1s 6ms/step - loss: 0.0238 - mae: 0.1086 - mse: 0.0238 - val_loss: 0.0386 - val_mae: 0.1282 - val_mse: 0.0386
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0330 - mae: 0.1156 - mse: 0.0330
64/86 [=====================>........] - ETA: 0s - loss: 0.0244 - mae: 0.0997 - mse: 0.0244
86/86 [==============================] - 1s 6ms/step - loss: 0.0238 - mae: 0.1014 - mse: 0.0238 - val_loss: 0.0299 - val_mae: 0.1135 - val_mse: 0.0299
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0316 - mae: 0.1445 - mse: 0.0316
64/86 [=====================>........] - ETA: 0s - loss: 0.0246 - mae: 0.1177 - mse: 0.0246
86/86 [==============================] - 1s 6ms/step - loss: 0.0236 - mae: 0.1171 - mse: 0.0236 - val_loss: 0.0188 - val_mae: 0.1015 - val_mse: 0.0188
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0242 - mae: 0.1124 - mse: 0.0242
64/86 [=====================>........] - ETA: 0s - loss: 0.0280 - mae: 0.1243 - mse: 0.0280
86/86 [==============================] - 1s 6ms/step - loss: 0.0255 - mae: 0.1153 - mse: 0.0255 - val_loss: 0.0180 - val_mae: 0.1009 - val_mse: 0.0180
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0340 - mae: 0.1415 - mse: 0.0340
64/86 [=====================>........] - ETA: 0s - loss: 0.0255 - mae: 0.1230 - mse: 0.0255
86/86 [==============================] - 1s 6ms/step - loss: 0.0233 - mae: 0.1176 - mse: 0.0233 - val_loss: 0.0233 - val_mae: 0.1187 - val_mse: 0.0233
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0121 - mae: 0.0904 - mse: 0.0121
64/86 [=====================>........] - ETA: 0s - loss: 0.0117 - mae: 0.0839 - mse: 0.0117
86/86 [==============================] - 1s 6ms/step - loss: 0.0149 - mae: 0.0879 - mse: 0.0149 - val_loss: 0.0215 - val_mae: 0.1027 - val_mse: 0.0215
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0227 - mae: 0.0966 - mse: 0.0227
64/86 [=====================>........] - ETA: 0s - loss: 0.0226 - mae: 0.1040 - mse: 0.0226
86/86 [==============================] - 1s 6ms/step - loss: 0.0242 - mae: 0.1088 - mse: 0.0242 - val_loss: 0.0198 - val_mae: 0.1059 - val_mse: 0.0198
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0353 - mae: 0.1453 - mse: 0.0353
64/86 [=====================>........] - ETA: 0s - loss: 0.0306 - mae: 0.1317 - mse: 0.0306
86/86 [==============================] - 1s 6ms/step - loss: 0.0279 - mae: 0.1196 - mse: 0.0279 - val_loss: 0.0214 - val_mae: 0.1022 - val_mse: 0.0214
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0329 - mae: 0.1249 - mse: 0.0329
64/86 [=====================>........] - ETA: 0s - loss: 0.0277 - mae: 0.1213 - mse: 0.0277
86/86 [==============================] - 0s 6ms/step - loss: 0.0266 - mae: 0.1155 - mse: 0.0266 - val_loss: 0.0152 - val_mae: 0.0930 - val_mse: 0.0152
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0262 - mae: 0.1297 - mse: 0.0262
64/86 [=====================>........] - ETA: 0s - loss: 0.0225 - mae: 0.1135 - mse: 0.0225
86/86 [==============================] - 0s 6ms/step - loss: 0.0197 - mae: 0.1049 - mse: 0.0197 - val_loss: 0.0136 - val_mae: 0.0869 - val_mse: 0.0136
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0113 - mae: 0.0694 - mse: 0.0113
64/86 [=====================>........] - ETA: 0s - loss: 0.0163 - mae: 0.0900 - mse: 0.0163
86/86 [==============================] - 0s 6ms/step - loss: 0.0152 - mae: 0.0891 - mse: 0.0152 - val_loss: 0.0144 - val_mae: 0.0888 - val_mse: 0.0144
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0163 - mae: 0.0950 - mse: 0.0163
64/86 [=====================>........] - ETA: 0s - loss: 0.0181 - mae: 0.0968 - mse: 0.0181
86/86 [==============================] - 0s 6ms/step - loss: 0.0170 - mae: 0.0964 - mse: 0.0170 - val_loss: 0.0154 - val_mae: 0.0928 - val_mse: 0.0154
Saving trained model...
99
Testing...
heightdiff= [ 0.          0.          0.         10.58883286  0.          0.        ]
average prediction= [2.6977308]
baseline= 9.1
eachuser= [0. 0. 0. 7. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.5126904078892298
85 -:- nan
60 -:- nan
['train-weight-9.py', '1']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
65 7
2_155_65_9_csi_a9_19.dat
65 9
65 10
65 11
65 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
65 15
2_155_65_9_csi_a9_7.dat
65 17
65 18
65 19
2_155_65_9_csi_a9_21.dat
65 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
65 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_9_csi_a9_29.dat
65 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
65 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
65 52
1_165_65_9_csi_a9_16.dat
65 54
65 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
65 58
65 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
65 62
65 63
1_165_65_9_csi_a9_5.dat
65 65
65 66
1_165_65_9_csi_a9_18.dat
65 68
65 69
1_165_65_9_csi_a9_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_9_csi_a9_22.dat
50 90
50 91
50 92
50 93
50 94
2_165_50_9_csi_a9_26.dat
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
1_180_85_9_csi_a9_7.dat
85 132
85 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
85 140
85 141
85 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
85 146
85 147
85 148
85 149
85 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
85 153
85 154
85 155
1_180_85_9_csi_a9_18.dat
85 157
85 158
85 159
85 160
75 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
75 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
75 171
1_180_75_9_csi_a9_25.dat
75 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
75 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
75 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
75 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
85 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
85 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
85 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
85 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
85 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 60 60 60 65 65 65
 65 65 65 65 65 65 65 65 65 65 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75 85 85 85 85
 85]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.3365 - mae: 0.4611 - mse: 0.3365
64/86 [=====================>........] - ETA: 0s - loss: 0.3127 - mae: 0.4557 - mse: 0.3127
86/86 [==============================] - 1s 10ms/step - loss: 0.2795 - mae: 0.4244 - mse: 0.2795 - val_loss: 0.0802 - val_mae: 0.2122 - val_mse: 0.0802
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1672 - mae: 0.3293 - mse: 0.1672
64/86 [=====================>........] - ETA: 0s - loss: 0.1330 - mae: 0.3002 - mse: 0.1330
86/86 [==============================] - 1s 6ms/step - loss: 0.1320 - mae: 0.2970 - mse: 0.1320 - val_loss: 0.0661 - val_mae: 0.2024 - val_mse: 0.0661
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1395 - mae: 0.3143 - mse: 0.1395
64/86 [=====================>........] - ETA: 0s - loss: 0.1081 - mae: 0.2770 - mse: 0.1081
86/86 [==============================] - 0s 6ms/step - loss: 0.1229 - mae: 0.2945 - mse: 0.1229 - val_loss: 0.0569 - val_mae: 0.1940 - val_mse: 0.0569
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1099 - mae: 0.2699 - mse: 0.1099
64/86 [=====================>........] - ETA: 0s - loss: 0.1056 - mae: 0.2722 - mse: 0.1056
86/86 [==============================] - 0s 6ms/step - loss: 0.0969 - mae: 0.2572 - mse: 0.0969 - val_loss: 0.0591 - val_mae: 0.1922 - val_mse: 0.0591
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0865 - mae: 0.2224 - mse: 0.0865
64/86 [=====================>........] - ETA: 0s - loss: 0.0784 - mae: 0.2226 - mse: 0.0784
86/86 [==============================] - 0s 5ms/step - loss: 0.0738 - mae: 0.2176 - mse: 0.0738 - val_loss: 0.0559 - val_mae: 0.1978 - val_mse: 0.0559
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0881 - mae: 0.2240 - mse: 0.0881
64/86 [=====================>........] - ETA: 0s - loss: 0.0768 - mae: 0.2216 - mse: 0.0768
86/86 [==============================] - 0s 5ms/step - loss: 0.0695 - mae: 0.2156 - mse: 0.0695 - val_loss: 0.0526 - val_mae: 0.1977 - val_mse: 0.0526
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0492 - mae: 0.1815 - mse: 0.0492
64/86 [=====================>........] - ETA: 0s - loss: 0.0487 - mae: 0.1851 - mse: 0.0487
86/86 [==============================] - 0s 5ms/step - loss: 0.0472 - mae: 0.1779 - mse: 0.0472 - val_loss: 0.0575 - val_mae: 0.2126 - val_mse: 0.0575
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0644 - mae: 0.1998 - mse: 0.0644
64/86 [=====================>........] - ETA: 0s - loss: 0.0505 - mae: 0.1727 - mse: 0.0505
86/86 [==============================] - 0s 5ms/step - loss: 0.0481 - mae: 0.1692 - mse: 0.0481 - val_loss: 0.0606 - val_mae: 0.2254 - val_mse: 0.0606
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0733 - mae: 0.1941 - mse: 0.0733
64/86 [=====================>........] - ETA: 0s - loss: 0.0534 - mae: 0.1609 - mse: 0.0534
86/86 [==============================] - 0s 5ms/step - loss: 0.0499 - mae: 0.1598 - mse: 0.0499 - val_loss: 0.0623 - val_mae: 0.2349 - val_mse: 0.0623
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0271 - mae: 0.1251 - mse: 0.0271
64/86 [=====================>........] - ETA: 0s - loss: 0.0400 - mae: 0.1436 - mse: 0.0400
86/86 [==============================] - 0s 5ms/step - loss: 0.0442 - mae: 0.1526 - mse: 0.0442 - val_loss: 0.0435 - val_mae: 0.1962 - val_mse: 0.0435
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0257 - mae: 0.1130 - mse: 0.0257
64/86 [=====================>........] - ETA: 0s - loss: 0.0252 - mae: 0.1197 - mse: 0.0252
86/86 [==============================] - 0s 5ms/step - loss: 0.0258 - mae: 0.1239 - mse: 0.0258 - val_loss: 0.0220 - val_mae: 0.1309 - val_mse: 0.0220
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0314 - mae: 0.1392 - mse: 0.0314
64/86 [=====================>........] - ETA: 0s - loss: 0.0313 - mae: 0.1313 - mse: 0.0313
86/86 [==============================] - 0s 5ms/step - loss: 0.0298 - mae: 0.1247 - mse: 0.0298 - val_loss: 0.0144 - val_mae: 0.1014 - val_mse: 0.0144
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0239 - mae: 0.1213 - mse: 0.0239
64/86 [=====================>........] - ETA: 0s - loss: 0.0314 - mae: 0.1326 - mse: 0.0314
86/86 [==============================] - 0s 5ms/step - loss: 0.0301 - mae: 0.1297 - mse: 0.0301 - val_loss: 0.0139 - val_mae: 0.1090 - val_mse: 0.0139
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0347 - mae: 0.1304 - mse: 0.0347
64/86 [=====================>........] - ETA: 0s - loss: 0.0258 - mae: 0.1175 - mse: 0.0258
86/86 [==============================] - 0s 5ms/step - loss: 0.0287 - mae: 0.1259 - mse: 0.0287 - val_loss: 0.0110 - val_mae: 0.0998 - val_mse: 0.0110
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0177 - mae: 0.1018 - mse: 0.0177
64/86 [=====================>........] - ETA: 0s - loss: 0.0259 - mae: 0.1198 - mse: 0.0259
86/86 [==============================] - 0s 5ms/step - loss: 0.0250 - mae: 0.1190 - mse: 0.0250 - val_loss: 0.0078 - val_mae: 0.0824 - val_mse: 0.0078
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0333 - mae: 0.1361 - mse: 0.0333
64/86 [=====================>........] - ETA: 0s - loss: 0.0249 - mae: 0.1178 - mse: 0.0249
86/86 [==============================] - 0s 5ms/step - loss: 0.0295 - mae: 0.1242 - mse: 0.0295 - val_loss: 0.0078 - val_mae: 0.0743 - val_mse: 0.0078
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0288 - mae: 0.1185 - mse: 0.0288
64/86 [=====================>........] - ETA: 0s - loss: 0.0281 - mae: 0.1204 - mse: 0.0281
86/86 [==============================] - 0s 5ms/step - loss: 0.0346 - mae: 0.1324 - mse: 0.0346 - val_loss: 0.0117 - val_mae: 0.1030 - val_mse: 0.0117
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0283 - mae: 0.1149 - mse: 0.0283
64/86 [=====================>........] - ETA: 0s - loss: 0.0237 - mae: 0.1098 - mse: 0.0237
86/86 [==============================] - 0s 6ms/step - loss: 0.0230 - mae: 0.1123 - mse: 0.0230 - val_loss: 0.0068 - val_mae: 0.0752 - val_mse: 0.0068
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0186 - mae: 0.1038 - mse: 0.0186
64/86 [=====================>........] - ETA: 0s - loss: 0.0244 - mae: 0.1140 - mse: 0.0244
86/86 [==============================] - 1s 7ms/step - loss: 0.0201 - mae: 0.1027 - mse: 0.0201 - val_loss: 0.0088 - val_mae: 0.0890 - val_mse: 0.0088
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0281 - mae: 0.1132 - mse: 0.0281
64/86 [=====================>........] - ETA: 0s - loss: 0.0243 - mae: 0.1096 - mse: 0.0243
86/86 [==============================] - 1s 12ms/step - loss: 0.0265 - mae: 0.1176 - mse: 0.0265 - val_loss: 0.0197 - val_mae: 0.1190 - val_mse: 0.0197
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0263 - mae: 0.1206 - mse: 0.0263
64/86 [=====================>........] - ETA: 0s - loss: 0.0191 - mae: 0.1016 - mse: 0.0191
86/86 [==============================] - 1s 11ms/step - loss: 0.0242 - mae: 0.1128 - mse: 0.0242 - val_loss: 0.0082 - val_mae: 0.0799 - val_mse: 0.0082
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0235 - mae: 0.1151 - mse: 0.0235
64/86 [=====================>........] - ETA: 0s - loss: 0.0199 - mae: 0.1072 - mse: 0.0199
86/86 [==============================] - 1s 6ms/step - loss: 0.0206 - mae: 0.1075 - mse: 0.0206 - val_loss: 0.0033 - val_mae: 0.0417 - val_mse: 0.0033
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0202 - mae: 0.1171 - mse: 0.0202
64/86 [=====================>........] - ETA: 0s - loss: 0.0333 - mae: 0.1412 - mse: 0.0333
86/86 [==============================] - 1s 6ms/step - loss: 0.0273 - mae: 0.1228 - mse: 0.0273 - val_loss: 0.0079 - val_mae: 0.0821 - val_mse: 0.0079
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0210 - mae: 0.1153 - mse: 0.0210
64/86 [=====================>........] - ETA: 0s - loss: 0.0198 - mae: 0.1103 - mse: 0.0198
86/86 [==============================] - 1s 6ms/step - loss: 0.0236 - mae: 0.1122 - mse: 0.0236 - val_loss: 0.0193 - val_mae: 0.1229 - val_mse: 0.0193
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0232 - mae: 0.1145 - mse: 0.0232
64/86 [=====================>........] - ETA: 0s - loss: 0.0213 - mae: 0.1079 - mse: 0.0213
86/86 [==============================] - 1s 6ms/step - loss: 0.0197 - mae: 0.1049 - mse: 0.0197 - val_loss: 0.0052 - val_mae: 0.0638 - val_mse: 0.0052
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0120 - mae: 0.0817 - mse: 0.0120
64/86 [=====================>........] - ETA: 0s - loss: 0.0215 - mae: 0.1018 - mse: 0.0215
86/86 [==============================] - 1s 6ms/step - loss: 0.0266 - mae: 0.1077 - mse: 0.0266 - val_loss: 0.0046 - val_mae: 0.0592 - val_mse: 0.0046
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0261 - mae: 0.1108 - mse: 0.0261
64/86 [=====================>........] - ETA: 0s - loss: 0.0274 - mae: 0.1138 - mse: 0.0274
86/86 [==============================] - 1s 6ms/step - loss: 0.0252 - mae: 0.1100 - mse: 0.0252 - val_loss: 0.0164 - val_mae: 0.1090 - val_mse: 0.0164
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0237 - mae: 0.1234 - mse: 0.0237
64/86 [=====================>........] - ETA: 0s - loss: 0.0290 - mae: 0.1289 - mse: 0.0290
86/86 [==============================] - 0s 6ms/step - loss: 0.0244 - mae: 0.1145 - mse: 0.0244 - val_loss: 0.0095 - val_mae: 0.0837 - val_mse: 0.0095
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0175 - mae: 0.0973 - mse: 0.0175
64/86 [=====================>........] - ETA: 0s - loss: 0.0215 - mae: 0.1114 - mse: 0.0215
86/86 [==============================] - 1s 6ms/step - loss: 0.0218 - mae: 0.1113 - mse: 0.0218 - val_loss: 0.0038 - val_mae: 0.0455 - val_mse: 0.0038
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0147 - mae: 0.0907 - mse: 0.0147
64/86 [=====================>........] - ETA: 0s - loss: 0.0156 - mae: 0.0943 - mse: 0.0156
86/86 [==============================] - 1s 6ms/step - loss: 0.0239 - mae: 0.1144 - mse: 0.0239 - val_loss: 0.0039 - val_mae: 0.0533 - val_mse: 0.0039
Saving trained model...
99
Testing...
heightdiff= [0.         0.         0.         5.57056046 0.         0.        ]
average prediction= [2.8217328]
baseline= 9.1
eachuser= [0. 0. 0. 5. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 1.114112091064453
85 -:- nan
60 -:- nan
['train-weight-9.py', '1']
2_155_65_9_csi_a9_18.dat
2_155_65_9_csi_a9_28.dat
2_155_65_9_csi_a9_3.dat
2_155_65_9_csi_a9_24.dat
2_155_65_9_csi_a9_14.dat
2_155_65_9_csi_a9_26.dat
65 7
2_155_65_9_csi_a9_19.dat
65 9
65 10
65 11
65 12
2_155_65_9_csi_a9_23.dat
2_155_65_9_csi_a9_15.dat
65 15
2_155_65_9_csi_a9_7.dat
65 17
65 18
65 19
2_155_65_9_csi_a9_21.dat
65 21
2_155_65_9_csi_a9_20.dat
2_155_65_9_csi_a9_9.dat
2_155_65_9_csi_a9_25.dat
2_155_65_9_csi_a9_27.dat
65 26
2_155_65_9_csi_a9_13.dat
2_155_65_9_csi_a9_6.dat
2_155_65_9_csi_a9_30.dat
2_155_65_9_csi_a9_8.dat
60 31
60 32
60 33
60 34
60 35
60 36
60 37
60 38
60 39
60 40
1_165_65_9_csi_a9_29.dat
65 42
1_165_65_9_csi_a9_15.dat
1_165_65_9_csi_a9_30.dat
1_165_65_9_csi_a9_28.dat
65 46
1_165_65_9_csi_a9_20.dat
1_165_65_9_csi_a9_10.dat
1_165_65_9_csi_a9_23.dat
1_165_65_9_csi_a9_4.dat
1_165_65_9_csi_a9_14.dat
65 52
1_165_65_9_csi_a9_16.dat
65 54
65 55
1_165_65_9_csi_a9_12.dat
1_165_65_9_csi_a9_21.dat
65 58
65 59
1_165_65_9_csi_a9_24.dat
1_165_65_9_csi_a9_17.dat
65 62
65 63
1_165_65_9_csi_a9_5.dat
65 65
65 66
1_165_65_9_csi_a9_18.dat
65 68
65 69
1_165_65_9_csi_a9_25.dat
50 71
50 72
50 73
50 74
50 75
50 76
50 77
50 78
50 79
50 80
50 81
50 82
50 83
50 84
50 85
50 86
50 87
50 88
2_165_50_9_csi_a9_22.dat
50 90
50 91
50 92
50 93
50 94
2_165_50_9_csi_a9_26.dat
50 96
50 97
50 98
50 99
50 100
70 101
70 102
70 103
70 104
70 105
70 106
70 107
70 108
70 109
70 110
70 111
70 112
70 113
70 114
70 115
70 116
70 117
70 118
70 119
70 120
70 121
70 122
70 123
70 124
70 125
70 126
70 127
70 128
70 129
70 130
1_180_85_9_csi_a9_7.dat
85 132
85 133
1_180_85_9_csi_a9_24.dat
1_180_85_9_csi_a9_8.dat
1_180_85_9_csi_a9_29.dat
1_180_85_9_csi_a9_3.dat
1_180_85_9_csi_a9_22.dat
1_180_85_9_csi_a9_1.dat
85 140
85 141
85 142
1_180_85_9_csi_a9_12.dat
1_180_85_9_csi_a9_5.dat
1_180_85_9_csi_a9_23.dat
85 146
85 147
85 148
85 149
85 150
1_180_85_9_csi_a9_20.dat
1_180_85_9_csi_a9_15.dat
85 153
85 154
85 155
1_180_85_9_csi_a9_18.dat
85 157
85 158
85 159
85 160
75 161
1_180_75_9_csi_a9_14.dat
1_180_75_9_csi_a9_9.dat
75 164
1_180_75_9_csi_a9_8.dat
1_180_75_9_csi_a9_5.dat
1_180_75_9_csi_a9_4.dat
1_180_75_9_csi_a9_12.dat
1_180_75_9_csi_a9_15.dat
1_180_75_9_csi_a9_19.dat
75 171
1_180_75_9_csi_a9_25.dat
75 173
1_180_75_9_csi_a9_23.dat
1_180_75_9_csi_a9_28.dat
1_180_75_9_csi_a9_10.dat
75 177
1_180_75_9_csi_a9_13.dat
1_180_75_9_csi_a9_6.dat
1_180_75_9_csi_a9_2.dat
1_180_75_9_csi_a9_7.dat
1_180_75_9_csi_a9_26.dat
75 183
1_180_75_9_csi_a9_3.dat
1_180_75_9_csi_a9_11.dat
1_180_75_9_csi_a9_30.dat
75 187
1_180_75_9_csi_a9_17.dat
1_180_75_9_csi_a9_27.dat
1_180_75_9_csi_a9_29.dat
1_173_85_9_csi_a9_9.dat
85 192
1_173_85_9_csi_a9_2.dat
1_173_85_9_csi_a9_24.dat
85 195
1_173_85_9_csi_a9_29.dat
1_173_85_9_csi_a9_13.dat
1_173_85_9_csi_a9_28.dat
1_173_85_9_csi_a9_16.dat
1_173_85_9_csi_a9_10.dat
85 201
1_173_85_9_csi_a9_23.dat
1_173_85_9_csi_a9_5.dat
1_173_85_9_csi_a9_7.dat
1_173_85_9_csi_a9_3.dat
1_173_85_9_csi_a9_27.dat
85 207
1_173_85_9_csi_a9_6.dat
1_173_85_9_csi_a9_1.dat
1_173_85_9_csi_a9_15.dat
1_173_85_9_csi_a9_11.dat
1_173_85_9_csi_a9_30.dat
1_173_85_9_csi_a9_17.dat
1_173_85_9_csi_a9_21.dat
85 215
1_173_85_9_csi_a9_22.dat
1_173_85_9_csi_a9_8.dat
1_173_85_9_csi_a9_18.dat
1_173_85_9_csi_a9_4.dat
1_173_85_9_csi_a9_20.dat
(121, 30, 3)
(121, 449, 30, 3)
[65 65 65 65 65 65 65 65 65 65 65 60 60 60 60 60 60 60 60 60 60 65 65 65
 65 65 65 65 65 65 65 65 65 65 50 50 50 50 50 50 50 50 50 50 50 50 50 50
 50 50 50 50 50 50 50 50 50 50 50 50 50 50 70 70 70 70 70 70 70 70 70 70
 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 70 85 85 85 85
 85 85 85 85 85 85 85 85 85 85 85 85 85 75 75 75 75 75 75 75 85 85 85 85
 85]
(121, 449, 30, 3, 1)

Loaded dataset of 121 samples, each sized (449, 30, 3, 1)


Train on 96 samples
Test on 25 samples

Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
name_model_input (InputLayer (None, 449, 30, 3, 1)     0         
_________________________________________________________________
time_distributed_1 (TimeDist (None, 449, 28, 1, 16)    160       
_________________________________________________________________
time_distributed_2 (TimeDist (None, 449, 14, 1, 16)    0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 449, 224)          0         
_________________________________________________________________
time_distributed_4 (TimeDist (None, 449, 64)           14400     
_________________________________________________________________
time_distributed_5 (TimeDist (None, 449, 64)           0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 449, 64)           4160      
_________________________________________________________________
gru_1 (GRU)                  (None, 128)               74112     
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0         
_________________________________________________________________
name_model_output (Dense)    (None, 1)                 129       
=================================================================
Total params: 92,961
Trainable params: 92,961
Non-trainable params: 0
_________________________________________________________________
Train on 86 samples, validate on 10 samples
Epoch 1/30

32/86 [==========>...................] - ETA: 0s - loss: 0.3270 - mae: 0.4859 - mse: 0.3270
64/86 [=====================>........] - ETA: 0s - loss: 0.2609 - mae: 0.4227 - mse: 0.2609
86/86 [==============================] - 1s 10ms/step - loss: 0.2406 - mae: 0.4030 - mse: 0.2406 - val_loss: 0.0994 - val_mae: 0.2354 - val_mse: 0.0994
Epoch 2/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1803 - mae: 0.3458 - mse: 0.1803
64/86 [=====================>........] - ETA: 0s - loss: 0.1736 - mae: 0.3369 - mse: 0.1736
86/86 [==============================] - 1s 6ms/step - loss: 0.1577 - mae: 0.3184 - mse: 0.1577 - val_loss: 0.0723 - val_mae: 0.2085 - val_mse: 0.0723
Epoch 3/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1287 - mae: 0.2977 - mse: 0.1287
64/86 [=====================>........] - ETA: 0s - loss: 0.1226 - mae: 0.2857 - mse: 0.1226
86/86 [==============================] - 1s 6ms/step - loss: 0.1208 - mae: 0.2867 - mse: 0.1208 - val_loss: 0.0757 - val_mae: 0.2131 - val_mse: 0.0757
Epoch 4/30

32/86 [==========>...................] - ETA: 0s - loss: 0.1438 - mae: 0.3172 - mse: 0.1438
64/86 [=====================>........] - ETA: 0s - loss: 0.1203 - mae: 0.2875 - mse: 0.1203
86/86 [==============================] - 1s 6ms/step - loss: 0.1144 - mae: 0.2816 - mse: 0.1144 - val_loss: 0.0878 - val_mae: 0.2239 - val_mse: 0.0878
Epoch 5/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0992 - mae: 0.2622 - mse: 0.0992
64/86 [=====================>........] - ETA: 0s - loss: 0.0909 - mae: 0.2520 - mse: 0.0909
86/86 [==============================] - 0s 6ms/step - loss: 0.0836 - mae: 0.2430 - mse: 0.0836 - val_loss: 0.0698 - val_mae: 0.2304 - val_mse: 0.0698
Epoch 6/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0717 - mae: 0.2216 - mse: 0.0717
64/86 [=====================>........] - ETA: 0s - loss: 0.0855 - mae: 0.2446 - mse: 0.0855
86/86 [==============================] - 0s 6ms/step - loss: 0.0791 - mae: 0.2395 - mse: 0.0791 - val_loss: 0.0565 - val_mae: 0.2270 - val_mse: 0.0565
Epoch 7/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0570 - mae: 0.2143 - mse: 0.0570
64/86 [=====================>........] - ETA: 0s - loss: 0.0594 - mae: 0.2123 - mse: 0.0594
86/86 [==============================] - 0s 6ms/step - loss: 0.0548 - mae: 0.2052 - mse: 0.0548 - val_loss: 0.0601 - val_mae: 0.2286 - val_mse: 0.0601
Epoch 8/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0483 - mae: 0.1853 - mse: 0.0483
64/86 [=====================>........] - ETA: 0s - loss: 0.0505 - mae: 0.1835 - mse: 0.0505
86/86 [==============================] - 1s 6ms/step - loss: 0.0465 - mae: 0.1762 - mse: 0.0465 - val_loss: 0.0637 - val_mae: 0.2359 - val_mse: 0.0637
Epoch 9/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0602 - mae: 0.2033 - mse: 0.0602
64/86 [=====================>........] - ETA: 0s - loss: 0.0603 - mae: 0.1962 - mse: 0.0603
86/86 [==============================] - 0s 6ms/step - loss: 0.0517 - mae: 0.1820 - mse: 0.0517 - val_loss: 0.0613 - val_mae: 0.2348 - val_mse: 0.0613
Epoch 10/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0565 - mae: 0.1901 - mse: 0.0565
64/86 [=====================>........] - ETA: 0s - loss: 0.0431 - mae: 0.1652 - mse: 0.0431
86/86 [==============================] - 0s 6ms/step - loss: 0.0453 - mae: 0.1712 - mse: 0.0453 - val_loss: 0.0542 - val_mae: 0.2167 - val_mse: 0.0542
Epoch 11/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0539 - mae: 0.1815 - mse: 0.0539
64/86 [=====================>........] - ETA: 0s - loss: 0.0596 - mae: 0.1750 - mse: 0.0596
86/86 [==============================] - 0s 6ms/step - loss: 0.0522 - mae: 0.1682 - mse: 0.0522 - val_loss: 0.0475 - val_mae: 0.1914 - val_mse: 0.0475
Epoch 12/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0494 - mae: 0.1758 - mse: 0.0494
64/86 [=====================>........] - ETA: 0s - loss: 0.0430 - mae: 0.1584 - mse: 0.0430
86/86 [==============================] - 0s 5ms/step - loss: 0.0422 - mae: 0.1584 - mse: 0.0422 - val_loss: 0.0453 - val_mae: 0.1816 - val_mse: 0.0453
Epoch 13/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0232 - mae: 0.1051 - mse: 0.0232
64/86 [=====================>........] - ETA: 0s - loss: 0.0238 - mae: 0.1133 - mse: 0.0238
86/86 [==============================] - 0s 6ms/step - loss: 0.0285 - mae: 0.1255 - mse: 0.0285 - val_loss: 0.0440 - val_mae: 0.1867 - val_mse: 0.0440
Epoch 14/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0279 - mae: 0.1218 - mse: 0.0279
64/86 [=====================>........] - ETA: 0s - loss: 0.0372 - mae: 0.1408 - mse: 0.0372
86/86 [==============================] - 0s 6ms/step - loss: 0.0356 - mae: 0.1340 - mse: 0.0356 - val_loss: 0.0449 - val_mae: 0.1904 - val_mse: 0.0449
Epoch 15/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0312 - mae: 0.1300 - mse: 0.0312
64/86 [=====================>........] - ETA: 0s - loss: 0.0291 - mae: 0.1149 - mse: 0.0291
86/86 [==============================] - 0s 6ms/step - loss: 0.0337 - mae: 0.1226 - mse: 0.0337 - val_loss: 0.0427 - val_mae: 0.1872 - val_mse: 0.0427
Epoch 16/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0249 - mae: 0.1191 - mse: 0.0249
64/86 [=====================>........] - ETA: 0s - loss: 0.0367 - mae: 0.1358 - mse: 0.0367
86/86 [==============================] - 0s 6ms/step - loss: 0.0331 - mae: 0.1280 - mse: 0.0331 - val_loss: 0.0386 - val_mae: 0.1721 - val_mse: 0.0386
Epoch 17/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0177 - mae: 0.1041 - mse: 0.0177
64/86 [=====================>........] - ETA: 0s - loss: 0.0249 - mae: 0.1159 - mse: 0.0249
86/86 [==============================] - 1s 6ms/step - loss: 0.0246 - mae: 0.1163 - mse: 0.0246 - val_loss: 0.0363 - val_mae: 0.1705 - val_mse: 0.0363
Epoch 18/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0276 - mae: 0.1287 - mse: 0.0276
64/86 [=====================>........] - ETA: 0s - loss: 0.0288 - mae: 0.1288 - mse: 0.0288
86/86 [==============================] - 0s 5ms/step - loss: 0.0267 - mae: 0.1229 - mse: 0.0267 - val_loss: 0.0364 - val_mae: 0.1687 - val_mse: 0.0364
Epoch 19/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0275 - mae: 0.1220 - mse: 0.0275
64/86 [=====================>........] - ETA: 0s - loss: 0.0240 - mae: 0.1150 - mse: 0.0240
86/86 [==============================] - 1s 6ms/step - loss: 0.0231 - mae: 0.1114 - mse: 0.0231 - val_loss: 0.0310 - val_mae: 0.1459 - val_mse: 0.0310
Epoch 20/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0312 - mae: 0.1306 - mse: 0.0312
64/86 [=====================>........] - ETA: 0s - loss: 0.0319 - mae: 0.1253 - mse: 0.0319
86/86 [==============================] - 0s 6ms/step - loss: 0.0297 - mae: 0.1234 - mse: 0.0297 - val_loss: 0.0321 - val_mae: 0.1526 - val_mse: 0.0321
Epoch 21/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0312 - mae: 0.1252 - mse: 0.0312
64/86 [=====================>........] - ETA: 0s - loss: 0.0272 - mae: 0.1214 - mse: 0.0272
86/86 [==============================] - 1s 6ms/step - loss: 0.0250 - mae: 0.1170 - mse: 0.0250 - val_loss: 0.0391 - val_mae: 0.1774 - val_mse: 0.0391
Epoch 22/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0171 - mae: 0.0957 - mse: 0.0171
64/86 [=====================>........] - ETA: 0s - loss: 0.0206 - mae: 0.1067 - mse: 0.0206
86/86 [==============================] - 0s 6ms/step - loss: 0.0182 - mae: 0.0997 - mse: 0.0182 - val_loss: 0.0370 - val_mae: 0.1689 - val_mse: 0.0370
Epoch 23/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0218 - mae: 0.1088 - mse: 0.0218
64/86 [=====================>........] - ETA: 0s - loss: 0.0184 - mae: 0.0997 - mse: 0.0184
86/86 [==============================] - 0s 5ms/step - loss: 0.0196 - mae: 0.1023 - mse: 0.0196 - val_loss: 0.0288 - val_mae: 0.1438 - val_mse: 0.0288
Epoch 24/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0148 - mae: 0.0963 - mse: 0.0148
64/86 [=====================>........] - ETA: 0s - loss: 0.0194 - mae: 0.1021 - mse: 0.0194
86/86 [==============================] - 0s 6ms/step - loss: 0.0218 - mae: 0.1079 - mse: 0.0218 - val_loss: 0.0257 - val_mae: 0.1346 - val_mse: 0.0257
Epoch 25/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0144 - mae: 0.0955 - mse: 0.0144
64/86 [=====================>........] - ETA: 0s - loss: 0.0136 - mae: 0.0867 - mse: 0.0136
86/86 [==============================] - 0s 6ms/step - loss: 0.0144 - mae: 0.0909 - mse: 0.0144 - val_loss: 0.0274 - val_mae: 0.1348 - val_mse: 0.0274
Epoch 26/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0253 - mae: 0.1186 - mse: 0.0253
64/86 [=====================>........] - ETA: 0s - loss: 0.0184 - mae: 0.0995 - mse: 0.0184
86/86 [==============================] - 0s 6ms/step - loss: 0.0214 - mae: 0.1058 - mse: 0.0214 - val_loss: 0.0383 - val_mae: 0.1496 - val_mse: 0.0383
Epoch 27/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0220 - mae: 0.1014 - mse: 0.0220
64/86 [=====================>........] - ETA: 0s - loss: 0.0237 - mae: 0.1087 - mse: 0.0237
86/86 [==============================] - 0s 6ms/step - loss: 0.0212 - mae: 0.1040 - mse: 0.0212 - val_loss: 0.0280 - val_mae: 0.1277 - val_mse: 0.0280
Epoch 28/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0113 - mae: 0.0782 - mse: 0.0113
64/86 [=====================>........] - ETA: 0s - loss: 0.0127 - mae: 0.0812 - mse: 0.0127
86/86 [==============================] - 0s 5ms/step - loss: 0.0167 - mae: 0.0935 - mse: 0.0167 - val_loss: 0.0231 - val_mae: 0.1293 - val_mse: 0.0231
Epoch 29/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0124 - mae: 0.0871 - mse: 0.0124
64/86 [=====================>........] - ETA: 0s - loss: 0.0153 - mae: 0.0942 - mse: 0.0153
86/86 [==============================] - 1s 6ms/step - loss: 0.0151 - mae: 0.0934 - mse: 0.0151 - val_loss: 0.0305 - val_mae: 0.1479 - val_mse: 0.0305
Epoch 30/30

32/86 [==========>...................] - ETA: 0s - loss: 0.0236 - mae: 0.1085 - mse: 0.0236
64/86 [=====================>........] - ETA: 0s - loss: 0.0175 - mae: 0.0897 - mse: 0.0175
86/86 [==============================] - 0s 6ms/step - loss: 0.0216 - mae: 0.1029 - mse: 0.0216 - val_loss: 0.0268 - val_mae: 0.1375 - val_mse: 0.0268
Saving trained model...
99
Testing...
heightdiff= [0.         0.         0.         3.92385483 0.         0.        ]
average prediction= [4.32191]
baseline= 9.7
eachuser= [0. 0. 0. 6. 0. 0.]
65 -:- nan
70 -:- nan
75 -:- nan
50 -:- 0.6539758046468099
85 -:- nan
60 -:- nan
